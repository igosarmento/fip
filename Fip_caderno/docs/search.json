[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nBem-vindo ao meu website!\n\n\n\n\n\nOlá! Me chamo [Igo Sarmento], sou Engenheiro Agrônomo graduado pela Universidade Federal do Amazonas (UFAM) e atualmente curso mestrado em Fitopatologia pela Universidade Federal de Viçosa (UFV), vinculado ao [Laboratório de Interação Planta-Patógeno (LIPP)].\nEste site foi desenvolvido por meio da plataforma Quarto Markdown como parte das atividades da disciplina FIP 606 - Análise e Visualização de Dados em Fitopatologia, ministrada pelo professor Emerson Del Ponte. A proposta deste espaço é reunir, sistematizar e compartilhar os conteúdos e análises realizadas ao longo do semestre, com ênfase no uso da linguagem R voltada à estatística e visualização de dados aplicados à Fitopatologia.\nFique à vontade para navegar pelos materiais disponíveis e explorar as possibilidades que as ferramentas computacionais oferecem na pesquisa fitopatológica.\nDúvidas, críticas construtivas ou sugestões são sempre bem-vindas — entre em contato!"
  },
  {
    "objectID": "index.html#aula-04-diagnóstico-de-resíduos-com-dharma",
    "href": "index.html#aula-04-diagnóstico-de-resíduos-com-dharma",
    "title": "Fip_caderno",
    "section": "AULA 04 – Diagnóstico de Resíduos com DHARMa",
    "text": "AULA 04 – Diagnóstico de Resíduos com DHARMa\n\nObjetivo:\nAvaliar a qualidade do modelo ajustado m3 utilizando o pacote DHARMa, que fornece diagnósticos de resíduos simulados para modelos lineares e generalizados, testando pressupostos como normalidade e homocedasticidade.\n\n\nCode\nlibrary(DHARMa)\nm3\n\n\n\nCall:\nlm(formula = rank(count + 0.1) ~ spray, data = insetos)\n\nCoefficients:\n(Intercept)       sprayB       sprayC       sprayD       sprayE       sprayF  \n     52.167        2.667      -40.708      -26.583      -32.833        3.458  \n\n\nCode\nplot(simulateResiduals(m3))"
  },
  {
    "objectID": "index.html#aula-05-análise-de-experimentos-com-blocos-e-transformações",
    "href": "index.html#aula-05-análise-de-experimentos-com-blocos-e-transformações",
    "title": "Fip_caderno",
    "section": "AULA 05 – Análise de Experimentos com Blocos e Transformações",
    "text": "AULA 05 – Análise de Experimentos com Blocos e Transformações\n\nImportação e preparação dos dados de campo\nNeste trecho, os dados experimentais de campo são importados do Google Sheets e as variáveis categóricas são corretamente convertidas em fatores. Também convertemos a variável FER para numérica, garantindo que esteja no formato adequado para análises.\n\n\nVisualização exploratória da variável resposta\nAntes de ajustar o modelo, é fundamental visualizar a dispersão dos dados para cada tratamento e possíveis tendências nos valores de PROD (produtividade).\n\n\nAjuste de modelo linear com transformação logarítmica\nAqui, ajustamos um modelo linear para a variável FER (fermentação), transformada via logaritmo natural, considerando os efeitos fixos de bloco e tratamento.\n\n\nVerificação da significância via ANOVA\nUtilizamos a ANOVA para avaliar a significância dos efeitos de tratamento e bloco sobre a variável resposta transformada.\n\n\nDiagnóstico de resíduos do modelo\nAvaliamos os resíduos simulados do modelo com o pacote DHARMa, verificando possíveis desvios das premissas de normalidade e homocedasticidade.\n\n\nEstimativas das médias marginais (EMMeans)\nCalculamos as médias ajustadas para cada tratamento, as visualizamos e comparamos com letras de significância.\n\n\nCorrelações entre variáveis contínuas\nCalculamos correlações entre variáveis importantes do experimento (FER, PROD, DFC) para investigar relações lineares entre elas.\n\n\nCode\nlibrary(gsheet)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(DHARMa)\nlibrary(emmeans)\nlibrary(multcomp)\n\n# Importando os dados\ncampo &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=866852711#gid=866852711\")\n\n# Convertendo variáveis em fatores corretamente\ncampo &lt;- campo |&gt; \n  mutate(TRAT = factor(TRAT),\n         BLOCO = factor(BLOCO),\n         FER = as.numeric(FER)) # Convertendo FER para numérico caso esteja como texto\n\n# Visualizando os dados\nggplot(campo, aes(x = TRAT, y = PROD)) + \n  geom_jitter(width = 0.1) + \n  stat_summary(fun.data = \"mean_cl_boot\", colour = \"red\")\n\n\n\n\n\n\n\n\n\nCode\n# Ajustando o modelo correto\nm_campo &lt;- lm(log(FER) ~ BLOCO + TRAT, data = campo)\n\n# Verificando ANOVA\nanova(m_campo)\n\n\nAnalysis of Variance Table\n\nResponse: log(FER)\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nBLOCO      3  0.2064 0.06880  1.7961    0.1788    \nTRAT       7 11.5210 1.64585 42.9665 4.838e-11 ***\nResiduals 21  0.8044 0.03831                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# Diagnóstico de resíduos\nplot(simulateResiduals(m_campo))\n\n\n\n\n\n\n\n\n\nCode\n# Médias marginais estimadas\nmeans_campo &lt;- emmeans(m_campo, ~ TRAT, type = \"response\")\n\n# Plotando e analisando as médias\nplot(means_campo)\n\n\n\n\n\n\n\n\n\nCode\nmeans_campo\n\n\n TRAT response    SE df lower.CL upper.CL\n 1       20.02 1.960 21    16.33    24.54\n 2        5.68 0.556 21     4.63     6.96\n 3        3.81 0.373 21     3.11     4.67\n 4        3.08 0.301 21     2.51     3.78\n 5        3.24 0.317 21     2.64     3.97\n 6        2.98 0.292 21     2.43     3.65\n 7        3.37 0.330 21     2.75     4.13\n 8        3.48 0.341 21     2.84     4.27\n\nResults are averaged over the levels of: BLOCO \nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \n\n\nCode\nlibrary(agricolae)\ncv.model(m_campo)\n\n\n[1] 13.13068\n\n\nCode\ncld(means_campo)\n\n\n TRAT response    SE df lower.CL upper.CL .group\n 6        2.98 0.292 21     2.43     3.65  1    \n 4        3.08 0.301 21     2.51     3.78  1    \n 5        3.24 0.317 21     2.64     3.97  1    \n 7        3.37 0.330 21     2.75     4.13  1    \n 8        3.48 0.341 21     2.84     4.27  1    \n 3        3.81 0.373 21     3.11     4.67  12   \n 2        5.68 0.556 21     4.63     6.96   2   \n 1       20.02 1.960 21    16.33    24.54    3  \n\nResults are averaged over the levels of: BLOCO \nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \nP value adjustment: tukey method for comparing a family of 8 estimates \nTests are performed on the log scale \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nCode\npwpp(means_campo)\n\n\n\n\n\n\n\n\n\nCode\npwpm(means_campo) # Comparação dos tratamentos numa matriz\n\n\n        1       2       3       4       5       6       7       8\n1 [20.02]  &lt;.0001  &lt;.0001  &lt;.0001  &lt;.0001  &lt;.0001  &lt;.0001  &lt;.0001\n2   3.525 [ 5.68]  0.1252  0.0048  0.0110  0.0028  0.0204  0.0343\n3   5.259   1.492 [ 3.81]  0.7832  0.9335  0.6440  0.9843  0.9976\n4   6.500   1.844   1.236 [ 3.08]  0.9999  1.0000  0.9976  0.9842\n5   6.178   1.753   1.175   0.951 [ 3.24]  0.9984  1.0000  0.9994\n6   6.721   1.906   1.278   1.034   1.088 [ 2.98]  0.9842  0.9431\n7   5.945   1.686   1.130   0.915   0.962   0.885 [ 3.37]  1.0000\n8   5.750   1.631   1.093   0.885   0.931   0.856   0.967 [ 3.48]\n\nRow and column labels: TRAT\nUpper triangle: P values   null = 1  adjust = \"tukey\"\nDiagonal: [Estimates] (response)   type = \"response\"\nLower triangle: Comparisons (ratio)   earlier vs. later\n\n\nCode\ncor(campo$FER, campo$PROD)\n\n\n[1] -0.6258321\n\n\nCode\ncor.test(campo$FER, campo$DFC)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  campo$FER and campo$DFC\nt = 14.049, df = 30, p-value = 9.864e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8635525 0.9664228\nsample estimates:\n      cor \n0.9316978 \n\n\nCode\ncampo |&gt; \n  ggplot(aes(FER, DFC)) +\n  geom_point() +  # Correção aqui\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "index.html#modelo-linear-misto-análise-com-dados-de-milho",
    "href": "index.html#modelo-linear-misto-análise-com-dados-de-milho",
    "title": "Fip_caderno",
    "section": "Modelo Linear Misto – Análise com Dados de Milho",
    "text": "Modelo Linear Misto – Análise com Dados de Milho\n\nImportação e visualização dos dados\nImportamos outro conjunto de dados com experimentos em milho e visualizamos os efeitos dos híbridos e métodos sobre o índice.\n\n\nPreparação dos dados e ajuste de modelo misto\nCriamos uma variável de interação entre híbrido e bloco, e ajustamos um modelo linear misto com efeito aleatório de hybrid_block.\n\n\nANOVA e diagnóstico do modelo misto\nUtilizamos a função Anova() para verificar a significância dos efeitos fixos e DHARMa para verificar resíduos simulados.\n\n\nAjuste de novo modelo para produtividade\nAjustamos um segundo modelo considerando a variável resposta yield, incorporando estrutura hierárquica com blocos aninhados em híbrido.\n\n\nAjuste de novo modelo para produtividade\nAjustamos um segundo modelo considerando a variável resposta yield, incorporando estrutura hierárquica com blocos aninhados em híbrido.\n\n\nRelação entre índice e produtividade\nExploramos a correlação entre index e yield, tanto visualmente quanto numericamente, calculando o coeficiente de determinação (R²).\n\n\nCode\nlibrary(gsheet)\nlibrary(lme4)\nlibrary(car)\nlibrary(DHARMa)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(multcomp)\nlibrary(emmeans)\n\n# Importando os dados\nmilho &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=1345524759#gid=1345524759\")\n\n# Visualização dos dados\nmilho |&gt; \n  ggplot(aes(hybrid, index, color = method)) +\n  geom_jitter(width = 0.1) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nCode\n# Criando a interação de híbrido e bloco\nmilho &lt;- milho |&gt; mutate(hybrid_block = interaction(hybrid, block))\n\n# Ajuste do modelo linear misto\nm_milho &lt;- lmer(index ~ hybrid * method + (1 | hybrid_block), data = milho)\n\n# ANOVA\nAnova(m_milho)\n\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: index\n                Chisq Df Pr(&gt;Chisq)   \nhybrid        11.4239  5    0.04359 * \nmethod         4.6964  1    0.03023 * \nhybrid:method 15.8062  5    0.00742 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# Diagnóstico de resíduos\nplot(simulateResiduals(m_milho))\n\n\n\n\n\n\n\n\n\nCode\nm_milho2 &lt;- lmer(yield ~ hybrid * method + (1 | block:hybrid_block), data = milho)\ncar::Anova(m_milho2)\n\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: yield\n                Chisq Df Pr(&gt;Chisq)    \nhybrid        22.5966  5  0.0004031 ***\nmethod         0.1052  1  0.7456932    \nhybrid:method 25.9302  5  9.206e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nplot(simulateResiduals(m_milho2))\n\n\n\n\n\n\n\n\n\nCode\nmedia_milho2 &lt;- emmeans(m_milho2, ~ method | hybrid)\ncld(media_milho2, Letters = letters)\n\n\nhybrid = 30F53 HX:\n method emmean  SE   df lower.CL upper.CL .group\n silk     9988 798 21.1     8328    11647  a    \n pin     11208 798 21.1     9548    12867   b   \n\nhybrid = 30F53 YH:\n method emmean  SE   df lower.CL upper.CL .group\n silk     9211 798 21.1     7552    10870  a    \n pin      9408 798 21.1     7748    11067  a    \n\nhybrid = 30K64:\n method emmean  SE   df lower.CL upper.CL .group\n silk    10361 798 21.1     8702    12020  a    \n pin     11675 798 21.1    10016    13334   b   \n\nhybrid = 30S31H:\n method emmean  SE   df lower.CL upper.CL .group\n pin      8118 798 21.1     6459     9777  a    \n silk     9185 798 21.1     7526    10844   b   \n\nhybrid = 30S31YH:\n method emmean  SE   df lower.CL upper.CL .group\n pin      7836 798 21.1     6177     9495  a    \n silk     8277 798 21.1     6618     9936  a    \n\nhybrid = BG7049H:\n method emmean  SE   df lower.CL upper.CL .group\n pin     11970 798 21.1    10311    13629  a    \n silk    12833 798 21.1    11174    14492  a    \n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nCode\n# Correção do erro de digitação: \"indez\" para \"index\"\nmilho |&gt; \n  ggplot(aes(index, yield)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nCode\ncor1 &lt;- cor(milho$index, milho$yield)\ncor1^2 * 100\n\n\n[1] 6.323713\n\n\nCode\ncor.test(milho$index, milho$yield)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  milho$index and milho$yield\nt = -1.7622, df = 46, p-value = 0.08468\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.49988704  0.03517829\nsample estimates:\n       cor \n-0.2514699"
  },
  {
    "objectID": "index.html#análise-de-dose-resposta-drc-e-cálculo-de-ec50",
    "href": "index.html#análise-de-dose-resposta-drc-e-cálculo-de-ec50",
    "title": "Fip_caderno",
    "section": "Análise de Dose-Resposta (DRC) e cálculo de EC50",
    "text": "Análise de Dose-Resposta (DRC) e cálculo de EC50\nNesta seção, utilizamos o pacote drc para ajustar modelos de regressão não-linear do tipo log-logístico, que são ideais para dados de dose-resposta. Avaliamos o efeito da dose sobre a germinação fúngica, e estimamos a dose efetiva 50% (EC50) para o isolado FGT43.\nAjuste do modelo log-logístico para um isolado específico (FGT43)\n\nEstimativa de EC50 para múltiplos isolados com ec50estimator\nNeste passo, usamos um pacote específico para automatizar a estimativa de EC50 para todos os isolados presentes no conjunto de da\n\n\nCode\n# Importando os dados\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(drc)\n\n# Gráfico com os dados\nfungi &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=465348652#gid=465348652\")\n\nfungi |&gt;\n  dplyr::group_by(code, dose) |&gt;\n  summarise(germination = mean(germination), .groups = \"drop\") |&gt;\n  ggplot(aes(dose, germination)) +\n  geom_point() +\n  facet_wrap(~ code)\n\n\n\n\n\n\n\n\n\nCode\n# Ajuste do modelo para FGT43\n\nFGT43 &lt;- fungi |&gt;\n  filter(code == \"FGT43\")\n\nm43 &lt;- drm(germination ~ dose,\n           data = FGT43,\n           fct = LL.3())\n\nplot(m43)\n\n\n\n\n\n\n\n\n\nCode\nED(m43, 50)\n\n\n\nEstimated effective doses\n\n       Estimate Std. Error\ne:1:50 0.495955   0.046022\n\n\nCode\nlibrary(\"ec50estimator\")\n\ndf_ec50 = estimate_EC50(germination ~dose,\n                        data = fungi,\n                        isolate_col = \"code\",\n                        strata_col = \"state\",\n                        interval = \"delta\",\n                        fct = drc:: LL.3())\ndf_ec50 |&gt;\nggplot(aes(reorder(ID, Estimate), Estimate))+\n  geom_point()+\n  coord_flip()\n\n\n\n\n\n\n\n\n\nCode\ndf_ec50 |&gt;\nggplot(aes(x = Estimate))+\n  geom_histogram(bins = 5, color = \"white\")\n\n\n\n\n\n\n\n\n\nNeste exercício, utilizaremos um conjunto de dados relacionados à ferrugem do cafeeiro (Hemileia vastatrix) na Etiópia. O objetivo é demonstrar como importar, visualizar e representar espacialmente dados fitopatológicos com ferramentas modernas de R. A base foi obtida por meio de levantamento em fazendas cafeeiras, contendo registros de latitude, longitude e incidência da doença.\nUtilizamos a função gsheet2tbl() do pacote gsheet para importar os dados diretamente da nuvem (Google Sheets). O objeto cr agora contém as observações de incidência de ferrugem do café, com coordenadas geográficas de coleta.\n\n\nCode\nlibrary(gsheet) \ncr &lt;-  gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=1871397229#gid=1871397229\")  \n\n\nUsamos a função datatable() do pacote DT para gerar uma tabela interativa, facilitando a visualização e a exploração dos dados. Essa visualização é útil para verificar rapidamente colunas como lat, lon e inc.\n\n\nCode\nlibrary(DT) \ndatatable(cr) \n\n\n\n\n\n\nEssa é uma visualização básica dos pontos geográficos de coleta. A função geom_point() do ggplot2 plota cada observação com base em sua longitude (lon) e latitude (lat).\nImportamos os limites administrativos da Etiópia em formato espacial usando o pacote rnaturalearthhires. O resultado (ETH) é um objeto sf que pode ser usado com ggplot2.\nEste gráfico mostra um mapa detalhado da Etiópia com os pontos de coleta georreferenciados. A intensidade da cor representa a incidência da ferrugem do cafeeiro, permitindo identificar padrões geoespaciais da doença.\nSalva o mapa gerado no formato .png, com fundo branco e largura de 10 polegadas. Isso permite utilizar a imagem em apresentações, relatórios ou publicações.\n\n\nCode\nlibrary(tidyverse)\n\ncr |&gt; \n  ggplot(aes(lon, lat))+\n  geom_point()\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rnaturalearth)\nlibrary(rnaturalearthhires)\n\nremotes:: install_github(\"ropensci/rnaturalearthhires\")\n\n\n* checking for file 'C:\\Users\\igo_a\\AppData\\Local\\Temp\\Rtmpao5npr\\remotes5ef06bff6922\\ropensci-rnaturalearthhires-e4736f6/DESCRIPTION' ... OK\n* preparing 'rnaturalearthhires':\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\n* building 'rnaturalearthhires_1.0.0.9000.tar.gz'\n\n\nCode\nETH &lt;- ne_states(country = \"Ethiopia\", returnclass = \"sf\")\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(ggspatial)\n\n\nETH |&gt;\nggplot()+ \n  geom_sf(fill = \"gray80\") + \n  geom_point(data = cr, aes(lon, lat, color = inc))+\n  scale_color_viridis_c()+\n  theme_minimal()+\n  theme(legend.position = \"bottom\")+\n  annotation_scale(location = \"tl\")+\n  annotation_north_arrow(location = \"br\", which_north = \"true\")+\n  labs(title = \"Ferrugem do café na Etiópia\", x = \"Longitude\", Y = \"Latiitude\", subtitle = \"Levantamento em fazendas\", caption = \"Fonte: Del Pnte et al. (2025)\", color = \"Incidência (%)\")\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"mapa_etiopia.png\", bg = \"white\", width = 10)"
  },
  {
    "objectID": "introducao.html",
    "href": "introducao.html",
    "title": "Introdução",
    "section": "",
    "text": "Introdução ao R - Análise de Dados\n\nExplorando o universo do R e do RStudio®\nO R é uma linguagem de programação criada com foco em análise estatística, ciência de dados e visualização gráfica. Por ser open source e gratuito, ganhou grande espaço entre pesquisadores, analistas e profissionais que lidam com dados, oferecendo um ecossistema robusto de pacotes para diferentes tipos de análise e modelagem.\nO RStudio, por sua vez, é um ambiente gráfico que torna o uso do R mais acessível e organizado. Ele integra, em uma só interface, áreas para digitação de código, visualização de gráficos, saída de resultados, além do gerenciamento de arquivos, pacotes e variáveis. As atualizações são automáticas e o próprio sistema sugere novas versões sempre que disponíveis.\nCom o RStudio, você pode:\n\nEscrever scripts de maneira estruturada;\nProduzir visualizações de dados sofisticadas;\nElaborar relatórios dinâmicos utilizando R Markdown;\nCriar aplicações interativas com o Shiny;\nControlar pacotes e projetos com facilidade.\n\nIniciar-se no R exige familiaridade com conceitos básicos e uma boa curadoria de materiais de apoio. Felizmente, há uma ampla oferta de conteúdos de qualidade, tanto gratuitos quanto pagos, que podem acelerar o processo de aprendizado.\nUm dos recursos recomendados é o livro Introdução à Ciência de Dados com R, que aborda desde conceitos iniciais até técnicas mais avançadas de manipulação e análise de dados.\nAgora que você conhece o cenário, vamos começar pelos fundamentos: como instalar os programas, criar objetos e aplicar funções — blocos essenciais para qualquer análise mais avançada no R.\n\n\nInstalação do R e do RStudio\nPara começar a trabalhar com a linguagem, é necessário instalar tanto o R quanto o RStudio. Ambos estão disponíveis gratuitamente:\n\nDownload do R\nDownload do RStudio\n\n\n\nEntendendo Objetos e Funções\nNo R, objetos funcionam como recipientes que armazenam informações — números, textos, tabelas, entre outros tipos. A criação de um objeto é feita com o operador &lt;-. Por exemplo:\nr\nCopiarEditar\na &lt;- 1\nAqui, o objeto a está armazenando o valor 1. Dados tabulares, por exemplo, geralmente são salvos em objetos do tipo data frame, ideais para organizar colunas e linhas.\nFunções são instruções que realizam tarefas específicas. Elas são acionadas pelo nome, seguido por parênteses contendo os argumentos necessários. Se houver mais de um argumento, eles devem ser separados por vírgulas.\n\n\nClasses de Dados\nPara representar textos, utilizamos aspas. Isso indica ao R que o conteúdo é uma sequência de caracteres e não um comando. Por exemplo:\nr\nCopiarEditar\nnome &lt;- \"Fitopatologia\"\nÉ fundamental para o R saber distinguir nomes de funções e objetos de palavras comuns.\n\n\nCriando Vetores\nVetores são conjuntos de dados organizados em sequência. Para construir um vetor, usamos a função c():\nr\nCopiarEditar\nvetor1 &lt;- c(1, 2, 3, 20, 50)\nOutra forma rápida de gerar uma sequência numérica é com o operador ::\nr\nCopiarEditar\n1:5\nEsse comando cria um vetor contendo os números de 1 a 5.\n\n\nTrabalhando com Valores Ausentes (NA)\nNo R, NA é o símbolo utilizado para representar um valor desconhecido ou não registrado. Em análises estatísticas, esse tipo de valor aparece com frequência e precisa ser tratado com atenção para não comprometer os resultados.\n\n\nO operador pipe\nOs pipes (%&gt;% ou |&gt;) são uma forma elegante de encadear funções. Eles pegam o resultado de uma operação e o enviam diretamente para a próxima função, como se fosse o seu primeiro argumento. Isso torna o código mais legível e fluido.\n\n\nComentários no Código\nO caractere # permite adicionar comentários no código. Tudo que vier após esse símbolo será ignorado pelo R, servindo apenas como anotação para o programador ou outras pessoas que lerem o script.\nr\nCopiarEditar\n# Criando um vetor numérico simples vetor1 &lt;- c(1, 2, 3, 20, 50)"
  },
  {
    "objectID": "introducao.html#introdução-ao-r-e-ao-rstudio",
    "href": "introducao.html#introdução-ao-r-e-ao-rstudio",
    "title": "Introdução",
    "section": "",
    "text": "O R é uma linguagem de programação voltada para estatística, ciência de dados e análise gráfica. Gratuito e de código aberto, o R oferece uma ampla gama de recursos para manipulação, visualização e modelagem de dados, sendo amplamente utilizado por estatísticos, pesquisadores e cientistas de dados.\nO RStudio é um ambiente de desenvolvimento integrado (IDE) criado especificamente para facilitar o uso do R. Ele oferece uma interface gráfica organizada, com painéis dedicados ao código, visualização de resultados, gráficos e gerenciamento de arquivos, tornando o processo de análise de dados muito mais prático e eficiente. O RStudio é atualizado algumas vezes por ano e notifica automaticamente o usuário quando uma nova versão é lançada, eliminando a necessidade de verificação manual.\nNo RStudio, é possível:\n\nEscrever e testar scripts em R com organização e clareza;\nCriar gráficos detalhados e interativos;\nDesenvolver relatórios dinâmicos com R Markdown;\nConstruir aplicações web com o pacote Shiny;\nGerenciar pacotes, projetos e conjuntos de dados com facilidade.\n\nAo iniciar sua trajetória no universo da análise de dados com o RStudio, é fundamental compreender os primeiros passos e saber onde buscar orientação. Há uma variedade de materiais ricos em conteúdo e extremamente úteis disponíveis, e saber utilizá-los de forma estratégica pode fazer toda a diferença no aprendizado.\nEntre as principais referências, destaca-se o livro Introdução à Ciência de Dados com R. Obras como essa são valiosas para quem deseja adquirir uma base sólida na ciência de dados e aprofundar-se nas técnicas de exploração e análise utilizando o R.\nCom esse entendimento inicial, damos início à nossa jornada no R, começando pelos seus elementos mais básicos, como instalar o R e o RStudio, objetos e funções que servirão de alicerce para análises mais avançadas no futuro."
  },
  {
    "objectID": "introducao.html#baixar-r-studio",
    "href": "introducao.html#baixar-r-studio",
    "title": "Introdução",
    "section": "",
    "text": "Link para instalação do R e do R Studio."
  },
  {
    "objectID": "introducao.html#objetos-e-funções",
    "href": "introducao.html#objetos-e-funções",
    "title": "Introdução",
    "section": "",
    "text": "Objeto: Em R, um objeto é uma estrutura que armazena um valor ou conjunto de informações. Para criá-lo, utiliza-se o operador de atribuição &lt;-. Por exemplo: a &lt;- 1 cria um objeto chamado a que contém o valor 1. Bases de dados em R geralmente são armazenadas em objetos do tipo data frame, que organizam os dados em formato de tabela (linhas e colunas). Quando executamos uma função, ela realiza o conjunto de instruções programadas e retorna um resultado. As informações fornecidas para uma função são chamadas de argumentos e devem ser colocadas entre parênteses, logo após o nome da função. Caso haja mais de um argumento, eles são separados por vírgulas.\nClasses: Para trabalhar com textos, colocamos os caracteres entre aspas (” “). Isso é importante porque o R precisa distinguir entre nomes usados no código (como funções, objetos ou pacotes) e textos (letras e palavras).\nVetores: São estruturas que armazenam uma sequência ordenada de elementos. Para criar um vetor, utilizamos a função c() e inserimos os valores desejados separados por vírgulas.\nExemplo: vetor1 &lt;- c(1, 2, 3, 20, 50) cria um vetor com cinco números.\nUma forma prática de gerar vetores com valores em sequência é usando o operador :. Por exemplo, 1:5 cria um vetor com os números de 1 a 5.\n\n\nCode\nvetor1 &lt;- c(1, 2, 3, 20, 50)\n\n1:5\n\n\n[1] 1 2 3 4 5\n\n\nValores especiais – NA: É utilizado para indicar que determinada informação está ausente. Isso significa que a variável existe, mas seu conteúdo é desconhecido.\nEm termos estatísticos, o NA representa um dado faltante ou omitido. Uma situação comum em análises, quando nem todos os dados estão disponíveis ou foram registrados.\nOperador pipe: (%&gt;% ou |&gt;) tem como objetivo facilitar a leitura e a construção de sequências de comandos no R. Ele funciona passando o resultado de uma expressão para a próxima função, como se fosse o primeiro argumento dessa nova função."
  },
  {
    "objectID": "introducao.html#comentários",
    "href": "introducao.html#comentários",
    "title": "Introdução",
    "section": "",
    "text": "O R ignorará qualquer texto depois #dessa linha. Isso permite que você escreva comentários, textos que são ignorados pelo R, mas lidos por outras pessoas. Os comentários podem ser úteis para descrever brevemente o que o código a seguir faz."
  },
  {
    "objectID": "pacotes.html",
    "href": "pacotes.html",
    "title": "pacotes",
    "section": "",
    "text": "Pacotes em R são conjuntos organizados de funções, dados e documentação, reunidos em uma estrutura padrão bem definida que pode ser baixada e instalada em R. Eles servem para ampliar as capacidades do R, permitindo que usuários realizem tarefas específicas de forma mais eficiente.\nEsses pacotes podem ser obtidos de diferentes fontes, sendo as mais conhecidas: CRAN, Bioconductor e o GitHub. CRAN – é o repositório oficial do R para pacotes montados por usuários.\nOs pacotes necessários para rodar análises no R depende do objetivo do trabalho, mas alguns pacotes são básicos e essenciais para a maioria das análises, como: Tidyverse, dplyr, readxl, ggplot, etc.\n\n\nA instalação pode ser feita através do menu: tools &gt; install packages, indo em packages &gt; install e digitando o nome do pacote para baixar ou usando a função instal.packages (nome do pacote) no console. \ninstall.packages(“nome do pacote de interesse”)\nExemplo: Instalar pacote tidyverse\n\n\nCode\n#install.packages(\"tidyverse\")"
  },
  {
    "objectID": "pacotes.html#instalação-de-pacotes",
    "href": "pacotes.html#instalação-de-pacotes",
    "title": "pacotes",
    "section": "",
    "text": "A instalação pode ser feita através do menu: tools &gt; install packages, indo em packages &gt; install e digitando o nome do pacote para baixar ou usando a função instal.packages (nome do pacote) no console. \ninstall.packages(“nome do pacote de interesse”)\nExemplo: Instalar pacote tidyverse\n\n\nCode\n#install.packages(\"tidyverse\")"
  },
  {
    "objectID": "pacotes.html#usando-pacotes-sem-carregar-com-library",
    "href": "pacotes.html#usando-pacotes-sem-carregar-com-library",
    "title": "pacotes",
    "section": "Usando pacotes sem carregar com library()",
    "text": "Usando pacotes sem carregar com library()\nPode usar a função :: para acessar funções de um pacote sem carregá-lo:\n\n\nCode\n# Usar a função ggplot do pacote ggplot2 sem carregar o pacote todo\nggplot2::ggplot(data = mtcars, aes(x = wt, y = mpg)) + \n  ggplot2::geom_point()\n\n\n\n\n\n\n\n\n\nPara verificar todos os pacotes instalados:\n\n\nCode\n#installed.packages()"
  },
  {
    "objectID": "importando_dados.html",
    "href": "importando_dados.html",
    "title": "importando_dados",
    "section": "",
    "text": "O R possui diversos conjuntos de dados incorporados que podem ser utilizados para fins de prática, ensino e demonstração de análises estatísticas.\nExemplo para o pacote Orange.\nOrange é um dataset interno do R que contém informações sobre o crescimento de árvores de laranja ao longo do tempo.\n\n\nCode\ndata(\"Orange\")\n\n\nAo conjunto de dados chamado Orange está atribuindo um novo objeto chamado dados. O operador &lt;- está dizendo: “pegue os dados de Orange e salve em um objeto chamado dados”.\n\n\nCode\ndados &lt;- Orange\ndados\n\n\n   Tree  age circumference\n1     1  118            30\n2     1  484            58\n3     1  664            87\n4     1 1004           115\n5     1 1231           120\n6     1 1372           142\n7     1 1582           145\n8     2  118            33\n9     2  484            69\n10    2  664           111\n11    2 1004           156\n12    2 1231           172\n13    2 1372           203\n14    2 1582           203\n15    3  118            30\n16    3  484            51\n17    3  664            75\n18    3 1004           108\n19    3 1231           115\n20    3 1372           139\n21    3 1582           140\n22    4  118            32\n23    4  484            62\n24    4  664           112\n25    4 1004           167\n26    4 1231           179\n27    4 1372           209\n28    4 1582           214\n29    5  118            30\n30    5  484            49\n31    5  664            81\n32    5 1004           125\n33    5 1231           142\n34    5 1372           174\n35    5 1582           177\n\n\nPara salvar o conjunto de dados Orange em formato de planilha Excel (.xlsx) no R, você pode usar o pacote writexl.\n\n\nCode\nlibrary(writexl)\n\nwrite_xlsx(Orange, path = \"Orange.xlsx\")\n\n\n\n\n\nO pacote readxl é usado para a importação de planilhas do excel, e permite gerar dataframe de planilhas excel, sendo necessário ser carregado sempre antes de usar. A maioria das funções do readxl permite ler dados de planilhas excel, a exemplo de read_excel.\nO primeiro argumento para read_excel é o caminho do arquivo que deve ser lido. O caminho do arquivo e o nome do arquivo (incluindo a extensão do arquivo, .xlsx) precisam ser colocados entre aspas duplas.\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(readxl)\ndados2 &lt;- read_excel(\"dados.xlsx\",\n          sheet = \"sensibilidade_fungicidas\")\n\n#O argumento sheet = \"sensibilidade_fungicidas\" indica que você está lendo especificamente a aba (planilha) com esse nome dentro do arquivo Excel.\n\n#Ou dessa forma:\n\n#Pelo nome da aba → \"nome_da_aba\".\ndados2&lt;- read_excel(\"dados.xlsx\", \"sensibilidade_fungicidas\")\n#Pela posição da aba → 1, 2, 3, etc.\ndados2 &lt;- read_excel(\"dados.xlsx\", 2)\n\n\ndados2 &lt;- dados2 |&gt;\n\nmutate(dose = as.numeric(dose)) #O código transforma a coluna dose do objeto dados2 em formato numérico e salva essa versão modificada de volta no próprio dados2.\n\n#Isso porque na tabela a coluna dose não está como número.\n\n\n\n\n\nPara importar dados de planilhas google, usa-se a função gsheet (read_sheet), presente no pacote gsheet. A função read_gsheet lê o arquivo a partir de uma URL (link da planilha desejada).\n\n\nCode\nlibrary(gsheet)\n\nsurvey &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=1118819738#gid=1118819738\")\n\n\n\n\n\nUma forma de carregar rapidamente dados no ambiente é por meio do argumento text. Você pode copiar (ctrl + c) os dados de uma planilha e colar (ctrl + v) entre aspas (““), após o argumento text da função read.table.\nExemplo: dados &lt;- read.table(h=T, text=” “)\n\n\n\n\n\nCode\nlibrary(writexl)\n# write_xlsx() #Excel\n# write_csv() #Csv\n\n\n\n\n\nPara verificar se os dados estão corretos, pode-se utilizar a função str ou glimpse.\nA função str() em R é usada para resumir a estrutura de um objeto. Ela mostra rapidamente o tipo, tamanho e os componentes de objetos como data frames, listas ou vetores. É muito útil na exploração inicial dos dados.\nA função glimpse mostra do pacote dplyr, permite uma visão geral compacta do conjunto de dados. Ela exibe:\n\nO número de linhas e colunas;\nO tipo de dado de cada variável (por exemplo, numérico, caracterer);\nOs primeiros valores presentes em cada coluna;\n\n\n\nCode\nstr(dados2) \n\n\ntibble [240 × 9] (S3: tbl_df/tbl/data.frame)\n $ code       : chr [1:240] \"FGT05\" \"FGT05\" \"FGT05\" \"FGT05\" ...\n $ year       : num [1:240] 2007 2007 2007 2007 2007 ...\n $ trial      : num [1:240] 1 1 1 1 1 1 1 1 1 1 ...\n $ state      : chr [1:240] \"RS\" \"RS\" \"RS\" \"RS\" ...\n $ dose       : num [1:240] 0 0 0.05 0.05 0.5 0.5 1 1 5 5 ...\n $ replicate  : num [1:240] 1 2 1 2 1 2 1 2 1 2 ...\n $ germination: num [1:240] 46 44 18 24 9 11 0 0 0 0 ...\n $ ...8       : logi [1:240] NA NA NA NA NA NA ...\n $ ...9       : logi [1:240] NA NA NA NA NA NA ...\n\n\nCode\nglimpse (dados2)\n\n\nRows: 240\nColumns: 9\n$ code        &lt;chr&gt; \"FGT05\", \"FGT05\", \"FGT05\", \"FGT05\", \"FGT05\", \"FGT05\", \"FGT…\n$ year        &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n$ trial       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ state       &lt;chr&gt; \"RS\", \"RS\", \"RS\", \"RS\", \"RS\", \"RS\", \"RS\", \"RS\", \"RS\", \"RS\"…\n$ dose        &lt;dbl&gt; 0.00, 0.00, 0.05, 0.05, 0.50, 0.50, 1.00, 1.00, 5.00, 5.00…\n$ replicate   &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2…\n$ germination &lt;dbl&gt; 46, 44, 18, 24, 9, 11, 0, 0, 0, 0, 0, 0, 50, 50, 43, 44, 2…\n$ ...8        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...9        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nA funçãoskim(), do pacoteskimrfornece um resumo estatístico mais detalhado. Inclui média, mediana, mínimo, máximo, número de valores ausentes (NA).\n\n\nCode\nlibrary(skimr) #warning:false #message:false\nskim(dados2)\n\n\n\nData summary\n\n\nName\ndados2\n\n\nNumber of rows\n240\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nlogical\n2\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncode\n0\n1\n3\n5\n0\n20\n0\n\n\nstate\n0\n1\n2\n2\n0\n2\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\n…8\n240\n0\nNaN\n:\n\n\n…9\n240\n0\nNaN\n:\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n2009.45\n2.02\n2007\n2007.75\n2009.50\n2011\n2012\n▇▁▁▅▃\n\n\ntrial\n0\n1\n1.00\n0.00\n1\n1.00\n1.00\n1\n1\n▁▁▇▁▁\n\n\ndose\n0\n1\n2.76\n3.67\n0\n0.05\n0.75\n5\n10\n▇▁▂▁▂\n\n\nreplicate\n0\n1\n1.50\n0.50\n1\n1.00\n1.50\n2\n2\n▇▁▁▁▇\n\n\ngermination\n0\n1\n19.45\n19.21\n0\n0.00\n13.50\n40\n50\n▇▂▁▂▅\n\n\n\n\n\nA funçãonames()é empregada para listar os nomes reais das colunas do dataframe, permitindo identificar com clareza as variáveis disponíveis para análise.\n\n\nCode\nnames(dados2)\n\n\n[1] \"code\"        \"year\"        \"trial\"       \"state\"       \"dose\"       \n[6] \"replicate\"   \"germination\" \"...8\"        \"...9\"       \n\n\n\n\n\nExemplos:\n\n\nCode\nlibrary(gsheet)\nsurvey &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=1118819738#gid=1118819738\")\n\nsurvey |&gt;\n  group_by(state,residue) |&gt;   # agrupar dentro dessa planilha\n  count()\n\n\n# A tibble: 3 × 3\n# Groups:   state, residue [3]\n  state residue     n\n  &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;\n1 PR    &lt;NA&gt;      216\n2 RS    corn      169\n3 RS    soybean   281\n\n\nCode\nRS &lt;- survey |&gt;  # Para criar uma planilha de um subconjunto\n  filter(state == 'RS') # Filter é para selecionar linhas\nPR &lt;- survey |&gt;\n  filter(state == 'PR')\n\n\ncombinado &lt;- rbind(RS,PR) # Combinar os dois conjuntos, um embaixo do outro\n\n\n\n\nCode\nsurvey_b &lt;- survey |&gt;\n dplyr::select(year, state, species) # Para selecionar colunas de um arquivo. Usar os dois pontos para chamar o pacote dplyr sem carrega-lo.\n\nsurvey_b |&gt;\n  group_by(year,species) |&gt; \n  count() |&gt;\n  ggplot(aes(year, n, fill = species))+ # fill é o preenchimento\n  geom_col()+\n  scale_fill_manual(values = c(\"red\", \"blue\")) #fazer escolha de cor manual\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"espécies.png\") #para salvar a imagem\n\nsurvey_b |&gt;\n  group_by(year,species) |&gt; \n  count() |&gt;\n  ggplot(aes(year, n, fill = species))+ #fill é o preenchimento, se colocar color depois do fill vai adicionar cor no controno das barras dos gráficos\n  geom_col()+\n  scale_fill_viridis_d()"
  },
  {
    "objectID": "importando_dados.html#importar-um-conjunto-de-dados-que-está-disponível-no-r",
    "href": "importando_dados.html#importar-um-conjunto-de-dados-que-está-disponível-no-r",
    "title": "importando_dados",
    "section": "",
    "text": "O R possui diversos conjuntos de dados incorporados que podem ser utilizados para fins de prática, ensino e demonstração de análises estatísticas.\nExemplo para o pacote Orange.\nOrange é um dataset interno do R que contém informações sobre o crescimento de árvores de laranja ao longo do tempo.\n\n\nCode\ndata(\"Orange\")\n\n\nAo conjunto de dados chamado Orange está atribuindo um novo objeto chamado dados. O operador &lt;- está dizendo: “pegue os dados de Orange e salve em um objeto chamado dados”.\n\n\nCode\ndados &lt;- Orange\ndados\n\n\n   Tree  age circumference\n1     1  118            30\n2     1  484            58\n3     1  664            87\n4     1 1004           115\n5     1 1231           120\n6     1 1372           142\n7     1 1582           145\n8     2  118            33\n9     2  484            69\n10    2  664           111\n11    2 1004           156\n12    2 1231           172\n13    2 1372           203\n14    2 1582           203\n15    3  118            30\n16    3  484            51\n17    3  664            75\n18    3 1004           108\n19    3 1231           115\n20    3 1372           139\n21    3 1582           140\n22    4  118            32\n23    4  484            62\n24    4  664           112\n25    4 1004           167\n26    4 1231           179\n27    4 1372           209\n28    4 1582           214\n29    5  118            30\n30    5  484            49\n31    5  664            81\n32    5 1004           125\n33    5 1231           142\n34    5 1372           174\n35    5 1582           177\n\n\nPara salvar o conjunto de dados Orange em formato de planilha Excel (.xlsx) no R, você pode usar o pacote writexl.\n\n\nCode\nlibrary(writexl)\n\nwrite_xlsx(Orange, path = \"Orange.xlsx\")"
  },
  {
    "objectID": "importando_dados.html#dados-em-formato-excel",
    "href": "importando_dados.html#dados-em-formato-excel",
    "title": "importando_dados",
    "section": "",
    "text": "O pacote readxl é usado para a importação de planilhas do excel, e permite gerar dataframe de planilhas excel, sendo necessário ser carregado sempre antes de usar. A maioria das funções do readxl permite ler dados de planilhas excel, a exemplo de read_excel.\nO primeiro argumento para read_excel é o caminho do arquivo que deve ser lido. O caminho do arquivo e o nome do arquivo (incluindo a extensão do arquivo, .xlsx) precisam ser colocados entre aspas duplas.\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(readxl)\ndados2 &lt;- read_excel(\"dados.xlsx\",\n          sheet = \"sensibilidade_fungicidas\")\n\n#O argumento sheet = \"sensibilidade_fungicidas\" indica que você está lendo especificamente a aba (planilha) com esse nome dentro do arquivo Excel.\n\n#Ou dessa forma:\n\n#Pelo nome da aba → \"nome_da_aba\".\ndados2&lt;- read_excel(\"dados.xlsx\", \"sensibilidade_fungicidas\")\n#Pela posição da aba → 1, 2, 3, etc.\ndados2 &lt;- read_excel(\"dados.xlsx\", 2)\n\n\ndados2 &lt;- dados2 |&gt;\n\nmutate(dose = as.numeric(dose)) #O código transforma a coluna dose do objeto dados2 em formato numérico e salva essa versão modificada de volta no próprio dados2.\n\n#Isso porque na tabela a coluna dose não está como número."
  },
  {
    "objectID": "importando_dados.html#dados-em-planilha-google",
    "href": "importando_dados.html#dados-em-planilha-google",
    "title": "importando_dados",
    "section": "",
    "text": "Para importar dados de planilhas google, usa-se a função gsheet (read_sheet), presente no pacote gsheet. A função read_gsheet lê o arquivo a partir de uma URL (link da planilha desejada).\n\n\nCode\nlibrary(gsheet)\n\nsurvey &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=1118819738#gid=1118819738\")"
  },
  {
    "objectID": "importando_dados.html#usando-o-argumento-text",
    "href": "importando_dados.html#usando-o-argumento-text",
    "title": "importando_dados",
    "section": "",
    "text": "Uma forma de carregar rapidamente dados no ambiente é por meio do argumento text. Você pode copiar (ctrl + c) os dados de uma planilha e colar (ctrl + v) entre aspas (““), após o argumento text da função read.table.\nExemplo: dados &lt;- read.table(h=T, text=” “)"
  },
  {
    "objectID": "importando_dados.html#para-salvar-do-r-para-arquivo-excel-ou-csv",
    "href": "importando_dados.html#para-salvar-do-r-para-arquivo-excel-ou-csv",
    "title": "importando_dados",
    "section": "",
    "text": "Code\nlibrary(writexl)\n# write_xlsx() #Excel\n# write_csv() #Csv"
  },
  {
    "objectID": "importando_dados.html#para-verificar-o-conjunto-de-dados",
    "href": "importando_dados.html#para-verificar-o-conjunto-de-dados",
    "title": "importando_dados",
    "section": "",
    "text": "Para verificar se os dados estão corretos, pode-se utilizar a função str ou glimpse.\nA função str() em R é usada para resumir a estrutura de um objeto. Ela mostra rapidamente o tipo, tamanho e os componentes de objetos como data frames, listas ou vetores. É muito útil na exploração inicial dos dados.\nA função glimpse mostra do pacote dplyr, permite uma visão geral compacta do conjunto de dados. Ela exibe:\n\nO número de linhas e colunas;\nO tipo de dado de cada variável (por exemplo, numérico, caracterer);\nOs primeiros valores presentes em cada coluna;\n\n\n\nCode\nstr(dados2) \n\n\ntibble [240 × 9] (S3: tbl_df/tbl/data.frame)\n $ code       : chr [1:240] \"FGT05\" \"FGT05\" \"FGT05\" \"FGT05\" ...\n $ year       : num [1:240] 2007 2007 2007 2007 2007 ...\n $ trial      : num [1:240] 1 1 1 1 1 1 1 1 1 1 ...\n $ state      : chr [1:240] \"RS\" \"RS\" \"RS\" \"RS\" ...\n $ dose       : num [1:240] 0 0 0.05 0.05 0.5 0.5 1 1 5 5 ...\n $ replicate  : num [1:240] 1 2 1 2 1 2 1 2 1 2 ...\n $ germination: num [1:240] 46 44 18 24 9 11 0 0 0 0 ...\n $ ...8       : logi [1:240] NA NA NA NA NA NA ...\n $ ...9       : logi [1:240] NA NA NA NA NA NA ...\n\n\nCode\nglimpse (dados2)\n\n\nRows: 240\nColumns: 9\n$ code        &lt;chr&gt; \"FGT05\", \"FGT05\", \"FGT05\", \"FGT05\", \"FGT05\", \"FGT05\", \"FGT…\n$ year        &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n$ trial       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ state       &lt;chr&gt; \"RS\", \"RS\", \"RS\", \"RS\", \"RS\", \"RS\", \"RS\", \"RS\", \"RS\", \"RS\"…\n$ dose        &lt;dbl&gt; 0.00, 0.00, 0.05, 0.05, 0.50, 0.50, 1.00, 1.00, 5.00, 5.00…\n$ replicate   &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2…\n$ germination &lt;dbl&gt; 46, 44, 18, 24, 9, 11, 0, 0, 0, 0, 0, 0, 50, 50, 43, 44, 2…\n$ ...8        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...9        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nA funçãoskim(), do pacoteskimrfornece um resumo estatístico mais detalhado. Inclui média, mediana, mínimo, máximo, número de valores ausentes (NA).\n\n\nCode\nlibrary(skimr) #warning:false #message:false\nskim(dados2)\n\n\n\nData summary\n\n\nName\ndados2\n\n\nNumber of rows\n240\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nlogical\n2\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncode\n0\n1\n3\n5\n0\n20\n0\n\n\nstate\n0\n1\n2\n2\n0\n2\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\n…8\n240\n0\nNaN\n:\n\n\n…9\n240\n0\nNaN\n:\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n2009.45\n2.02\n2007\n2007.75\n2009.50\n2011\n2012\n▇▁▁▅▃\n\n\ntrial\n0\n1\n1.00\n0.00\n1\n1.00\n1.00\n1\n1\n▁▁▇▁▁\n\n\ndose\n0\n1\n2.76\n3.67\n0\n0.05\n0.75\n5\n10\n▇▁▂▁▂\n\n\nreplicate\n0\n1\n1.50\n0.50\n1\n1.00\n1.50\n2\n2\n▇▁▁▁▇\n\n\ngermination\n0\n1\n19.45\n19.21\n0\n0.00\n13.50\n40\n50\n▇▂▁▂▅\n\n\n\n\n\nA funçãonames()é empregada para listar os nomes reais das colunas do dataframe, permitindo identificar com clareza as variáveis disponíveis para análise.\n\n\nCode\nnames(dados2)\n\n\n[1] \"code\"        \"year\"        \"trial\"       \"state\"       \"dose\"       \n[6] \"replicate\"   \"germination\" \"...8\"        \"...9\""
  },
  {
    "objectID": "importando_dados.html#trabalhando-com-os-dados-importados",
    "href": "importando_dados.html#trabalhando-com-os-dados-importados",
    "title": "importando_dados",
    "section": "",
    "text": "Exemplos:\n\n\nCode\nlibrary(gsheet)\nsurvey &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=1118819738#gid=1118819738\")\n\nsurvey |&gt;\n  group_by(state,residue) |&gt;   # agrupar dentro dessa planilha\n  count()\n\n\n# A tibble: 3 × 3\n# Groups:   state, residue [3]\n  state residue     n\n  &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;\n1 PR    &lt;NA&gt;      216\n2 RS    corn      169\n3 RS    soybean   281\n\n\nCode\nRS &lt;- survey |&gt;  # Para criar uma planilha de um subconjunto\n  filter(state == 'RS') # Filter é para selecionar linhas\nPR &lt;- survey |&gt;\n  filter(state == 'PR')\n\n\ncombinado &lt;- rbind(RS,PR) # Combinar os dois conjuntos, um embaixo do outro\n\n\n\n\nCode\nsurvey_b &lt;- survey |&gt;\n dplyr::select(year, state, species) # Para selecionar colunas de um arquivo. Usar os dois pontos para chamar o pacote dplyr sem carrega-lo.\n\nsurvey_b |&gt;\n  group_by(year,species) |&gt; \n  count() |&gt;\n  ggplot(aes(year, n, fill = species))+ # fill é o preenchimento\n  geom_col()+\n  scale_fill_manual(values = c(\"red\", \"blue\")) #fazer escolha de cor manual\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"espécies.png\") #para salvar a imagem\n\nsurvey_b |&gt;\n  group_by(year,species) |&gt; \n  count() |&gt;\n  ggplot(aes(year, n, fill = species))+ #fill é o preenchimento, se colocar color depois do fill vai adicionar cor no controno das barras dos gráficos\n  geom_col()+\n  scale_fill_viridis_d()"
  },
  {
    "objectID": "visualizando_dados.html",
    "href": "visualizando_dados.html",
    "title": "graficos",
    "section": "",
    "text": "format: html editor: visual —"
  },
  {
    "objectID": "visualizando_dados.html#gráficos-em-ggplot",
    "href": "visualizando_dados.html#gráficos-em-ggplot",
    "title": "graficos",
    "section": "Gráficos em ggplot",
    "text": "Gráficos em ggplot\nO ggplot2 é o pacote usado para visualização dos dados. Ele pode ser carregado de forma independente (library(ggplot2)) ou como parte do conjunto de pacotes do tidyverse.\nA construção de gráficos com o ggplot2 segue uma lógica baseada em camadas, que são adicionadas à medida que se confecciona o gráfico, por isso que se usa o sinal de +, porque significa a adição de mais uma camada. A estrutura básica começa com a função ggplot(), que define a estética da distribuição dos dados que serão trabalhados. Em seguida, adiciona-se uma ou mais camadas com funções do tipo geom_(), que especificam o tipo de gráfico desejado, como pontos (geom_point()), linhas (geom_line()), barras (geom_bar()), entre outros.\nUsa-se o pipe (|&gt;) para enfatizar uma sequência de comandos ou ações no chunk e para evitar adicionar o nome do data frame dentro da função ggplot(). O pipe deve ter sempre um espaço antes dele e, geralmente, deve ser seguido por uma nova linha."
  },
  {
    "objectID": "visualizando_dados.html#exemplo-de-gráficos-no-ggplot2",
    "href": "visualizando_dados.html#exemplo-de-gráficos-no-ggplot2",
    "title": "graficos",
    "section": "Exemplo de gráficos no ggplot2",
    "text": "Exemplo de gráficos no ggplot2\n\nImportação de dados\n\n\nCode\nlibrary(gsheet)\n\n# usou conjunto de dados do Magnésio, planilha online. Usou o gsheet para chamar os dados do link.\ndat_mg &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=983033137#gid=983033137\")\n\n\n\n\nConstruindo o gráfico\nA primeira função para criar um ggplot é a função ggplot, que define o conjunto de dados a ser utilizado (o data frame onde os dados foram armazenados, ex. magnésio).\nA função ggplot() define o plano com os eixos x e y. Usamos a função aestetic (aes), para descrever como as variáveis são mapeadas (eixos x e y). Para adionar cor usa-se a função color = nome da cor, ou código. Ao incluir esta função dentro da função aes, dizemos ao ggplot que os pontos devem ser mapeados esteticamente utilizando cores para cada variavel."
  },
  {
    "objectID": "visualizando_dados.html#exemplos",
    "href": "visualizando_dados.html#exemplos",
    "title": "graficos",
    "section": "Exemplos:",
    "text": "Exemplos:\n\nGráficos de pontos\nA função geom_point define que a forma geométrica a ser utilizada é baseada em pontos, gerando um gráfico de dispersão. A função alpha trabalha com a tansparência.\n\n\nCode\nlibrary(ggplot2)\n\ndat_mg |&gt; \n  ggplot (aes(trat, comp, color = trat))+\n  geom_point(alpha = 1)\n\n\n\n\n\n\n\n\n\nA função shape é utilizada para adicionar diferentes formas ou tipos de marcadores para diferenciar as variáveis (exemplo: círculo e triângulo).\n\n\nCode\ndat_mg |&gt;\n  ggplot(aes(trat, comp, shape = trat, color = trat))+\n  geom_point(alpha = 1)\n\n\n\n\n\n\n\n\n\n\n\nGráficos de pontos + geom_jitter\nFrequentemente, em gráficos com muitos dados agrupados, ocorre a sobreposição de pontos, dificultando a visualização individual de cada observação. Para resolver esse problema, pode-se utilizar a função geom_jitter() no ggplot2.\nEssa função desloca ligeiramente os pontos de forma aleatória, permitindo visualizá-los de forma mais clara e separada. Para evitar uma dispersão excessiva dos dados, é possível controlar esse deslocamento por meio do argumento width, que define a largura do espalhamento horizontal. Isso garante que os pontos permaneçam próximos ao seu valor original, mas sem se sobrepor.\n\n\nCode\ndat_mg |&gt;\n  ggplot(aes(trat, comp, shape = trat, color = trat))+\n  geom_point(alpha = 1)+\n  geom_jitter(width = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nGráfico de barras\nNeste exemplo, utilizamos os pacotes ggplot2 e dplyr para gerar um gráfico de barras que representa a média da variável comppara cada tratamento (trat) no conjunto de dados dat_mg. Base dat_mg com as colunas trat (tratamentos) e comp (comprimento).\nPrimeiro, os dados são agrupados pela variável trat com a função group_by(), e em seguida é calculada a média da variável comp usando summarise(). O argumento na.rm = TRUE garante que valores ausentes (NA) sejam ignorados no cálculo da média.\nCom os dados resumidos, utilizamos a função ggplot() para construir o gráfico. A função geom_bar() com o argumento stat = \"identity\" indica que os valores do eixo y já foram previamente calculados (não devem ser contados ou somados automaticamente). O preenchimento das barras é definido por fill = trat e as bordas recebem cor preta para destacar as divisões.\nOs rótulos dos eixos são definidos com labs(), e o tema theme_minimal() é aplicado para um visual mais limpo e moderno.\nEste tipo de gráfico é útil para comparar médias entre diferentes grupos, facilitando a interpretação visual de experimentos com tratamentos ou categorias distintas.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Calcular média por tratamento\nmedia_mg &lt;- dat_mg |&gt;\n  group_by(trat) |&gt;\n  summarise(media_comp = mean(comp, na.rm = TRUE))\n\n# Gráfico de barras\nggplot(media_mg, aes(x = trat, y = media_comp, fill = trat)) +\n  geom_bar(stat = \"identity\", color = \"black\", width = 0.6) +\n  labs(x = \"Tratamento\", y = \"Média de Comprimento\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nGráfico de linhas:\nNeste trecho de código, é criado um gráfico de linhas e pontos utilizando os dados de média por tratamento (media_mg), previamente calculados.\n\n\nCode\nmedia_mg$trat &lt;- factor(media_mg$trat, levels = unique(media_mg$trat))\n\n# Criar gráfico de linhas com pontos\nggplot(media_mg, aes(x = trat, y = media_comp, group = 1)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_point(size = 3, shape = 21, fill = \"white\", color = \"steelblue\") +\n  labs(x = \"Tratamento\", y = \"Comprimento Médio\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nÉ possível adicionar título, subtítulo, rótulos dos eixos, legenda explicativa e fonte dos dados usando a função labs(). Esses elementos enriquecem o gráfico, tornando-o mais informativo e interpretável, especialmente em relatórios, artigos e apresentações.\n\n\nCode\n# Gráfico de barras com informações adicionais\nggplot(media_mg, aes(x = trat, y = media_comp, fill = trat)) +\n  geom_bar(stat = \"identity\", color = \"black\", width = 0.6) +\n  labs(\n    title = \"Média de Comprimento por Tratamento\",\n    subtitle = \"Visualização comparativa das médias da variável 'comp'\",\n    x = \"Tratamento\",\n    y = \"Média de Comprimento\",\n    caption = \"Fonte: Dados experimentais - dat_mg\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nGráfico de Boxplot\nOs boxplots são gráficos muito úteis para analisar a distribuição de uma variável numérica, especialmente quando queremos comparar essa distribuição entre diferentes grupos (por exemplo, tratamentos experimentais).\nEsse tipo de gráfico resume visualmente medidas estatísticas importantes, como:\n\nMediana (linha central da caixa);\nQuartis (limites superior e inferior da caixa);\nValores máximos e mínimos dentro dos limites definidos;\nOutliers (valores discrepantes), que aparecem como pontos fora das “linhas” do boxplot.\n\nNo R, utilizamos a função geom_boxplot() dentro do pacote ggplot2 para criar esse tipo de visualização. O eixo x deve conter uma variável categórica (como trat), e o eixo y uma variável numérica (como comp).\n\n\nCode\nlibrary(ggplot2)\n\n# Criar boxplot da variável 'comp' por tratamento\nggplot(dat_mg, aes(x = trat, y = comp, fill = trat)) +\n  geom_boxplot(outlier.color = \"red\", outlier.shape = 21, outlier.fill = \"white\") +\n  labs(\n    title = \"Distribuição da Variável 'comp' por Tratamento\",\n    subtitle = \"Boxplot representando a dispersão e tendência dos dados por grupo\",\n    x = \"Tratamento\",\n    y = \"Comprimento\",\n    caption = \"Fonte: Dados experimentais - dat_mg\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nComposição de gráficos com o pacote Patchwork\nO pacote patchwork permite combinar diferentes gráficos criados com o ggplot2 em uma única visualização. Após instalar e carregar o pacote, é necessário atribuir um nome a cada gráfico individual, o que deve ser feito no chunk correspondente a cada um.\nPara isso, escolha um nome representativo e atribua a ele o resultado da análise utilizando o banco de dados, seguido pelo uso do operador pipe (%&gt;%) para continuar a construção do gráfico.\nDepois de definir todos os gráficos separadamente, a combinação é feita utilizando seus nomes e conectando-os com o sinal de adição (+) ou com a barra vertical (|), dependendo da organização desejada (horizontal ou vertical).\nExemplo de uso do pacote patchwork:\n\n\nCode\nlibrary(ggplot2)\nlibrary(patchwork)\n\n#Simulando um conjunto de dados \nset.seed(123)\n\n\n# Simulando os dados\ndados_fitopatologia &lt;- data.frame(\n  irrigacao = rep(c(\"Gotejamento\", \"Aspersão\", \"Sulco\"), each = 30),\n  umidade = rep(c(\"Alta\", \"Moderada\"), times = 45),\n  severidade = c(\n    rnorm(30, mean = 0.6, sd = 0.1),  # Gotejamento\n    rnorm(30, mean = 0.8, sd = 0.15), # Aspersão\n    rnorm(30, mean = 0.4, sd = 0.1)   # Sulco\n  )\n)\n\n# Gráfico 1: Boxplot\ngrafico1 &lt;- dados_fitopatologia |&gt;\n  ggplot(aes(x = irrigacao, y = severidade, fill = irrigacao)) +\n  geom_boxplot() +\n  labs(\n    title = \"Severidade por tipo de irrigação\",\n    x = \"Irrigação\",\n    y = \"Severidade\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", margin = margin(b = 10)),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10)),\n    plot.margin = margin(t = 15, r = 15, b = 15, l = 15)\n  )\n\n# Gráfico 2: Jitter por umidade\ngrafico2 &lt;- dados_fitopatologia |&gt;\n  ggplot(aes(x = irrigacao, y = severidade, color = umidade)) +\n  geom_jitter(width = 0.2, size = 2) +\n  labs(\n    title = \"Severidade por irrigação e umidade\",\n    x = \"Irrigação\",\n    y = \"Severidade\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", margin = margin(b = 10)),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10)),\n    plot.margin = margin(t = 15, r = 15, b = 15, l = 15)\n  )\n\n# Composição com patchwork\n(grafico1 | grafico2) + \n  plot_annotation(\n    title = \"Análise Fitopatológica: Severidade do Oídio em Tomate\",\n    theme = theme(\n      plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5, margin = margin(b = 15))\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\nGráficos com facet_wrap()\nA função facet_wrap() no ggplot2 é usada para dividir um gráfico em vários painéis com base em uma variável categórica — ótimo para comparar subgrupos dentro dos dados.\nExemplo:\n\n\nCode\n# Simulando os dados\n\nset.seed(123)\ndados_fitopatologia &lt;- data.frame(\n  irrigacao = rep(c(\"Gotejamento\", \"Aspersão\", \"Sulco\"), each = 30),\n  umidade = rep(c(\"Alta\", \"Moderada\"), times = 45),\n  severidade = c(\n    rnorm(30, mean = 0.6, sd = 0.1),\n    rnorm(30, mean = 0.8, sd = 0.15),\n    rnorm(30, mean = 0.4, sd = 0.1)\n  )\n)\n\n# Gráfico com facet_wrap por nível de umidade\ngrafico_facetado &lt;- ggplot(dados_fitopatologia, aes(x = irrigacao, y = severidade, fill = irrigacao)) +\n  geom_boxplot() +\n  facet_wrap(~ umidade) +  # Dividindo os gráficos por nível de umidade\n  labs(\n    title = \"Severidade da doença por tipo de irrigação e umidade\",\n    x = \"Tipo de Irrigação\",\n    y = \"Severidade\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5, margin = margin(b = 12)),\n    strip.text = element_text(size = 14, face = \"bold\"),  # Título de cada painel (Alta / Moderada)\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n\n# Mostrar o gráfico\ngrafico_facetado"
  },
  {
    "objectID": "analise_estatistica.html",
    "href": "analise_estatistica.html",
    "title": "analise_estatistica",
    "section": "",
    "text": "Nesta aula, serão abordados os tipos de análise estatística em função da quantidade e da natureza das variáveis independentes (níveis dos fatores), bem como do número de tratamentos ou grupos experimentais a serem comparados.\n\n\n\nO teste t é um dos testes estatísticos mais utilizados para comparar médias entre grupos. Ele é especialmente útil quando queremos verificar se duas amostras apresentam diferenças estatisticamente significativas. Como todo teste estatístico, o teste t também tem como produto a medida do valor de p (calculado a probabilidade da diferença encontrada (entre as médias) terem sido por acaso).\nNo RStudio, esse teste pode ser realizado de forma simples usando a função t.test().\nTrabalhando um conjunto de dados e aplicando o “Teste t”.\n\n\nO teste t para amostras independentes avalia se há evidência estatística suficiente para afirmar que as médias dos dois grupos são diferentes. Esse teste assume que os dois grupos são independentes entre si, isto é, os valores de um grupo não influenciam os do outro.\nExemplo: Experimento com o objetivo de avaliar o efeito do micronutriente magnésio (Mg), adicionado na solução do solo cultivado com plantas, no manejo de uma doença. O experimento foi conduzido em delineamento inteiramente casualizado com 10 repetições. Um dos tratamentos é o controle “control” (testemunha, sem a aplicação do micronutriente. O segundo é Magnésio “Mg2”, onde houve aplicação do mineral. Em cada uma das repetições foi obtido um valor médio do comprimento (mm) de lesões.\nO conjunto de dados utilizado neste exemplo está disponível em uma planilha online no Google Sheets.Para acessar esses dados diretamente no R, foi utilizada a biblioteca gsheet, que permite importar planilhas do Google Sheets de forma prática.\n\n\nCode\n#Primeiramente, carregou-se a biblioteca com o comando:\nlibrary(gsheet)\n\n#Em seguida, os dados foram lidos a partir do link da planilha online usando a função gsheet2tbl, que converte a planilha em um data frame no R:\ndat_mg &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=983033137#gid=983033137\")\n\n\nCom os dados carregados, foi feita uma visualização gráfica utilizando a biblioteca ggplot2. O gráfico gerado é do tipo jitter plot, que ajuda a visualizar a dispersão dos dados ao longo dos diferentes tratamentos (trat) em relação à variável de interesse (comp):\n\n\nCode\nlibrary(ggplot2)\ndat_mg |&gt; \n  ggplot(aes(trat, comp))+\n  geom_jitter(width=0.1)\n\n\n\n\n\n\n\n\n\nEsse tipo de gráfico é útil para observar possíveis padrões, sobreposição de valores e variações entre os grupos experimentais.\nAgora, vamos iniciar a análise dos dados e extrair estatísticas que resumem o conjunto, tanto em relação à tendência central quanto à dispersão.\n\n\nCode\nlibrary(dplyr)\ndata2 &lt;- dat_mg |&gt;\n  group_by(trat) |&gt;\n  summarise(\n    mean_com = mean(comp),\n    sd_comp = sd(comp),\n    var_comp = var(comp),\n    n = n(),\n    se_comp = sd_comp / sqrt(n - 1),\n    ci = se_comp * qt(0,025, df = 9))\ndata2\n\n\n# A tibble: 2 × 7\n  trat    mean_com sd_comp var_comp     n se_comp    ci\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Mg2         10.5    1.54     2.39    10   0.515  -Inf\n2 control     15.7    1.27     1.61    10   0.424  -Inf\n\n\nAqui visualizaremos os dados em gráfico de barras vertical com erro padrão.\n\n\nCode\ndata2 |&gt; \n  ggplot(aes(trat, mean_com)) +\n  geom_col(width = 0.5,\n           fill = \"#99E89D\") +\n  geom_errorbar(aes(\n    ymin = mean_com - se_comp,\n    ymax = mean_com + se_comp),\n    width = 0.1) +\n  ylim(0,20) +\nlabs(x = \"Tratamentos\", y = \"Tamanho médio das lesões (mm)\")\n\n\n\n\n\n\n\n\n\nIntervalo de confiança:\n\n\nCode\ndata2 |&gt; \n  ggplot(aes(trat, mean_com)) +\n  geom_col(width = 0.5, fill = \"#99E89D\") +\n  geom_errorbar(aes(\n    ymin = mean_com - ci,\n    ymax = mean_com + ci),\n    width = 0.1) +\n  ylim(0,20) +\nlabs(x = \"Tratamentos\", y = \"Tamanho médio das lesões  (mm)\")\n\n\n\n\n\n\n\n\n\nApós importar e visualizar os dados sobre tratamentos com magnésio, procedeu-se com a transformação do formato dos dados e aplicação de um teste estatístico para comparação entre os grupos.\n\n\nInicialmente, os dados foram reorganizados com a função pivot_wider() da biblioteca tidyr, com o objetivo de converter a estrutura do dataset do formato longo (onde os valores de tratamento estão em uma única coluna) para o formato largo (em que cada tratamento ocupa uma coluna distinta com seus respectivos valores):\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)  # &lt;- Adicione isso para usar pivot_wider()\ndat_mg2 &lt;- dat_mg |&gt;\n  pivot_wider (names_from = trat, values_from = comp) |&gt; #Transformando o formato dos dados: O que era uma coluna \"longa\" vai virar várias colunas \"largas\".\n  ##   Pivot (passar de longo para largo)\n  \n  dplyr::select(-rep)\n\n\nCom isso, a coluna trat, que antes indicava os diferentes grupos de tratamento (por exemplo, “control” e “Mg2”), é desmembrada em colunas separadas, facilitando a aplicação de testes estatísticos comparativos. A coluna rep, que representa as repetições experimentais, foi removida por não ser necessária nessa etapa da análise.\nUtilizou-se a função t.test() para comparar as médias dos dois tratamentos: control (sem magnésio) e Mg2 (tratamento com magnésio):\n\n\nCode\nattach(dat_mg2)\n\nt_results &lt;- t.test(control, Mg2)\nt_results\n\n\n\n    Welch Two Sample t-test\n\ndata:  control and Mg2\nt = 8.1549, df = 17.354, p-value = 2.423e-07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 3.825607 6.490393\nsample estimates:\nmean of x mean of y \n   15.678    10.520 \n\n\n\n\n\nO resultado do teste inclui o valor de p (p-value), que representa a probabilidade de observar uma diferença entre as médias tão grande quanto (ou maior que) a encontrada nos dados, assumindo que a hipótese nula seja verdadeira.\n\nHipótese nula (H₀): as médias dos dois grupos são iguais.\n\nHipótese alternativa (H₁): as médias dos grupos são diferentes\nSe o valor de p for muito pequeno (geralmente menor que 0,05), rejeita-se a hipótese nula. Isso significa que a diferença observada entre os grupos é estatisticamente significativa, e aceita-se a hipótese alternativa: os grupos são diferentes entre si.\nPortanto, o teste t aplicado permite concluir se a aplicação de magnésio (Mg2) gerou um efeito significativo em relação ao controle.\n\n\n\nApós a realização do teste t para comparar dois tratamentos experimentais aplicados em plantas (por exemplo, controle e suplementação com magnésio), foram utilizadas ferramentas adicionais no R para interpretar, organizar e visualizar os resultados de forma mais clara e acessível.\nO pacote report foi utilizado para gerar uma interpretação automática e em linguagem natural dos resultados do teste t. Esse pacote é útil para descrever os principais resultados estatísticos em formato descritivo, ideal para relatórios e publicações.\n\n\nCode\nlibrary(report) #Pacote para explicar os resultados das análises\nreport(t_results)\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference between control and Mg2\n(mean of x = 15.68, mean of y = 10.52) suggests that the effect is positive,\nstatistically significant, and large (difference = 5.16, 95% CI [3.83, 6.49],\nt(17.35) = 8.15, p &lt; .001; Cohen's d = 3.65, 95% CI [2.14, 5.12])\n\n\nA função report() aplica uma análise interpretativa ao objeto do teste t (t_results), exibindo informações como estatísticas do teste, valor de p, médias dos grupos comparados e interpretação textual (por exemplo, se a diferença entre os grupos é significativa ou não).\nComo o objeto t_results foi gerado com os dados em formato longo, não foi necessário fazer transformações adicionais para usar a função report().\n\n\n\nOutra forma de realizar o teste t, especialmente útil para análises com estrutura de dados em formato longo, é com o pacote rstatix. Ele fornece uma sintaxe clara e compatível com o tidyverse.\nAqui, comp ~ trat indica que estamos comparando a variável de resposta comp entre os níveis da variável trat. O resultado é armazenado no objeto test, que pode ser usado para visualização.\n\n\nCode\nlibrary(rstatix)\ntest &lt;- t_test(comp ~ trat, data = dat_mg)\ntest\n\n\n# A tibble: 1 × 8\n  .y.   group1  group2    n1    n2 statistic    df           p\n* &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 comp  control Mg2       10    10      8.15  17.4 0.000000242\n\n\n\n\n\nPara complementar a análise, foi construído um gráfico boxplot com o pacote ggpubr, que facilita a geração de gráficos estatísticos com suporte a p-values.\n\n\nCode\nlibrary(ggpubr)\n\np &lt;- ggboxplot(\n  dat_mg, x = \"trat\", y = \"comp\",\n  color = \"trat\", palette = \"jco\")\nprint(p)\n\n\n\n\n\n\n\n\n\nEm seguida, o valor de p obtido no teste t foi adicionado manualmente ao gráfico com stat_pvalue_manual(), indicando se há diferença estatística entre os grupos:\nO parâmetro y.position = 18 define a altura em que o valor de p será exibido no gráfico. A função ylim(0, 20) ajusta os limites do eixo y para acomodar melhor os dados e o texto.\n\n\nCode\n#add p-value manually\np + stat_pvalue_manual(test, label = \"p\",\n  y.position = 18)+\n    ylim(0,20)+\n  labs(x = \"Tratamento\",\n       y = \"Comprimento (mm)\")\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"plot2.png\", bg = \"white\") #Por fim, o gráfico foi salvo em um arquivo de imagem.\n\n\n\n\n\nAo aplicar o teste t para comparação de médias entre dois grupos independentes, é fundamental verificar se os dados atendem às suposições básicas desse teste paramétrico. Essas suposições incluem:\n\nDistribuição normal dos dados em cada grupo\nHomogeneidade das variâncias entre os grupos\n\nO não atendimento dessas condições pode invalidar os resultados do teste ou exigir ajustes na sua aplicação.\n\n\n\nA normalidade dos dados foi avaliada para cada grupo separadamente com o teste de Shapiro-Wilk, por meio da função shapiro.test():\nEsse teste verifica se os dados seguem uma distribuição normal. A hipótese nula (H₀) é de que os dados são normalmente distribuídos.\n\nSe o valor de p &gt; 0,05, não se rejeita H₀ → os dados são considerados normalmente distribuídos;\nSe o valor de p &lt; 0,05, rejeita-se H₀ → os dados não seguem distribuição normal.\n\nAlém do teste, foram gerados histogramas para visualização da distribuição:\n\n\nCode\n#Testando a normalidade dos dados\nshapiro.test(Mg2)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Mg2\nW = 0.97269, p-value = 0.9146\n\n\nCode\nshapiro.test(control)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  control\nW = 0.93886, p-value = 0.5404\n\n\nCode\nhist(Mg2)\n\n\n\n\n\n\n\n\n\nCode\nhist(control)\n\n\n\n\n\n\n\n\n\nOs histogramas ajudam a identificar visualmente desvios da normalidade, como assimetrias ou presença de outliers.\nAnálise visual da premissa de normalidade: A análise visual da premissa de normalidade é realizada por qqplot (QQ-Plot), que permite verificar se uma amostra segue uma distribuição gaussiana. Podemos simplesmente fazer usando as funções qqnorm() e qqline() para cada umas das variáveis analisadas.\n\n\nCode\nqqnorm (Mg2)\nqqline(Mg2)\n\n\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(control)\nqqline(control)\n\n\n\n\n\n\n\n\n\n\n\n\nPara verificar se as variâncias dos dois grupos são semelhantes (homogêneas), utilizou-se o teste de F com a função var.test():\nA hipótese nula (H₀) nesse teste é de que as variâncias dos dois grupos são iguais.\n\nSe o valor de p &gt; 0,05, considera-se que as variâncias são homogêneas.\nSe o valor de p &lt; 0,05, as variâncias são diferentes (heterocedásticas).\n\n\n\nCode\n#Testando a homogeneidade das variâncias\nvar.test(dat_mg2$Mg2,\n         dat_mg2$control) \n\n\n\n    F test to compare two variances\n\ndata:  dat_mg2$Mg2 and dat_mg2$control\nF = 1.4781, num df = 9, denom df = 9, p-value = 0.5698\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3671417 5.9508644\nsample estimates:\nratio of variances \n          1.478111 \n\n\n\n\n\nSe o teste indicar variâncias homogêneas, pode-se aplicar o teste t com o argumento var.equal = TRUE (padrão).\nNo entanto, se for detectada heterogeneidade de variâncias, o teste t deve ser ajustado com:\n\n\nCode\nt_results &lt;- t.test(control, Mg2, var.equal = FALSE)\n# O parâmetro var.equal = FALSE ativa a correção de Welch, que ajusta os graus de liberdade do teste, tornando-o mais robusto quando as variâncias são diferentes.\n\n\n\n\n\n\nExemplo: Foi realizado um experimento com o objetivo de verificar o impacto do uso de uma escala na acurácia e precisão de estimativas visuais de severidade feitas por avaliadores. A hipótese testada foi de que o uso de uma escala diagramática auxilia na obtenção de estimativas mais acuradas em comparação com avaliações feitas sem esse recurso. Dez avaliadores foram selecionados aleatoriamente, e cada um realizou duas avaliações. Foram coletadas cinco variáveis relacionadas à concordância das estimativas. Como as medições foram repetidas para os mesmos avaliadores em momentos distintos, tratam-se de amostras dependentes.\n\n\nAqui, os dados são importados diretamente de uma planilha online no Google Sheets com o pacote gsheet. A planilha contém escores de acurácia medidos antes e depois de uma intervenção (avaliados como Unaided e Aided1).\n\n\nCode\n#importação dos dados\nescala &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=1729131173#gid=1729131173\")\nhead(escala)\n\n\n# A tibble: 6 × 7\n  assessment rater acuracia precisao vies_geral vies_sistematico vies_constante\n  &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 Unaided    A         0.81     0.83       0.98             1.19           0.11\n2 Unaided    B         0.72     0.73       0.99             0.92          -0.11\n3 Unaided    C         0.4      0.71       0.78             1.16           0.73\n4 Unaided    D         0.82     0.82       1                0.95          -0.01\n5 Unaided    E         0.75     0.75       0.99             1.1            0.07\n6 Unaided    F         0.45     0.75       0.92             0.8            0.34\n\n\n\n\n\n\nEsse teste verifica se houve diferença significativa entre os dois momentos de avaliação (antes e depois da ajuda), utilizando o t_test() do pacote rstatix.\nA opção paired = TRUE indica que são as mesmas pessoas avaliadas duas vezes, e var.equal = FALSE corrige o teste caso as variâncias sejam diferentes (teste de Welch).\n\n\nCode\ntest &lt;- t_test(acuracia ~ assessment,\n       data = escala,\n       paired = TRUE,\n       var.equal = FALSE)\ntest\n\n\n# A tibble: 1 × 8\n  .y.      group1 group2     n1    n2 statistic    df       p\n* &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 acuracia Aided1 Unaided    10    10      4.42     9 0.00167\n\n\nVisualização: É gerado um boxplot para visualizar a distribuição das acurácias por tipo de avaliação (Unaided vs Aided1).\n\n\nCode\nescala |&gt; \n  ggplot(aes(assessment, acuracia))+\n         geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nunaided &lt;- escala |&gt;\n  filter(assessment == \"Unaided\") |&gt;\n  pull(acuracia)\n\naided &lt;- escala |&gt;\n  filter(assessment == \"Aided\") |&gt;\n  pull(acuracia)\n\n\nif (length(unaided) &gt;= 2 && length(aided) &gt;= 2) {\n  var.test(unaided, aided) # Homogeneidade de variâncias\n} else {\n  message(\"Número de observações insuficiente em um dos grupos.\")\n}\n\nhist(unaided)\n\n\n\n\n\n\n\n\n\nCode\nif (length(aided) &gt; 1) {\n  hist(aided)\n} else {\n  message(\"Não há dados suficientes para gerar o histograma de 'aided'.\")\n}     # Visualização da distribuição\n\nshapiro.test(unaided) # Normalidade\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  unaided\nW = 0.7748, p-value = 0.007155\n\n\nVerifica se os dados atendem às premissas do teste t (normalidade e variâncias iguais).\n\nSe normalidade falha → usar teste não paramétrico.\nSe variâncias são diferentes, continuar com var.equal = FALSE.\n\n\n\n\n\nUm teste não paramétrico não faz nenhuma suposição sobre a distribuição da população ou tamanho da amostra. O Wilcox.test é o teste para dados não paramétricos equivalente ao teste t para dados paramétricos. o teste de Wilcoxon é usado para testar se as medianas das amostras são iguais nos casos em que a suposição de normalidade não é satisfeita ou quando não for possível checar essa suposição.\nUsar Wilcoxon se os dados não forem normais. O primeiro é o equivalente ao t pareado, o segundo ao t para amostras independentes.\n\n\nCode\n# wilcox.test(unaided, aided)             # Wilcoxon pareado\n# wilcox.test(unaided, aided, paired = FALSE) # Mann-Whitney (independente)"
  },
  {
    "objectID": "analise_estatistica.html#tipos-de-testes-para-análise-estatística",
    "href": "analise_estatistica.html#tipos-de-testes-para-análise-estatística",
    "title": "analise_estatistica",
    "section": "",
    "text": "Nesta aula, serão abordados os tipos de análise estatística em função da quantidade e da natureza das variáveis independentes (níveis dos fatores), bem como do número de tratamentos ou grupos experimentais a serem comparados."
  },
  {
    "objectID": "analise_estatistica.html#teste-t",
    "href": "analise_estatistica.html#teste-t",
    "title": "analise_estatistica",
    "section": "",
    "text": "O teste t é um dos testes estatísticos mais utilizados para comparar médias entre grupos. Ele é especialmente útil quando queremos verificar se duas amostras apresentam diferenças estatisticamente significativas. Como todo teste estatístico, o teste t também tem como produto a medida do valor de p (calculado a probabilidade da diferença encontrada (entre as médias) terem sido por acaso).\nNo RStudio, esse teste pode ser realizado de forma simples usando a função t.test().\nTrabalhando um conjunto de dados e aplicando o “Teste t”.\n\n\nO teste t para amostras independentes avalia se há evidência estatística suficiente para afirmar que as médias dos dois grupos são diferentes. Esse teste assume que os dois grupos são independentes entre si, isto é, os valores de um grupo não influenciam os do outro.\nExemplo: Experimento com o objetivo de avaliar o efeito do micronutriente magnésio (Mg), adicionado na solução do solo cultivado com plantas, no manejo de uma doença. O experimento foi conduzido em delineamento inteiramente casualizado com 10 repetições. Um dos tratamentos é o controle “control” (testemunha, sem a aplicação do micronutriente. O segundo é Magnésio “Mg2”, onde houve aplicação do mineral. Em cada uma das repetições foi obtido um valor médio do comprimento (mm) de lesões.\nO conjunto de dados utilizado neste exemplo está disponível em uma planilha online no Google Sheets.Para acessar esses dados diretamente no R, foi utilizada a biblioteca gsheet, que permite importar planilhas do Google Sheets de forma prática.\n\n\nCode\n#Primeiramente, carregou-se a biblioteca com o comando:\nlibrary(gsheet)\n\n#Em seguida, os dados foram lidos a partir do link da planilha online usando a função gsheet2tbl, que converte a planilha em um data frame no R:\ndat_mg &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=983033137#gid=983033137\")\n\n\nCom os dados carregados, foi feita uma visualização gráfica utilizando a biblioteca ggplot2. O gráfico gerado é do tipo jitter plot, que ajuda a visualizar a dispersão dos dados ao longo dos diferentes tratamentos (trat) em relação à variável de interesse (comp):\n\n\nCode\nlibrary(ggplot2)\ndat_mg |&gt; \n  ggplot(aes(trat, comp))+\n  geom_jitter(width=0.1)\n\n\n\n\n\n\n\n\n\nEsse tipo de gráfico é útil para observar possíveis padrões, sobreposição de valores e variações entre os grupos experimentais.\nAgora, vamos iniciar a análise dos dados e extrair estatísticas que resumem o conjunto, tanto em relação à tendência central quanto à dispersão.\n\n\nCode\nlibrary(dplyr)\ndata2 &lt;- dat_mg |&gt;\n  group_by(trat) |&gt;\n  summarise(\n    mean_com = mean(comp),\n    sd_comp = sd(comp),\n    var_comp = var(comp),\n    n = n(),\n    se_comp = sd_comp / sqrt(n - 1),\n    ci = se_comp * qt(0,025, df = 9))\ndata2\n\n\n# A tibble: 2 × 7\n  trat    mean_com sd_comp var_comp     n se_comp    ci\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Mg2         10.5    1.54     2.39    10   0.515  -Inf\n2 control     15.7    1.27     1.61    10   0.424  -Inf\n\n\nAqui visualizaremos os dados em gráfico de barras vertical com erro padrão.\n\n\nCode\ndata2 |&gt; \n  ggplot(aes(trat, mean_com)) +\n  geom_col(width = 0.5,\n           fill = \"#99E89D\") +\n  geom_errorbar(aes(\n    ymin = mean_com - se_comp,\n    ymax = mean_com + se_comp),\n    width = 0.1) +\n  ylim(0,20) +\nlabs(x = \"Tratamentos\", y = \"Tamanho médio das lesões (mm)\")\n\n\n\n\n\n\n\n\n\nIntervalo de confiança:\n\n\nCode\ndata2 |&gt; \n  ggplot(aes(trat, mean_com)) +\n  geom_col(width = 0.5, fill = \"#99E89D\") +\n  geom_errorbar(aes(\n    ymin = mean_com - ci,\n    ymax = mean_com + ci),\n    width = 0.1) +\n  ylim(0,20) +\nlabs(x = \"Tratamentos\", y = \"Tamanho médio das lesões  (mm)\")\n\n\n\n\n\n\n\n\n\nApós importar e visualizar os dados sobre tratamentos com magnésio, procedeu-se com a transformação do formato dos dados e aplicação de um teste estatístico para comparação entre os grupos.\n\n\nInicialmente, os dados foram reorganizados com a função pivot_wider() da biblioteca tidyr, com o objetivo de converter a estrutura do dataset do formato longo (onde os valores de tratamento estão em uma única coluna) para o formato largo (em que cada tratamento ocupa uma coluna distinta com seus respectivos valores):\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)  # &lt;- Adicione isso para usar pivot_wider()\ndat_mg2 &lt;- dat_mg |&gt;\n  pivot_wider (names_from = trat, values_from = comp) |&gt; #Transformando o formato dos dados: O que era uma coluna \"longa\" vai virar várias colunas \"largas\".\n  ##   Pivot (passar de longo para largo)\n  \n  dplyr::select(-rep)\n\n\nCom isso, a coluna trat, que antes indicava os diferentes grupos de tratamento (por exemplo, “control” e “Mg2”), é desmembrada em colunas separadas, facilitando a aplicação de testes estatísticos comparativos. A coluna rep, que representa as repetições experimentais, foi removida por não ser necessária nessa etapa da análise.\nUtilizou-se a função t.test() para comparar as médias dos dois tratamentos: control (sem magnésio) e Mg2 (tratamento com magnésio):\n\n\nCode\nattach(dat_mg2)\n\nt_results &lt;- t.test(control, Mg2)\nt_results\n\n\n\n    Welch Two Sample t-test\n\ndata:  control and Mg2\nt = 8.1549, df = 17.354, p-value = 2.423e-07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 3.825607 6.490393\nsample estimates:\nmean of x mean of y \n   15.678    10.520 \n\n\n\n\n\nO resultado do teste inclui o valor de p (p-value), que representa a probabilidade de observar uma diferença entre as médias tão grande quanto (ou maior que) a encontrada nos dados, assumindo que a hipótese nula seja verdadeira.\n\nHipótese nula (H₀): as médias dos dois grupos são iguais.\n\nHipótese alternativa (H₁): as médias dos grupos são diferentes\nSe o valor de p for muito pequeno (geralmente menor que 0,05), rejeita-se a hipótese nula. Isso significa que a diferença observada entre os grupos é estatisticamente significativa, e aceita-se a hipótese alternativa: os grupos são diferentes entre si.\nPortanto, o teste t aplicado permite concluir se a aplicação de magnésio (Mg2) gerou um efeito significativo em relação ao controle.\n\n\n\nApós a realização do teste t para comparar dois tratamentos experimentais aplicados em plantas (por exemplo, controle e suplementação com magnésio), foram utilizadas ferramentas adicionais no R para interpretar, organizar e visualizar os resultados de forma mais clara e acessível.\nO pacote report foi utilizado para gerar uma interpretação automática e em linguagem natural dos resultados do teste t. Esse pacote é útil para descrever os principais resultados estatísticos em formato descritivo, ideal para relatórios e publicações.\n\n\nCode\nlibrary(report) #Pacote para explicar os resultados das análises\nreport(t_results)\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference between control and Mg2\n(mean of x = 15.68, mean of y = 10.52) suggests that the effect is positive,\nstatistically significant, and large (difference = 5.16, 95% CI [3.83, 6.49],\nt(17.35) = 8.15, p &lt; .001; Cohen's d = 3.65, 95% CI [2.14, 5.12])\n\n\nA função report() aplica uma análise interpretativa ao objeto do teste t (t_results), exibindo informações como estatísticas do teste, valor de p, médias dos grupos comparados e interpretação textual (por exemplo, se a diferença entre os grupos é significativa ou não).\nComo o objeto t_results foi gerado com os dados em formato longo, não foi necessário fazer transformações adicionais para usar a função report().\n\n\n\nOutra forma de realizar o teste t, especialmente útil para análises com estrutura de dados em formato longo, é com o pacote rstatix. Ele fornece uma sintaxe clara e compatível com o tidyverse.\nAqui, comp ~ trat indica que estamos comparando a variável de resposta comp entre os níveis da variável trat. O resultado é armazenado no objeto test, que pode ser usado para visualização.\n\n\nCode\nlibrary(rstatix)\ntest &lt;- t_test(comp ~ trat, data = dat_mg)\ntest\n\n\n# A tibble: 1 × 8\n  .y.   group1  group2    n1    n2 statistic    df           p\n* &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 comp  control Mg2       10    10      8.15  17.4 0.000000242\n\n\n\n\n\nPara complementar a análise, foi construído um gráfico boxplot com o pacote ggpubr, que facilita a geração de gráficos estatísticos com suporte a p-values.\n\n\nCode\nlibrary(ggpubr)\n\np &lt;- ggboxplot(\n  dat_mg, x = \"trat\", y = \"comp\",\n  color = \"trat\", palette = \"jco\")\nprint(p)\n\n\n\n\n\n\n\n\n\nEm seguida, o valor de p obtido no teste t foi adicionado manualmente ao gráfico com stat_pvalue_manual(), indicando se há diferença estatística entre os grupos:\nO parâmetro y.position = 18 define a altura em que o valor de p será exibido no gráfico. A função ylim(0, 20) ajusta os limites do eixo y para acomodar melhor os dados e o texto.\n\n\nCode\n#add p-value manually\np + stat_pvalue_manual(test, label = \"p\",\n  y.position = 18)+\n    ylim(0,20)+\n  labs(x = \"Tratamento\",\n       y = \"Comprimento (mm)\")\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"plot2.png\", bg = \"white\") #Por fim, o gráfico foi salvo em um arquivo de imagem.\n\n\n\n\n\nAo aplicar o teste t para comparação de médias entre dois grupos independentes, é fundamental verificar se os dados atendem às suposições básicas desse teste paramétrico. Essas suposições incluem:\n\nDistribuição normal dos dados em cada grupo\nHomogeneidade das variâncias entre os grupos\n\nO não atendimento dessas condições pode invalidar os resultados do teste ou exigir ajustes na sua aplicação.\n\n\n\nA normalidade dos dados foi avaliada para cada grupo separadamente com o teste de Shapiro-Wilk, por meio da função shapiro.test():\nEsse teste verifica se os dados seguem uma distribuição normal. A hipótese nula (H₀) é de que os dados são normalmente distribuídos.\n\nSe o valor de p &gt; 0,05, não se rejeita H₀ → os dados são considerados normalmente distribuídos;\nSe o valor de p &lt; 0,05, rejeita-se H₀ → os dados não seguem distribuição normal.\n\nAlém do teste, foram gerados histogramas para visualização da distribuição:\n\n\nCode\n#Testando a normalidade dos dados\nshapiro.test(Mg2)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Mg2\nW = 0.97269, p-value = 0.9146\n\n\nCode\nshapiro.test(control)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  control\nW = 0.93886, p-value = 0.5404\n\n\nCode\nhist(Mg2)\n\n\n\n\n\n\n\n\n\nCode\nhist(control)\n\n\n\n\n\n\n\n\n\nOs histogramas ajudam a identificar visualmente desvios da normalidade, como assimetrias ou presença de outliers.\nAnálise visual da premissa de normalidade: A análise visual da premissa de normalidade é realizada por qqplot (QQ-Plot), que permite verificar se uma amostra segue uma distribuição gaussiana. Podemos simplesmente fazer usando as funções qqnorm() e qqline() para cada umas das variáveis analisadas.\n\n\nCode\nqqnorm (Mg2)\nqqline(Mg2)\n\n\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(control)\nqqline(control)\n\n\n\n\n\n\n\n\n\n\n\n\nPara verificar se as variâncias dos dois grupos são semelhantes (homogêneas), utilizou-se o teste de F com a função var.test():\nA hipótese nula (H₀) nesse teste é de que as variâncias dos dois grupos são iguais.\n\nSe o valor de p &gt; 0,05, considera-se que as variâncias são homogêneas.\nSe o valor de p &lt; 0,05, as variâncias são diferentes (heterocedásticas).\n\n\n\nCode\n#Testando a homogeneidade das variâncias\nvar.test(dat_mg2$Mg2,\n         dat_mg2$control) \n\n\n\n    F test to compare two variances\n\ndata:  dat_mg2$Mg2 and dat_mg2$control\nF = 1.4781, num df = 9, denom df = 9, p-value = 0.5698\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3671417 5.9508644\nsample estimates:\nratio of variances \n          1.478111 \n\n\n\n\n\nSe o teste indicar variâncias homogêneas, pode-se aplicar o teste t com o argumento var.equal = TRUE (padrão).\nNo entanto, se for detectada heterogeneidade de variâncias, o teste t deve ser ajustado com:\n\n\nCode\nt_results &lt;- t.test(control, Mg2, var.equal = FALSE)\n# O parâmetro var.equal = FALSE ativa a correção de Welch, que ajusta os graus de liberdade do teste, tornando-o mais robusto quando as variâncias são diferentes.\n\n\n\n\n\n\nExemplo: Foi realizado um experimento com o objetivo de verificar o impacto do uso de uma escala na acurácia e precisão de estimativas visuais de severidade feitas por avaliadores. A hipótese testada foi de que o uso de uma escala diagramática auxilia na obtenção de estimativas mais acuradas em comparação com avaliações feitas sem esse recurso. Dez avaliadores foram selecionados aleatoriamente, e cada um realizou duas avaliações. Foram coletadas cinco variáveis relacionadas à concordância das estimativas. Como as medições foram repetidas para os mesmos avaliadores em momentos distintos, tratam-se de amostras dependentes.\n\n\nAqui, os dados são importados diretamente de uma planilha online no Google Sheets com o pacote gsheet. A planilha contém escores de acurácia medidos antes e depois de uma intervenção (avaliados como Unaided e Aided1).\n\n\nCode\n#importação dos dados\nescala &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=1729131173#gid=1729131173\")\nhead(escala)\n\n\n# A tibble: 6 × 7\n  assessment rater acuracia precisao vies_geral vies_sistematico vies_constante\n  &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 Unaided    A         0.81     0.83       0.98             1.19           0.11\n2 Unaided    B         0.72     0.73       0.99             0.92          -0.11\n3 Unaided    C         0.4      0.71       0.78             1.16           0.73\n4 Unaided    D         0.82     0.82       1                0.95          -0.01\n5 Unaided    E         0.75     0.75       0.99             1.1            0.07\n6 Unaided    F         0.45     0.75       0.92             0.8            0.34\n\n\n\n\n\n\nEsse teste verifica se houve diferença significativa entre os dois momentos de avaliação (antes e depois da ajuda), utilizando o t_test() do pacote rstatix.\nA opção paired = TRUE indica que são as mesmas pessoas avaliadas duas vezes, e var.equal = FALSE corrige o teste caso as variâncias sejam diferentes (teste de Welch).\n\n\nCode\ntest &lt;- t_test(acuracia ~ assessment,\n       data = escala,\n       paired = TRUE,\n       var.equal = FALSE)\ntest\n\n\n# A tibble: 1 × 8\n  .y.      group1 group2     n1    n2 statistic    df       p\n* &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 acuracia Aided1 Unaided    10    10      4.42     9 0.00167\n\n\nVisualização: É gerado um boxplot para visualizar a distribuição das acurácias por tipo de avaliação (Unaided vs Aided1).\n\n\nCode\nescala |&gt; \n  ggplot(aes(assessment, acuracia))+\n         geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nunaided &lt;- escala |&gt;\n  filter(assessment == \"Unaided\") |&gt;\n  pull(acuracia)\n\naided &lt;- escala |&gt;\n  filter(assessment == \"Aided\") |&gt;\n  pull(acuracia)\n\n\nif (length(unaided) &gt;= 2 && length(aided) &gt;= 2) {\n  var.test(unaided, aided) # Homogeneidade de variâncias\n} else {\n  message(\"Número de observações insuficiente em um dos grupos.\")\n}\n\nhist(unaided)\n\n\n\n\n\n\n\n\n\nCode\nif (length(aided) &gt; 1) {\n  hist(aided)\n} else {\n  message(\"Não há dados suficientes para gerar o histograma de 'aided'.\")\n}     # Visualização da distribuição\n\nshapiro.test(unaided) # Normalidade\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  unaided\nW = 0.7748, p-value = 0.007155\n\n\nVerifica se os dados atendem às premissas do teste t (normalidade e variâncias iguais).\n\nSe normalidade falha → usar teste não paramétrico.\nSe variâncias são diferentes, continuar com var.equal = FALSE.\n\n\n\n\n\nUm teste não paramétrico não faz nenhuma suposição sobre a distribuição da população ou tamanho da amostra. O Wilcox.test é o teste para dados não paramétricos equivalente ao teste t para dados paramétricos. o teste de Wilcoxon é usado para testar se as medianas das amostras são iguais nos casos em que a suposição de normalidade não é satisfeita ou quando não for possível checar essa suposição.\nUsar Wilcoxon se os dados não forem normais. O primeiro é o equivalente ao t pareado, o segundo ao t para amostras independentes.\n\n\nCode\n# wilcox.test(unaided, aided)             # Wilcoxon pareado\n# wilcox.test(unaided, aided, paired = FALSE) # Mann-Whitney (independente)"
  },
  {
    "objectID": "analise_estatistica.html#transformação-logarítmica",
    "href": "analise_estatistica.html#transformação-logarítmica",
    "title": "analise_estatistica",
    "section": "Transformação logarítmica",
    "text": "Transformação logarítmica\nA transformação logarítmica é uma técnica comum utilizada na análise de dados para lidar com variáveis que apresentam distribuição assimétrica ou variância heterogênea. Ela pode ajudar a estabilizar a variância, aproximar os dados de uma distribuição normal e melhorar a interpretação dos resultados estatísticos.\nNo RStudio, a transformação logarítmica pode ser aplicada facilmente com funções como log(), log10() (logaritmo na base 10) ou log2() (logaritmo na base 2), dependendo do contexto da análise. Podemos realizar essa transformação com o uso da função mutate. Através da função mutate() realizamos a criação/adição de uma nova variável (ou novas variaveis), que são funções de variáveis existentes, e também criamos/modificamos colunas.\n\n\nCode\nmofo2 &lt;- mofo |&gt;\n  mutate (scl2 = log(scl))\n  mofo2\n\n\n# A tibble: 52 × 6\n   study treat   inc   scl   yld  scl2\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     1    76  2194  2265  7.69\n 2     1     2    53  1663  2618  7.42\n 3     1     3    42  1313  2554  7.18\n 4     1     4    37  1177  2632  7.07\n 5     1     5    29   753  2820  6.62\n 6     1     6    42  1343  2799  7.20\n 7     1     7    55  1519  2503  7.33\n 8     1     8    40   516  2967  6.25\n 9     1     9    26   643  2965  6.47\n10     1    10    18   400  3088  5.99\n# ℹ 42 more rows\n\n\n\nVisualizar os dados tranformados\nHistograma\n\n\nCode\nmofo2 |&gt;\n  ggplot(aes(scl2)) +\n  geom_histogram(bins = 10, fill = \"#1A8C8C\", color = \"black\")"
  },
  {
    "objectID": "analise_estatistica.html#transformação-em-raiz-quadrada",
    "href": "analise_estatistica.html#transformação-em-raiz-quadrada",
    "title": "analise_estatistica",
    "section": "Transformação em raiz quadrada",
    "text": "Transformação em raiz quadrada\nA transformação em raiz quadrada é uma técnica estatística utilizada para corrigir assimetrias nos dados e estabilizar a variância, especialmente quando os dados representam contagens ou variáveis discretas com distribuição assimétrica.\nEsse tipo de transformação é útil quando os dados apresentam variância crescente com a média, o que viola pressupostos importantes de muitos testes estatísticos, como a ANOVA e o teste t.\nNo RStudio, a transformação em raiz quadrada pode ser feita com a função sqrt():\n\n\nCode\nmofo2 &lt;- mofo |&gt;\n  mutate (scl2 = sqrt(scl))\n  mofo2\n\n\n# A tibble: 52 × 6\n   study treat   inc   scl   yld  scl2\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     1    76  2194  2265  46.8\n 2     1     2    53  1663  2618  40.8\n 3     1     3    42  1313  2554  36.2\n 4     1     4    37  1177  2632  34.3\n 5     1     5    29   753  2820  27.4\n 6     1     6    42  1343  2799  36.6\n 7     1     7    55  1519  2503  39.0\n 8     1     8    40   516  2967  22.7\n 9     1     9    26   643  2965  25.4\n10     1    10    18   400  3088  20  \n# ℹ 42 more rows\n\n\n\nVisualizar os dados tranformados\nHistograma\n\n\nCode\n  mofo2 |&gt;\n    ggplot(aes(scl2))+\n    geom_histogram(bins = 10, fill = \"#1A8C8C\", color = \"black\")"
  },
  {
    "objectID": "analise_estatistica.html#transformação-de-dados-box-cox",
    "href": "analise_estatistica.html#transformação-de-dados-box-cox",
    "title": "analise_estatistica",
    "section": "Transformação de dados Box-Cox",
    "text": "Transformação de dados Box-Cox\nA transformação de Box-Cox é uma técnica estatística utilizada para estabilizar a variância e aproximar os dados de uma distribuição normal. Diferente de outras transformações fixas, como log ou raiz quadrada, a Box-Cox aplica uma família de transformações parametrizadas, permitindo encontrar automaticamente o melhor ajuste aos pressupostos dos modelos estatísticos.\nEla é especialmente útil quando não se sabe previamente qual transformação aplicar, pois estima um parâmetro lambda (λ) que define a forma ideal da transformação.\nA transformação de Box-Cox é definida pela seguinte equação: y(lambda) = (x^lambda - 1) / lambda\nNessa equação, “x” representa a variável original, “y(lambda)” representa a variável transformada para um determinado valor de lambda e “lambda” é o parâmetro de transformação que varia de -∞ a +∞. O valor de lambda determina o tipo de transformação aplicada: Se lambda = 0, a transformação de Box-Cox é equivalente ao logaritmo natural (ln). Se lambda = 1, a transformação de Box-Cox é equivalente à transformação linear (sem transformação). Se lambda &lt; 0, é aplicada uma transformação inversa.\nA transformação Box-Cox pode ser aplicada usando a função boxcox() do pacote MASS.\n\n\nCode\nlibrary(MASS)\n\n\nExemplo: InsectSprays, do próprio R. A função boxcox() pode ser utilizada para calcular a transformação de Box-Cox e identificar o valor de lambda ótimo para uma determinada variável. Essa função retorna uma lista de resultados, incluindo o valor de lambda ótimo e gráficos de diagnóstico.\n\nImportando dados\n\n\nCode\ninsects &lt;- InsectSprays\n\nb &lt;- boxcox(lm(insects$count+0.1 ~1))\n\n\n\n\n\n\n\n\n\n\n\nCode\nlambda &lt;- b$x[which.max(b$y)]\nlambda\n\n\n[1] 0.4242424\n\n\n\n\nCode\ninsects$count2 &lt;- (insects$count ^ lambda - 1) / lambda\n\nhist(insects$count, \n     col = \"#1A8C8C\",        # cor do histograma\n     main = \"Histograma de Count Transformado\", \n     xlab = \"Contagem Transformada\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nhist(insects$count2,\n     col = \"#1A8C8C\",\n     main = \"Histograma de Count2 Transformado\", \n     xlab = \"Contagem Transformada\"\n)"
  },
  {
    "objectID": "analise_estatistica.html#anova-com-1-fator-one-way-anova",
    "href": "analise_estatistica.html#anova-com-1-fator-one-way-anova",
    "title": "analise_estatistica",
    "section": "Anova com 1 fator (One-way Anova)",
    "text": "Anova com 1 fator (One-way Anova)\nÉ uma técnica estatística utilizada para comparar as médias de três ou mais grupos que diferem em relação a um único fator (ou variável independente). Esse fator pode representar, por exemplo, diferentes tratamentos, cultivares, doses de um produto ou condições experimentais.\nO objetivo é verificar se há diferença significativa entre as médias dos grupos. A hipótese nula assume que todas as médias são iguais, enquanto a hipótese alternativa indica que pelo menos uma delas é diferente.\nExemplo: Experimento com um fator e em delineamento inteiramente casualizado (DIC) para comparar o crescimento micelial de diferentes espécies de um fungo fitopatogênico. A resposta a ser estudada é a TCM = taxa de crescimento micelial.\n\nImportando o conjunto de dados:\n\n\nCode\nlibrary(readxl)\nmicelial &lt;- read_excel(\"dados.xlsx\", \"micelial\")\nhead(micelial)\n\n\n# A tibble: 6 × 3\n  especie   rep   tcm\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Fasi        1  1.5 \n2 Fasi        2  1.59\n3 Fasi        3  1.52\n4 Fasi        4  1.52\n5 Fasi        5  1.6 \n6 Fasi        6  1.7 \n\n\n\n\nCarregando pacotes:\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\n\nVisualização dos dados:\n\n\nCode\nmicelial |&gt;\n  ggplot(aes(x = especie, y = tcm)) +\n  geom_boxplot(utlier.color = NA, fill = \"#1A8C8C\", color = \"black\") +\n   geom_jitter(width = 0.1)+\n  labs(\n    x = \"Espécie Fúngica\",\n    y = \"Taxa de Crescimento Micelial\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nModelo usando aov()\nPara verificar os dados usando anova, um novo modelo para atribuir a função aov() contendo os argumentos tratamento em função da variável resposta deve ser criado (ex.: tcm ~ espécie), o banco de dados referido deve ser enunciado após o argumento separado por vírgula seguido do nome data = nome do conjunto de dados (ex.: micelial). Depois disso, pede um quadro de resumo do novo modelo criado.\n\n\nCode\naov1 &lt;- aov(tcm ~ especie, data = micelial)\nsummary(aov1)\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nespecie      4 1.4696  0.3674   19.63 2.03e-07 ***\nResiduals   25 0.4679  0.0187                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\naov2 &lt;- lm(tcm ~ especie, data = micelial) # Outra forma de fazer a ANOVA\naov2\n\n\n\nCall:\nlm(formula = tcm ~ especie, data = micelial)\n\nCoefficients:\n(Intercept)  especieFaus  especieFcor  especieFgra  especieFmer  \n      1.572       -0.335       -0.250       -0.660       -0.145  \n\n\n\nTestando as premissas\n\nTestes de Normalidade e Homocedasticidade\nTeste de Normalidade\nA normalidade dos dados é uma condição importante para muitos testes estatísticos. Ela garante que os resultados das análises, como ANOVA e teste t, sejam confiáveis, pois esses métodos assumem que os dados vêm de uma população com distribuição normal.\nTeste de Homocedasticidade\nNa ANOVA, é necessário que os grupos comparados tenham variâncias semelhantes. Essa condição é chamada de homocedasticidade. Se as variâncias forem muito diferentes (heterocedasticidade), os resultados do teste F podem ser comprometidos.\nPara testar as premissas, é necessário instalar e carregar o pacote performance e o pacote DHARMa.\nO pacote performance permite checar as premissas (check_), já o pacote DHARMA (Distributed Hierarchical Accumulation of Residuals for Generalized Linear Models in R) é para visualizar os dados pelo diagnóstico do resíduo. O pacote DHARMa permite faz uma comparação dos resíduos simulados, que são gerados pelo pacote, com os resíduos observados e ver graficamente quando a distribuição dos dados não é normal e/ou quando há variação heterocedástica.\nApós isso, deve-se fazer o teste de normalidade dos resíduos com a interação entre a anova e os resíduos.\n\n\nCode\nlibrary(performance)\ncheck_heteroscedasticity(aov1)\n\n\nOK: Error variance appears to be homoscedastic (p = 0.880).\n\n\n\n\nCode\ncheck_normality(aov1)\n\n\nOK: residuals appear as normally distributed (p = 0.878).\n\n\n\n\nCode\nlibrary(DHARMa)\nhist (aov1$residuals) #Ou hist(residuals(aov1))\n\n\n\n\n\n\n\n\n\nCode\n# Mostra a distribuição visual dos resíduos.\n\n\n\n\nCode\nqqnorm(aov1$residuals)\nqqline(aov1$residuals)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(simulateResiduals(aov1))\n\n\n\n\n\n\n\n\n\n\n\nCode\nshapiro.test(aov1$residuals) #Ou shapiro.test(residuals(aov1)) \n\n\n\n    Shapiro-Wilk normality test\n\ndata:  aov1$residuals\nW = 0.9821, p-value = 0.8782\n\n\nO teste verifica a seguinte hipótese:\n\nHipótese nula (H₀): Os dados seguem distribuição normal;\nHipótese alternativa (H₁): Os dados não seguem distribuição normal.\n\nComparamos o p-valor com um nível de significância comum, geralmente α = 0,05:\np-valor = 0,8782 &gt; 0,05 → Não rejeitamos a hipótese nula\nHomogeneidade de variâncias:\n\n\nCode\n# Teste de Bartlett – mais sensível a desvios da normalidade\nbartlett.test(tcm ~ especie, data = micelial)\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  tcm by especie\nBartlett's K-squared = 4.4367, df = 4, p-value = 0.3501\n\n\n\n\nCode\n# Teste de Levene – mais robusto à violação da normalidade\nlibrary(car)\nleveneTest(tcm ~ especie, data = micelial)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  4  1.7563 0.1693\n      25               \n\n\n\n\nInterpretação dos Resultados:\n\np-valor &gt; 0,05 → Não há evidência de variâncias diferentes → Premissa atendida.\np-valor &lt; 0,05 → As variâncias são significativamente diferentes → Premissa violada.\n\n\n\n\nComparações múltiplas e médias ajustadas\nPacote “emmeans”\n(“estimated marginal means”, ou médias marginais estimadas) é usado para realizar testes de comparação de médias entre grupos, ajustando para outros fatores importantes que podem influenciar as médias. O pacote é particularmente útil em modelos lineares generalizados (GLM).\nemmeans(...): calcula as médias ajustadas (médias marginais) para cada grupo de especie com base no modelo.\nImportante para comparações entre grupos quando há mais de 2 níveis.\n\n\nCode\nlibrary(emmeans)\nm &lt;- emmeans(aov2, ~ especie)\nm\n\n\n especie emmean     SE df lower.CL upper.CL\n Fasi     1.572 0.0559 25    1.457     1.69\n Faus     1.237 0.0559 25    1.122     1.35\n Fcor     1.322 0.0559 25    1.207     1.44\n Fgra     0.912 0.0559 25    0.797     1.03\n Fmer     1.427 0.0559 25    1.312     1.54\n\nConfidence level used: 0.95 \n\n\nTestes post-hoc (comparações entre grupos)\nPacote “multcomp” - multcomp: para fazer comparações múltiplas entre grupos.\nTestes simultâneos e intervalos de confiança para hipóteses lineares gerais em modelos paramétricos, incluindo efeitos lineares, lineares generalizados, lineares mistos e modelos de sobrevivência.\nPacote “multcompView” - multcompView: para gerar letras compactas, indicando quais grupos são diferentes.\nConverte um vetor lógico ou um vetor de valores-p ou uma matriz de correlação, diferença ou distância em uma exibição identificando os pares para os quais as diferenças não foram significativamente diferentes.\nCld - Extrai e exibe informações sobre todas as comparações pareadas de médias de mínimos quadrados.\n\n\nCode\nlibrary(multcomp)\nlibrary(multcompView)\n\ncld(m)\n\n\n especie emmean     SE df lower.CL upper.CL .group\n Fgra     0.912 0.0559 25    0.797     1.03  1    \n Faus     1.237 0.0559 25    1.122     1.35   2   \n Fcor     1.322 0.0559 25    1.207     1.44   2   \n Fmer     1.427 0.0559 25    1.312     1.54   23  \n Fasi     1.572 0.0559 25    1.457     1.69    3  \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 5 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n\nCode\npairs(m) #Mostra os testes pareados (comparação entre pares de grupos).\n\n\n contrast    estimate    SE df t.ratio p.value\n Fasi - Faus    0.335 0.079 25   4.241  0.0023\n Fasi - Fcor    0.250 0.079 25   3.165  0.0302\n Fasi - Fgra    0.660 0.079 25   8.356  &lt;.0001\n Fasi - Fmer    0.145 0.079 25   1.836  0.3765\n Faus - Fcor   -0.085 0.079 25  -1.076  0.8169\n Faus - Fgra    0.325 0.079 25   4.115  0.0031\n Faus - Fmer   -0.190 0.079 25  -2.405  0.1469\n Fcor - Fgra    0.410 0.079 25   5.191  0.0002\n Fcor - Fmer   -0.105 0.079 25  -1.329  0.6761\n Fgra - Fmer   -0.515 0.079 25  -6.520  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 5 estimates \n\n\n\n\nCode\npwpm(m) #Exibe uma matriz com as médias na diagonal e comparações entre os grupos fora dela.\n\n\n        Fasi    Faus    Fcor    Fgra    Fmer\nFasi [1.572]  0.0023  0.0302  &lt;.0001  0.3765\nFaus   0.335 [1.237]  0.8169  0.0031  0.1469\nFcor   0.250  -0.085 [1.322]  0.0002  0.6761\nFgra   0.660   0.325   0.410 [0.912]  &lt;.0001\nFmer   0.145  -0.190  -0.105  -0.515 [1.427]\n\nRow and column labels: especie\nUpper triangle: P values   adjust = \"tukey\"\nDiagonal: [Estimates] (emmean) \nLower triangle: Comparisons (estimate)   earlier vs. later"
  },
  {
    "objectID": "analise_estatistica.html#anova-fatorial-two-way-anova",
    "href": "analise_estatistica.html#anova-fatorial-two-way-anova",
    "title": "analise_estatistica",
    "section": "ANOVA fatorial (two-way ANOVA)",
    "text": "ANOVA fatorial (two-way ANOVA)\nA ANOVA fatorial é utilizada quando há duas ou mais variáveis independentes (fatores), cada uma com dois ou mais níveis. Ela é apropriada para experimentos fatoriais completos, nos quais todas as combinações possíveis entre os níveis dos fatores são testadas. Além de avaliar os efeitos individuais de cada fator, essa análise também permite verificar se existe interação entre os fatores, ou seja, se o efeito de um fator depende dos níveis do outro.\n\nImportando o conjunto de dados:\nBanco de dados utilizado: fungicida-vaso (conjunto de dados do dados diversos). Objeto nomeado como fung_vaso.\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\n\nfung_vaso &lt;- read_xlsx(\"dados.xlsx\", sheet = \"fungicida_vaso\")\n\n\nfactor(dose): converte a variável dose em fator (categórica);\nseverity * 100: transforma a variável de severidade em percentual;\ngeom_jitter(): mostra os pontos com leve deslocamento horizontal, evitando sobreposição;\nfacet_wrap(~ treat): separa os gráficos por tratamento (treat).\n\n\nCode\nfung_vaso |&gt; \n  ggplot(aes(factor(dose), severity*100))+ #transformando dose em um fator e ##transformar para percentual *100\n  geom_jitter(width = 0.1)+\n  facet_wrap(~ treat)\n\n\n\n\n\n\n\n\n\n\n\nModelo linear com interação\n\n\nCode\nm_anti &lt;- lm(severity ~ treat * dose, data = fung_vaso)\nanova(m_anti)\n\n\nAnalysis of Variance Table\n\nResponse: severity\n           Df   Sum Sq  Mean Sq F value    Pr(&gt;F)    \ntreat       1 0.113232 0.113232  30.358 4.754e-05 ***\ndose        1 0.073683 0.073683  19.755 0.0004077 ***\ntreat:dose  1 0.072739 0.072739  19.502 0.0004326 ***\nResiduals  16 0.059678 0.003730                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nChecagem das premissas e visualização com DHARMa:\n\n\nCode\nlibrary(DHARMa)\nplot(simulateResiduals(m_anti))\n\n\n\n\n\n\n\n\n\n\n\nMédias ajustadas com emmeans\n\n\nCode\nmedia_anti &lt;- emmeans(m_anti, ~ treat | dose)\nmedia_anti\n\n\ndose = 0.5:\n treat        emmean     SE df lower.CL upper.CL\n Ionic liquid 0.2921 0.0273 16  0.23420   0.3500\n Tebuconazole 0.0210 0.0273 16 -0.03690   0.0789\n\ndose = 2.0:\n treat        emmean     SE df lower.CL upper.CL\n Ionic liquid 0.0501 0.0273 16 -0.00781   0.1080\n Tebuconazole 0.0202 0.0273 16 -0.03768   0.0781\n\nConfidence level used: 0.95 \n\n\n\n\nComparações múltiplas\n\n\nCode\ncld(media_anti)\n\n\ndose = 0.5:\n treat        emmean     SE df lower.CL upper.CL .group\n Tebuconazole 0.0210 0.0273 16 -0.03690   0.0789  1    \n Ionic liquid 0.2921 0.0273 16  0.23420   0.3500   2   \n\ndose = 2.0:\n treat        emmean     SE df lower.CL upper.CL .group\n Tebuconazole 0.0202 0.0273 16 -0.03768   0.0781  1    \n Ionic liquid 0.0501 0.0273 16 -0.00781   0.1080  1    \n\nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\nAgora inverte: médias de doses dentro de tratamentos\n\n\nCode\nmedia_anti &lt;- emmeans(m_anti, ~ dose | treat)\nmedia_anti\n\n\ntreat = Ionic liquid:\n dose emmean     SE df lower.CL upper.CL\n  0.5 0.2921 0.0273 16  0.23420   0.3500\n  2.0 0.0501 0.0273 16 -0.00781   0.1080\n\ntreat = Tebuconazole:\n dose emmean     SE df lower.CL upper.CL\n  0.5 0.0210 0.0273 16 -0.03690   0.0789\n  2.0 0.0202 0.0273 16 -0.03768   0.0781\n\nConfidence level used: 0.95 \n\n\nCode\ncld(media_anti)\n\n\ntreat = Ionic liquid:\n dose emmean     SE df lower.CL upper.CL .group\n  2.0 0.0501 0.0273 16 -0.00781   0.1080  1    \n  0.5 0.2921 0.0273 16  0.23420   0.3500   2   \n\ntreat = Tebuconazole:\n dose emmean     SE df lower.CL upper.CL .group\n  2.0 0.0202 0.0273 16 -0.03768   0.0781  1    \n  0.5 0.0210 0.0273 16 -0.03690   0.0789  1    \n\nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n\n\nCoeficiente de variação\nEssa função do pacote agricolae calcula o coeficiente de variação (CV%) do modelo.\nAjuda a avaliar a precisão experimental. Valores abaixo de 20% geralmente são considerados bons (mas depende do contexto).\n\n\nCode\nlibrary(agricolae)\ncv.model(m_anti)\n\n\n[1] 63.7165\n\n\n\n\nE se não houver interação significativa?\nMostra os efeitos individuais de dose e tratamento, ignorando a interação.\n\n\nCode\nlibrary(patchwork)\n\np1 &lt;- fung_vaso |&gt; \n  ggplot(aes(factor(dose), severity*100)) + \n  geom_jitter(width = 0.1)\np2 &lt;- fung_vaso |&gt; \n  ggplot(aes(treat, severity*100)) + \n  geom_jitter(width = 0.1)\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\nVisualização da interação\n\nGera um gráfico de interação;\nSe as linhas forem paralelas, não há interação;\nSe forem cruzadas ou afastadas, pode indicar interação.\n\n\n\nCode\ninteraction.plot(fung_vaso$treat, fung_vaso$dose, fung_vaso$severity)\n\n\n\n\n\n\n\n\n\nTabela\n\n\n\n\n0.5\n0.2\n\n\n\n\nLI\n29.2 Aa\n5.0 Ab\n\n\nTEBU\n2.1 Ba\n2.0 Aa\n\n\ncv = 63%\n\n\n\n\n\n\n\n\nExemplo:\n\nPacote epifitter e dados:\n\n\nCode\n#install.packages(\"epifitter\")\nlibrary(epifitter)\noidio &lt;- PowderyMildew\n\n\n\n\nVisualização dos dados filtrados\nFiltra apenas 3 tipos de irrigação.\nsev*100: transforma a severidade (que vai de 0 a 1) para percentual (0–100%).\nfacet_grid(moisture ~ irrigation_type): cria um painel com um gráfico para cada combinação de moisture (umidade) e irrigation_type (tipo de irrigação).\nO gráfico mostra como a doença evolui ao longo do tempo (time).\n\n\nCode\noidio |&gt; \n  filter(irrigation_type %in% c(\"MS\", \"MS above canopy\", \"Overhead\")) |&gt; \n  ggplot(aes(time, sev*100)) + \n  geom_jitter(width = 0.1) +\n  facet_grid(moisture ~ irrigation_type)\n\n\n\n\n\n\n\n\n\n\n\nCálculo da AUDPC (Área Abaixo da Curva de Progresso da Doença)\ngroup_by(...) agrupa os dados por tratamento (irrigação, umidade e bloco);\nAUDPC(...) calcula a área abaixo da curva para cada grupo;\nA AUDPC resume a intensidade da doença ao longo do tempo.\n\n\nCode\nlibrary(dplyr)\nlibrary(epifitter)\n\noidio3 &lt;- oidio |&gt;\n  group_by(irrigation_type, moisture, block) |&gt;\n  summarise(AUDPC = AUDPC(time, sev), .groups = \"drop\")\n\n\n\n\nVisualizando a AUDPC\n\n\nCode\noidio3 |&gt; \n  filter(irrigation_type %in% c(\"MS\", \"MS above canopy\", \"Overhead\")) |&gt;\n  ggplot(aes(irrigation_type, AUDPC, color = moisture)) +\n  geom_point(width = 0.1) +\n  scale_y_continuous(limits = c(0, 20))\n\n\n\n\n\n\n\n\n\n\n\nANOVA fatorial (efeito da irrigação e umidade na AUDPC)\n\n\nCode\noidio4 &lt;- oidio3 |&gt; \n  filter(irrigation_type %in% c(\"MS\", \"MS above canopy\", \"Overhead\"))\n\nanov_oidio &lt;- lm(AUDPC ~ irrigation_type * moisture, data = oidio4)\nanova(anov_oidio)\n\n\nAnalysis of Variance Table\n\nResponse: AUDPC\n                         Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nirrigation_type           2 134.341  67.170 451.721 5.073e-12 ***\nmoisture                  1   6.680   6.680  44.924 2.188e-05 ***\nirrigation_type:moisture  2   5.104   2.552  17.162 0.0003022 ***\nResiduals                12   1.784   0.149                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nDiagnóstico do modelo\n\n\nCode\nplot(simulateResiduals(anov_oidio))\n\n\n\n\n\n\n\n\n\n\n\nMédias ajustadas com emmeans\n\n\nCode\nmedias_oidio &lt;- emmeans(anov_oidio, ~ irrigation_type | moisture)\nmedias_oidio\n\n\nmoisture = High moisture:\n irrigation_type emmean    SE df lower.CL upper.CL\n MS                8.52 0.223 12     8.04     9.01\n MS above canopy   3.99 0.223 12     3.51     4.48\n Overhead          3.68 0.223 12     3.20     4.17\n\nmoisture = Moderate moisture:\n irrigation_type emmean    SE df lower.CL upper.CL\n MS               11.18 0.223 12    10.70    11.67\n MS above canopy   4.86 0.223 12     4.37     5.34\n Overhead          3.81 0.223 12     3.33     4.30\n\nConfidence level used: 0.95 \n\n\nCode\ncld(medias_oidio)\n\n\nmoisture = High moisture:\n irrigation_type emmean    SE df lower.CL upper.CL .group\n Overhead          3.68 0.223 12     3.20     4.17  1    \n MS above canopy   3.99 0.223 12     3.51     4.48  1    \n MS                8.52 0.223 12     8.04     9.01   2   \n\nmoisture = Moderate moisture:\n irrigation_type emmean    SE df lower.CL upper.CL .group\n Overhead          3.81 0.223 12     3.33     4.30  1    \n MS above canopy   4.86 0.223 12     4.37     5.34   2   \n MS               11.18 0.223 12    10.70    11.67    3  \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 3 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nAgora, inverte: mostra as médias de umidade dentro de cada tipo de irrigação.\n\n\nCode\nmedias_oidio2 &lt;- emmeans(anov_oidio, ~ moisture | irrigation_type)\nmedias_oidio2\n\n\nirrigation_type = MS:\n moisture          emmean    SE df lower.CL upper.CL\n High moisture       8.52 0.223 12     8.04     9.01\n Moderate moisture  11.18 0.223 12    10.70    11.67\n\nirrigation_type = MS above canopy:\n moisture          emmean    SE df lower.CL upper.CL\n High moisture       3.99 0.223 12     3.51     4.48\n Moderate moisture   4.86 0.223 12     4.37     5.34\n\nirrigation_type = Overhead:\n moisture          emmean    SE df lower.CL upper.CL\n High moisture       3.68 0.223 12     3.20     4.17\n Moderate moisture   3.81 0.223 12     3.33     4.30\n\nConfidence level used: 0.95 \n\n\nCode\ncld(medias_oidio2)\n\n\nirrigation_type = MS:\n moisture          emmean    SE df lower.CL upper.CL .group\n High moisture       8.52 0.223 12     8.04     9.01  1    \n Moderate moisture  11.18 0.223 12    10.70    11.67   2   \n\nirrigation_type = MS above canopy:\n moisture          emmean    SE df lower.CL upper.CL .group\n High moisture       3.99 0.223 12     3.51     4.48  1    \n Moderate moisture   4.86 0.223 12     4.37     5.34   2   \n\nirrigation_type = Overhead:\n moisture          emmean    SE df lower.CL upper.CL .group\n High moisture       3.68 0.223 12     3.20     4.17  1    \n Moderate moisture   3.81 0.223 12     3.33     4.30  1    \n\nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n\nCoeficiente de variação do modelo\nIndica a precisão do experimento - valores menores geralmente indicam maior confiabilidade\n\n\nCode\ncv.model(anov_oidio)\n\n\n[1] 6.418205\n\n\nTabela\n\n\n\n\nH. moisture\nM. moisture\n\n\n\n\nMS\n8.52 Aa\n11.18 Ab\n\n\nMS Ac.\n3.99 Ba\n4.86 Bb\n\n\nOverhead\n3.68 Ba\n3.81 Ca\n\n\nCV = 6.41"
  },
  {
    "objectID": "analise_estatistica.html#anova-fatorial---3-fatores",
    "href": "analise_estatistica.html#anova-fatorial---3-fatores",
    "title": "analise_estatistica",
    "section": "Anova Fatorial - 3 Fatores",
    "text": "Anova Fatorial - 3 Fatores\n\nExemplo:\nDados sobre a interação entre tipo de armazenamento e umidade.\n\n\nCode\nmilho &lt;- read_excel(\"dados.xlsx\", \"armazena\")\nmilho |&gt;\n  filter(tempo ==8) |&gt;\n  ggplot(aes(factor(tipo), peso_mil,\n             color = factor(umidade)))+\n  geom_jitter(width = 0.1)+\n  facet_wrap(~ umidade)\n\n\n\n\n\n\n\n\n\nTestar a interação entre o tipo de armazenamento e o tempo 8\n\n\nCode\nmilho2 &lt;- milho |&gt;\n  filter(tempo ==8)\n\nm2 &lt;- aov(peso_mil ~ factor(tipo)*factor(umidade),\n          data = milho2)\nsummary(m2)\n\n\n                             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactor(tipo)                  1  11215   11215  2375.8 3.64e-15 ***\nfactor(umidade)               2  42814   21407  4534.8  &lt; 2e-16 ***\nfactor(tipo):factor(umidade)  2   2329    1165   246.7 1.79e-10 ***\nResiduals                    12     57       5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTestanto tipo de inoculação na incidencia de Fusarium sp. em milho\n\n\nCode\nmilho3 &lt;- read_excel(\"dados.xlsx\", \"milho\")\n\nm4 &lt;- aov(yield ~hybrid*method,\n          data = milho3)\nsummary(m4)\n\n\n              Df    Sum Sq  Mean Sq F value   Pr(&gt;F)    \nhybrid         5 105876446 21175289   8.312 2.66e-05 ***\nmethod         1     42951    42951   0.017    0.897    \nhybrid:method  5  10619453  2123891   0.834    0.534    \nResiduals     36  91709593  2547489                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nChecagem das premissas\n\n\nCode\ncheck_heteroscedasticity(m4)\n\n\nOK: Error variance appears to be homoscedastic (p = 0.928).\n\n\n\n\nCode\nplot(simulateResiduals(m4))\n\n\n\n\n\n\n\n\n\nMédias ajustadas com emmeans\n\n\nCode\nmedias_m4 &lt;- emmeans(m4, ~ hybrid)\nmedias_m4\n\n\n hybrid   emmean  SE df lower.CL upper.CL\n 30F53 HX  10598 564 36     9453    11742\n 30F53 YH   9309 564 36     8165    10454\n 30K64     11018 564 36     9874    12162\n 30S31H     8652 564 36     7507     9796\n 30S31YH    8056 564 36     6912     9201\n BG7049H   12402 564 36    11257    13546\n\nResults are averaged over the levels of: method \nConfidence level used: 0.95 \n\n\n\n\nCode\ncld(medias_m4)\n\n\n hybrid   emmean  SE df lower.CL upper.CL .group\n 30S31YH    8056 564 36     6912     9201  1    \n 30S31H     8652 564 36     7507     9796  12   \n 30F53 YH   9309 564 36     8165    10454  12   \n 30F53 HX  10598 564 36     9453    11742   23  \n 30K64     11018 564 36     9874    12162   23  \n BG7049H   12402 564 36    11257    13546    3  \n\nResults are averaged over the levels of: method \nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 6 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nCaso a interação não dê sifnificativa, tira a interação e deixa só o fator que teve significancia (isola o fator)\n\n\nCode\nm5 &lt;- aov(yield ~hybrid, data = milho3)\nsummary(m5)\n\n\n            Df    Sum Sq  Mean Sq F value   Pr(&gt;F)    \nhybrid       5 105876446 21175289   8.688 1.02e-05 ***\nResiduals   42 102371996  2437428                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nm4 &lt;- aov(yield ~hybrid,\n          data = milho3)\nsummary(m5)\n\n\n            Df    Sum Sq  Mean Sq F value   Pr(&gt;F)    \nhybrid       5 105876446 21175289   8.688 1.02e-05 ***\nResiduals   42 102371996  2437428                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\ncheck_heteroscedasticity(m5)\n\n\nOK: Error variance appears to be homoscedastic (p = 0.763).\n\n\n\n\nCode\nmedias_m5 &lt;- emmeans(m5, ~hybrid)\nmedias_m5\n\n\n hybrid   emmean  SE df lower.CL upper.CL\n 30F53 HX  10598 552 42     9484    11712\n 30F53 YH   9309 552 42     8195    10423\n 30K64     11018 552 42     9904    12132\n 30S31H     8652 552 42     7538     9765\n 30S31YH    8056 552 42     6942     9170\n BG7049H   12402 552 42    11288    13516\n\nConfidence level used: 0.95 \n\n\n\n\nCode\ncld(medias_m5)\n\n\n hybrid   emmean  SE df lower.CL upper.CL .group\n 30S31YH    8056 552 42     6942     9170  1    \n 30S31H     8652 552 42     7538     9765  12   \n 30F53 YH   9309 552 42     8195    10423  123  \n 30F53 HX  10598 552 42     9484    11712   234 \n 30K64     11018 552 42     9904    12132    34 \n BG7049H   12402 552 42    11288    13516     4 \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 6 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n\nCode\npwpm(medias_m5)\n\n\n         30F53 HX 30F53 YH   30K64  30S31H 30S31YH BG7049H\n30F53 HX  [10598]   0.5709  0.9942  0.1494  0.0254  0.2125\n30F53 YH     1288  [ 9309]  0.2643  0.9576  0.5999  0.0036\n30K64        -420    -1709 [11018]  0.0447  0.0059  0.4938\n30S31H       1946      658    2366 [ 8652]  0.9723  0.0003\n30S31YH      2541     1253    2962     595 [ 8056]  &lt;.0001\nBG7049H     -1804    -3092   -1384   -3750   -4345 [12402]\n\nRow and column labels: hybrid\nUpper triangle: P values   adjust = \"tukey\"\nDiagonal: [Estimates] (emmean) \nLower triangle: Comparisons (estimate)   earlier vs. later"
  },
  {
    "objectID": "analise_estatistica.html#anova-com-bloco",
    "href": "analise_estatistica.html#anova-com-bloco",
    "title": "analise_estatistica",
    "section": "ANOVA com bloco",
    "text": "ANOVA com bloco\n\nAnova com bloco - Delineamento em Blocos Casualizado (DBC)\nO (DBC) envolve os três princípios da experimentação: repetição, casualização e controle local. Neste caso, as condições locais não são homogêneas e podem ter efeito significativo sobre os tratamentos.\n\nCarregando pacotes e dados\nUsando o conjunto de dados fungicida_campo\n\n\nCode\nlibrary(readxl)\nlibrary(Hmisc)\nfung_campo &lt;- read_xlsx(\"dados.xlsx\", sheet = \"fungicida_campo\")\n\n\n\n\nGráfico de produção por tratamento\nmutate(TRAT = factor(TRAT)): transforma os tratamentos (TRAT) em fatores para garantir que o ggplot os trate como categorias.\ngeom_jitter: mostra os dados de cada parcela/bloco, deslocados horizontalmente para evitar sobreposição.\nstat_summary(fun.data = \"mean_cl_boot\"): adiciona médias com intervalos de confiança via bootstrap.\n\n\nCode\nfung_campo |&gt; \n  mutate(TRAT = factor(TRAT)) |&gt; \n  ggplot(aes(TRAT, PROD)) +\n  geom_jitter(width = 0.2) +\n  stat_summary(fun.data = \"mean_cl_boot\", colour = \"red\", width = 0.3)\n\n\n\n\n\n\n\n\n\n\n\nConvertendo variáveis em fatores\nAqui, você transforma TRAT e BLOCO explicitamente em fatores, pois o R trata números como contínuos por padrão.\n\n\nCode\nfung_campo$TRAT &lt;- factor(fung_campo$TRAT)\nfung_campo$BLOCO &lt;- factor(fung_campo$BLOCO)\n\n\n\n\nModelo Anova com bloco\nANOVA com efeito de blocos e tratamentos:\n\n\nCode\nanova_campo &lt;- lm(PROD ~ BLOCO + TRAT, data = fung_campo)\nanova(anova_campo)\n\n\nAnalysis of Variance Table\n\nResponse: PROD\n          Df  Sum Sq Mean Sq F value Pr(&gt;F)  \nBLOCO      3  105716   35239  0.2172 0.8833  \nTRAT       7 2994142  427735  2.6369 0.0402 *\nResiduals 21 3406384  162209                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nChecagem das premissas\n\n\nCode\nlibrary(performance)\nlibrary(DHARMa)\ncheck_normality(anova_campo)\n\n\nOK: residuals appear as normally distributed (p = 0.542).\n\n\nCode\ncheck_heteroscedasticity(anova_campo)\n\n\nOK: Error variance appears to be homoscedastic (p = 0.216).\n\n\n\n\nCode\nlibrary(DHARMa)\nplot(simulateResiduals(anova_campo))\n\n\n\n\n\n\n\n\n\n\n\nEstimativa e comparação das médias dos tratamentos\n\n\nCode\nmeans_campo &lt;- emmeans(anova_campo, ~ TRAT)\nmeans_campo\n\n\n TRAT emmean  SE df lower.CL upper.CL\n 1      4219 201 21     3800     4638\n 2      4935 201 21     4516     5354\n 3      5110 201 21     4691     5529\n 4      5140 201 21     4722     5559\n 5      5122 201 21     4703     5541\n 6      5256 201 21     4838     5675\n 7      5128 201 21     4709     5546\n 8      5078 201 21     4659     5497\n\nResults are averaged over the levels of: BLOCO \nConfidence level used: 0.95 \n\n\n\n\nCode\nlibrary(multcomp)\ncld(means_campo)\n\n\n TRAT emmean  SE df lower.CL upper.CL .group\n 1      4219 201 21     3800     4638  1    \n 2      4935 201 21     4516     5354  12   \n 8      5078 201 21     4659     5497  12   \n 3      5110 201 21     4691     5529  12   \n 5      5122 201 21     4703     5541  12   \n 7      5128 201 21     4709     5546  12   \n 4      5140 201 21     4722     5559  12   \n 6      5256 201 21     4838     5675   2   \n\nResults are averaged over the levels of: BLOCO \nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 8 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n\nCode\nplot(means_campo)\n\n\n\n\n\n\n\n\n\nCode\npwpp(means_campo)\n\n\n\n\n\n\n\n\n\nCode\npwpm(means_campo)\n\n\n        1       2       3       4       5       6       7      8\n1  [4219]  0.2431  0.0793  0.0639  0.0728  0.0272  0.0699 0.0985\n2  -715.7  [4935]  0.9983  0.9953  0.9974  0.9429  0.9968 0.9995\n3  -890.5  -174.8  [5110]  1.0000  1.0000  0.9994  1.0000 1.0000\n4  -921.1  -205.4   -30.6  [5140]  1.0000  0.9999  1.0000 1.0000\n5  -902.7  -187.0   -12.2    18.4  [5122]  0.9997  1.0000 1.0000\n6 -1037.1  -321.4  -146.6  -116.0  -134.4  [5256]  0.9998 0.9981\n7  -908.4  -192.7   -17.9    12.7    -5.7   128.7  [5128] 1.0000\n8  -859.0  -143.3    31.5    62.1    43.7   178.0    49.4 [5078]\n\nRow and column labels: TRAT\nUpper triangle: P values   adjust = \"tukey\"\nDiagonal: [Estimates] (emmean) \nLower triangle: Comparisons (estimate)   earlier vs. later\n\n\nAnálise da Severidade da Ferrugem (FER):\n\n\nANOVA com transformação logarítmica\n\n\nCode\nanova_fer &lt;- lm(log(FER) ~ BLOCO + TRAT, data = fung_campo)\nanova(anova_fer)\n\n\nAnalysis of Variance Table\n\nResponse: log(FER)\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nBLOCO      3  0.2064 0.06880  1.7961    0.1788    \nTRAT       7 11.5210 1.64585 42.9665 4.838e-11 ***\nResiduals 21  0.8044 0.03831                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nDiagnóstico do modelo\n\n\nCode\nplot(simulateResiduals(anova_fer))\n\n\n\n\n\n\n\n\n\n\n\nMédias com back-transformation\n\n\nCode\nmeans_fer &lt;- emmeans(anova_fer, ~ TRAT, type = \"response\")\n\n\n\n\nCode\ncld(means_fer)\n\n\n TRAT response    SE df lower.CL upper.CL .group\n 6        2.98 0.292 21     2.43     3.65  1    \n 4        3.08 0.301 21     2.51     3.78  1    \n 5        3.24 0.317 21     2.64     3.97  1    \n 7        3.37 0.330 21     2.75     4.13  1    \n 8        3.48 0.341 21     2.84     4.27  1    \n 3        3.81 0.373 21     3.11     4.67  12   \n 2        5.68 0.556 21     4.63     6.96   2   \n 1       20.02 1.960 21    16.33    24.54    3  \n\nResults are averaged over the levels of: BLOCO \nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \nP value adjustment: tukey method for comparing a family of 8 estimates \nTests are performed on the log scale \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nCode\nplot(means_fer)\n\n\n\n\n\n\n\n\n\nCode\npwpp(means_fer)\n\n\n\n\n\n\n\n\n\nCode\npwpm(means_fer)\n\n\n        1       2       3       4       5       6       7       8\n1 [20.02]  &lt;.0001  &lt;.0001  &lt;.0001  &lt;.0001  &lt;.0001  &lt;.0001  &lt;.0001\n2   3.525 [ 5.68]  0.1252  0.0048  0.0110  0.0028  0.0204  0.0343\n3   5.259   1.492 [ 3.81]  0.7832  0.9335  0.6440  0.9843  0.9976\n4   6.500   1.844   1.236 [ 3.08]  0.9999  1.0000  0.9976  0.9842\n5   6.178   1.753   1.175   0.951 [ 3.24]  0.9984  1.0000  0.9994\n6   6.721   1.906   1.278   1.034   1.088 [ 2.98]  0.9842  0.9431\n7   5.945   1.686   1.130   0.915   0.962   0.885 [ 3.37]  1.0000\n8   5.750   1.631   1.093   0.885   0.931   0.856   0.967 [ 3.48]\n\nRow and column labels: TRAT\nUpper triangle: P values   null = 1  adjust = \"tukey\"\nDiagonal: [Estimates] (response)   type = \"response\"\nLower triangle: Comparisons (ratio)   earlier vs. later"
  },
  {
    "objectID": "analise_estatistica.html#delineamento-em-parcela-subdividida-split-plot",
    "href": "analise_estatistica.html#delineamento-em-parcela-subdividida-split-plot",
    "title": "analise_estatistica",
    "section": "Delineamento em parcela subdividida (Split-plot)",
    "text": "Delineamento em parcela subdividida (Split-plot)\n\nImportando o conjunto de dados:\nExemplo:\n\n\nCode\nmilho &lt;- read_excel(\"dados.xlsx\", \"milho\")\n\n\n\nVisualizando os dados\n\n\nCode\nmilho |&gt; \n  \n  ggplot(aes(hybrid, index, color = method))+\n  geom_jitter(width = 0.1)+\n coord_flip()\n\n\n\n\n\n\n\n\n\n\n\nAjustando o modelo:\n\n\nCode\naov_milho_bloco &lt;- aov(index ~ factor(block) + hybrid*method + \nError(factor(block)/hybrid/method), data = milho)\n\nsummary(aov_milho_bloco)\n\n\n\nError: factor(block)\n              Df Sum Sq Mean Sq\nfactor(block)  3  592.2   197.4\n\nError: factor(block):hybrid\n          Df Sum Sq Mean Sq F value Pr(&gt;F)  \nhybrid     5  974.2  194.84    3.14 0.0389 *\nResiduals 15  930.9   62.06                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: factor(block):hybrid:method\n              Df Sum Sq Mean Sq F value Pr(&gt;F)  \nmethod         1  79.61   79.61   4.726 0.0433 *\nhybrid:method  5 265.28   53.06   3.150 0.0324 *\nResiduals     18 303.18   16.84                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nChecagem das premissas\nEm parcelas subdivididas não é possível checar as premissas pelo check_, então usa lme4, para checar pelo modelo misto.\n\nPacote “lme4”\nAjusta modelos de efeitos mistos lineares e lineares generalizados. Os modelos e seus componentes são representados usando classes e métodos S4.\n\n\nFunção “lmer”\nGera um componente aleatório que é específico a cada indivíduo, de modo que podemos ter, para cada um, um intercepto e uma inclinação distintas.\n\n\nCode\nlibrary(Matrix)\nlibrary(lme4)\nmilho$block &lt;- as.factor(milho$block)\nmix2 &lt;- lmer(index ~ block + hybrid*method + \n(1|block/hybrid), data =  milho)\n\nlibrary(car)\nAnova(mix2)\n\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: index\n                Chisq Df Pr(&gt;Chisq)   \nblock          0.3192  3   0.956380   \nhybrid        15.6987  5   0.007759 **\nmethod         4.7262  1   0.029706 * \nhybrid:method 15.7498  5   0.007596 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\ncheck_normality(mix2)\n\n\nOK: residuals appear as normally distributed (p = 0.621).\n\n\n\n\nCode\ncheck_heteroscedasticity(mix2)\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.009).\n\n\n\n\n\nNecessário transformar os dados\n\n\nCode\nmilho$block &lt;- as.factor(milho$block)\nmix2 &lt;- lmer(sqrt(index) ~ block + hybrid*method + (1|block/hybrid), data = milho)\nlibrary(car)\nAnova(mix2)\n\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: sqrt(index)\n                Chisq Df Pr(&gt;Chisq)   \nblock          0.0764  3   0.994506   \nhybrid        15.4171  5   0.008721 **\nmethod         3.9239  1   0.047605 * \nhybrid:method 13.3025  5   0.020703 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nChecagem\n\n\nCode\ncheck_normality(mix2)\n\n\nOK: residuals appear as normally distributed (p = 0.422).\n\n\n\n\nCode\ncheck_heteroscedasticity(mix2)\n\n\nOK: Error variance appears to be homoscedastic (p = 0.970).\n\n\n\n\n\nComparação de médias\n\n\nCode\nmeans_mix2 &lt;- emmeans(mix2, ~hybrid | method)\nmeans_mix2\n\n\nmethod = pin:\n hybrid   emmean   SE   df lower.CL upper.CL\n 30F53 HX   5.00 1.17 5356     2.69     7.30\n 30F53 YH   4.95 1.17 5356     2.65     7.25\n 30K64      4.50 1.17 5356     2.20     6.81\n 30S31H     6.10 1.17 5356     3.79     8.40\n 30S31YH    5.63 1.17 5356     3.33     7.93\n BG7049H    4.40 1.17 5356     2.10     6.71\n\nmethod = silk:\n hybrid   emmean   SE   df lower.CL upper.CL\n 30F53 HX   4.94 1.17 5356     2.64     7.25\n 30F53 YH   5.10 1.17 5356     2.80     7.41\n 30K64      4.61 1.17 5356     2.31     6.91\n 30S31H     5.13 1.17 5356     2.83     7.43\n 30S31YH    5.14 1.17 5356     2.84     7.44\n BG7049H    4.37 1.17 5356     2.07     6.67\n\nResults are averaged over the levels of: block \nDegrees-of-freedom method: kenward-roger \nResults are given on the sqrt (not the response) scale. \nConfidence level used: 0.95 \n\n\n\n\nCode\ncld(means_mix2)\n\n\nmethod = pin:\n hybrid   emmean   SE   df lower.CL upper.CL .group\n BG7049H    4.40 1.17 5356     2.10     6.71  1    \n 30K64      4.50 1.17 5356     2.20     6.81  1    \n 30F53 YH   4.95 1.17 5356     2.65     7.25  12   \n 30F53 HX   5.00 1.17 5356     2.69     7.30  12   \n 30S31YH    5.63 1.17 5356     3.33     7.93  12   \n 30S31H     6.10 1.17 5356     3.79     8.40   2   \n\nmethod = silk:\n hybrid   emmean   SE   df lower.CL upper.CL .group\n BG7049H    4.37 1.17 5356     2.07     6.67  1    \n 30K64      4.61 1.17 5356     2.31     6.91  1    \n 30F53 HX   4.94 1.17 5356     2.64     7.25  1    \n 30F53 YH   5.10 1.17 5356     2.80     7.41  1    \n 30S31H     5.13 1.17 5356     2.83     7.43  1    \n 30S31YH    5.14 1.17 5356     2.84     7.44  1    \n\nResults are averaged over the levels of: block \nDegrees-of-freedom method: kenward-roger \nResults are given on the sqrt (not the response) scale. \nConfidence level used: 0.95 \nNote: contrasts are still on the sqrt scale. Consider using\n      regrid() if you want contrasts of back-transformed estimates. \nP value adjustment: tukey method for comparing a family of 6 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same."
  },
  {
    "objectID": "analise_estatistica.html#teste---modelo-anova",
    "href": "analise_estatistica.html#teste---modelo-anova",
    "title": "analise_estatistica",
    "section": "Teste - Modelo ANOVA",
    "text": "Teste - Modelo ANOVA\nQuando se analisa um conjunto de dados e esses dados apresentam-se como não paramétricos, deve-se trabalhar esses dados de uma forma diferente. Mas antes, deve-se comprovar por meio da anova e da checagem das premissas, que os dados realmente não são normais e homogêneos.\n\n\nCode\naov2 &lt;- aov(count ~ spray, data = insects)\nsummary(aov2)\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nspray        5   2669   533.8    34.7 &lt;2e-16 ***\nResiduals   66   1015    15.4                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nChecagem das premissas\n\n\nCode\nlibrary(performance)\ncheck_normality(aov2)\n\n\nWarning: Non-normality of residuals detected (p = 0.022).\n\n\n\n\nCode\ncheck_heteroscedasticity(aov2)\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\nA partir da checagem das premissas, observa-se que os dados não são normais e homogeneos."
  },
  {
    "objectID": "analise_estatistica.html#alternativas-para-dados-não-paramétricos",
    "href": "analise_estatistica.html#alternativas-para-dados-não-paramétricos",
    "title": "analise_estatistica",
    "section": "Alternativas para dados não paramétricos",
    "text": "Alternativas para dados não paramétricos\nQuando se tem dados não paramétricos, tem-se 3 alternativas:\n\nTransformar os dados (Exemplo: raiz quadrada, log, Box cox);\nUsar testes não paramétricos (Kruskal-Wallis);\nOu usar modelos lineares generalizados.\n\n\n1. Transformar os dados para normalizar\nExemplo: Usando a raiz quadrada para tentar normalizar e tornar os dados normais e homogenos.\nPode-se também tentar o log da variável resposta + 0.5.\n\n\nCode\naov2 &lt;- aov(sqrt(count) ~ spray, data = insects)\nsummary(aov2)\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nspray        5  88.44  17.688    44.8 &lt;2e-16 ***\nResiduals   66  26.06   0.395                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nChecagem das premissas\n\n\nCode\ncheck_normality(aov2)\n\n\nOK: residuals appear as normally distributed (p = 0.681).\n\n\n\n\nCode\ncheck_heteroscedasticity(aov2)\n\n\nOK: Error variance appears to be homoscedastic (p = 0.854).\n\n\n\n\n\n2. Uso de testes não paramétricos\nSe com as transformações não normalizar e ainda forem heterogêneos, usa-se testes não paramétricos.\nUma das saídas para normalizar os dados é a utilização do teste de Kruskal-Wallis. O teste de Kruskal-Wallis utiliza os valores numéricos transformados em postos e agrupados num só conjunto de dados, é testado se as amostras vêm de uma mesma população, ou se pelo menos uma delas vêm de população distinta das demais. O teste de Kruskal-Wallis dispensa a pressuposição de normalidade e homocedasticidade. Tem 2 opções de teste Kruskal. Para usar essa opção, é necessário baixar e carregar o pacote agricolae.\n\nTeste de Kruskal-Wallis\nÉ utilizado em situações onde queremos comparar mais de dois grupos independentes, de tamanhos iguais ou não, com variável resposta quantitativa. É uma alternativa quando os pressupostos necesários para o teste F da Anova não são atendidos, pois este teste dispensa a pressuposição de normalidade e homocedasticidade.\n\n\nCode\nlibrary(agricolae)\n\nkruskal.test(count ~ spray, data = insects)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  count by spray\nKruskal-Wallis chi-squared = 54.691, df = 5, p-value = 1.511e-10\n\n\n\n\nCode\nkruskal(insects$count, insects$spray, \n        console = TRUE)\n\n\n\nStudy: insects$count ~ insects$spray\nKruskal-Wallis test's\nTies or no Ties\n\nCritical Value: 54.69134\nDegrees of freedom: 5\nPvalue Chisq  : 1.510845e-10 \n\ninsects$spray,  means of the ranks\n\n  insects.count  r\nA      52.16667 12\nB      54.83333 12\nC      11.45833 12\nD      25.58333 12\nE      19.33333 12\nF      55.62500 12\n\nPost Hoc Analysis\n\nt-Student: 1.996564\nAlpha    : 0.05\nMinimum Significant Difference: 8.462804 \n\nTreatments with the same letter are not significantly different.\n\n  insects$count groups\nF      55.62500      a\nB      54.83333      a\nA      52.16667      a\nD      25.58333      b\nE      19.33333     bc\nC      11.45833      c\n\n\nO pacote emmeans é muito útil na análise de Modelos Lineares Generalizados (GLM), pois permite obter as médias marginais estimadas dos fatores no modelo.\n\n\nCode\naov2 &lt;- aov(count ~ spray, data = insects)\nsummary(aov2)\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nspray        5   2669   533.8    34.7 &lt;2e-16 ***\nResiduals   66   1015    15.4                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nChecagem das premissas\n\n\nCode\ncheck_normality(aov2)\n\n\nWarning: Non-normality of residuals detected (p = 0.022).\n\n\n\n\nCode\ncheck_heteroscedasticity(aov2)\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\nFunção emmeans: tirar a média da variável inseticida. Para dar o valor original da média e não o valor transformado, usa-se a função type = response.\n\n\nCode\nlibrary(emmeans)\naov2_means &lt;- emmeans(aov2, ~ spray,\n                         type = \"response\")\naov2_means\n\n\n spray emmean   SE df lower.CL upper.CL\n A      14.50 1.13 66   12.240    16.76\n B      15.33 1.13 66   13.073    17.59\n C       2.08 1.13 66   -0.177     4.34\n D       4.92 1.13 66    2.656     7.18\n E       3.50 1.13 66    1.240     5.76\n F      16.67 1.13 66   14.406    18.93\n\nConfidence level used: 0.95 \n\n\nA função pwpm gera uma tabela de comparação das médias e cld é uma função que serve para gerar os números que diferenciam os grupos de médias.\n\n\nCode\npwpm(aov2_means)\n\n\n        A       B       C       D       E       F\nA [14.50]  0.9952  &lt;.0001  &lt;.0001  &lt;.0001  0.7542\nB  -0.833 [15.33]  &lt;.0001  &lt;.0001  &lt;.0001  0.9603\nC  12.417  13.250 [ 2.08]  0.4921  0.9489  &lt;.0001\nD   9.583  10.417  -2.833 [ 4.92]  0.9489  &lt;.0001\nE  11.000  11.833  -1.417   1.417 [ 3.50]  &lt;.0001\nF  -2.167  -1.333 -14.583 -11.750 -13.167 [16.67]\n\nRow and column labels: spray\nUpper triangle: P values   adjust = \"tukey\"\nDiagonal: [Estimates] (emmean)   type = \"response\"\nLower triangle: Comparisons (estimate)   earlier vs. later\n\n\n\n\nCode\nlibrary(MASS)\nlibrary(mvtnorm)\nlibrary(survival)\nlibrary(TH.data)\nlibrary(multcomp)\nlibrary(multcompView)\ncld(aov2_means)\n\n\n spray emmean   SE df lower.CL upper.CL .group\n C       2.08 1.13 66   -0.177     4.34  1    \n E       3.50 1.13 66    1.240     5.76  1    \n D       4.92 1.13 66    2.656     7.18  1    \n A      14.50 1.13 66   12.240    16.76   2   \n B      15.33 1.13 66   13.073    17.59   2   \n F      16.67 1.13 66   14.406    18.93   2   \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 6 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n\n\n3. GLM – Modelos Lineares Generalizados\nA função glm é utilizada para ajustar Modelos Lineares Generalizados no R. Esses modelos permitem trabalhar com diferentes distribuições de erro, como binomial, Poisson e outras, tornando possível a análise de variáveis resposta que não seguem uma distribuição normal. O modelo é definido por uma fórmula simbólica que relaciona a variável resposta aos preditores, e pela escolha de uma família de distribuição que representa o tipo de dado analisado. Para publicação de artigos, essa é a opção mais aconselhável.\nPara a geração de modelos, a função a ser utilizada é a glm e precisa indicar os argumentos family = poisson(link = “identity”). Para visualizar, pode usar o pacote Dharma e gerar um plot.\n\n\nCode\nlibrary(DHARMa)\n\nglm1 &lt;- glm(count ~spray,\n             data = insects,\n             family = poisson(link = \"identity\"))\nplot(simulateResiduals(glm1))\n\n\n\n\n\n\n\n\n\n\n\nCode\nsummary(glm1)\n\n\n\nCall:\nglm(formula = count ~ spray, family = poisson(link = \"identity\"), \n    data = insects)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  14.5000     1.0992  13.191  &lt; 2e-16 ***\nsprayB        0.8333     1.5767   0.529    0.597    \nsprayC      -12.4167     1.1756 -10.562  &lt; 2e-16 ***\nsprayD       -9.5833     1.2720  -7.534 4.92e-14 ***\nsprayE      -11.0000     1.2247  -8.981  &lt; 2e-16 ***\nsprayF        2.1667     1.6116   1.344    0.179    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 409.041  on 71  degrees of freedom\nResidual deviance:  98.329  on 66  degrees of freedom\nAIC: 376.59\n\nNumber of Fisher Scoring iterations: 3\n\n\n\n\nCode\nglm1_means &lt;- emmeans(glm1, ~ spray)\ncld(glm1_means)\n\n\n spray emmean    SE  df asymp.LCL asymp.UCL .group\n C       2.08 0.417 Inf      1.27      2.90  1    \n E       3.50 0.540 Inf      2.44      4.56  12   \n D       4.92 0.640 Inf      3.66      6.17   2   \n A      14.50 1.100 Inf     12.35     16.65    3  \n B      15.33 1.130 Inf     13.12     17.55    3  \n F      16.67 1.180 Inf     14.36     18.98    3  \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 6 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same."
  },
  {
    "objectID": "analise_estatistica.html#modelo-de-melhor-ajuste",
    "href": "analise_estatistica.html#modelo-de-melhor-ajuste",
    "title": "analise_estatistica",
    "section": "Modelo de melhor ajuste",
    "text": "Modelo de melhor ajuste\nDeve-se testar o modelo que melhor se ajusta aos dados. Pode-se testar fazer a análise de regressão para cada experimento (isola cada experimento) ou analisar em grupos (modelos mistos).\n\nAnálise de regressão por experimento\nAnalisando cada experimento isoladamente:\nÉ preciso criar um novo objeto de dados, chamado exp1, atribuindo a ele o conjunto estande. Em seguida, deve-se filtrar o experimento de interesse e gerar um novo objeto com esse subconjunto, o que possibilita a execução da análise de regressão.\nExperimento 1:\n\n\nCode\n# Filtrar experimento 1 e calcular média por tratamento\nexp1 &lt;- estande |&gt;\n  filter(exp == 1) |&gt;\n  group_by(trat) |&gt;\n  summarise(nplants2 = mean(nplants, na.rm = TRUE))\n\n# Gráfico da média\nexp1 |&gt;\n  ggplot(aes(trat, nplants2)) +\n  geom_point() +\n  ylim(20, 60)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Regressão linear com bloco (precisa existir a variável 'bloco')\nexp1_model &lt;- estande |&gt;\n  filter(exp == 1)\n\nm_exp1 &lt;- lm(nplants ~ trat + bloco, data = exp1_model)\nsummary(m_exp1)\n\n\n\nCall:\nlm(formula = nplants ~ trat + bloco, data = exp1_model)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.0769  -6.7847  -0.7817   4.0522  22.6091 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  75.5833     5.7164  13.222 1.19e-11 ***\ntrat         -0.2419     0.1323  -1.829 0.081623 .  \nbloco        -9.2333     1.9485  -4.739 0.000111 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.67 on 21 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5086 \nF-statistic:  12.9 on 2 and 21 DF,  p-value: 0.0002216\n\n\nFoi ajustado um modelo linear para avaliar o efeito do tratamento e do bloco sobre o número de plantas. O modelo apresentou um bom ajuste, explicando cerca de 55% da variação nos dados (R² = 0,55). O efeito do bloco foi altamente significativo (p &lt; 0,001), indicando variações importantes entre os blocos experimentais. Já o efeito do tratamento foi marginalmente significativo (p = 0,082), sugerindo uma possível tendência de diferença entre tratamentos, embora com menor evidência estatística. O erro padrão residual foi de 10,67, e o modelo geral foi significativo pelo teste F (p &lt; 0,001).\nExperimento 2:\n\n\nCode\nexp2 &lt;- estande |&gt;\n  filter(exp == 2)\n\nm_exp2 &lt;- lm(nplants ~ trat, data = exp2)\nsummary(m_exp2)\n\n\n\nCall:\nlm(formula = nplants ~ trat, data = exp2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.7816  -7.7150   0.5653   8.1929  19.2184 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  60.9857     3.6304  16.798 4.93e-14 ***\ntrat         -0.7007     0.1605  -4.365 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.95 on 22 degrees of freedom\nMultiple R-squared:  0.4641,    Adjusted R-squared:  0.4398 \nF-statistic: 19.05 on 1 and 22 DF,  p-value: 0.0002473\n\n\nFoi ajustado um modelo linear para avaliar o efeito do tratamento (trat) sobre o número de plantas (nplants). O modelo apresentou um bom ajuste, explicando cerca de 46% da variação observada (R² = 0,46). O tratamento teve efeito estatisticamente significativo (p &lt; 0,001), com uma estimativa de redução de 0,70 plantas por unidade do fator trat. O modelo como um todo foi altamente significativo (p &lt; 0,001), indicando que trat é um fator importante na determinação do número de plantas nesta análise.\nExperimento 3:\n\n\nCode\nexp3 &lt;- estande |&gt;\n  filter(exp == 3)\n\nm_exp3 &lt;- lm(nplants ~ trat, data = exp3)\nsummary(m_exp3)\n\n\n\nCall:\nlm(formula = nplants ~ trat, data = exp3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.5887  -3.9597   0.7177   5.5806  19.8952 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  95.7500     2.9529  32.425  &lt; 2e-16 ***\ntrat         -0.7634     0.1306  -5.847 6.97e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.53 on 22 degrees of freedom\nMultiple R-squared:  0.6085,    Adjusted R-squared:  0.5907 \nF-statistic: 34.19 on 1 and 22 DF,  p-value: 6.968e-06\n\n\nFoi ajustado um modelo linear simples para investigar o efeito do tratamento sobre o número de plantas no experimento 3. O modelo apresentou um ajuste estatisticamente significativo (p &lt; 0,001), explicando aproximadamente 61% da variação nos dados (R² = 0,61). O efeito do tratamento também foi altamente significativo (p &lt; 0,001), com uma estimativa de redução média de 0,76 plantas para cada unidade de trat. O valor médio estimado de plantas no grupo de referência foi 95,75. O erro padrão residual foi de 10,53, indicando um bom ajuste aos dados.\n\n\nCode\nlibrary(report)\nreport(m_exp3)\n\n\nWe fitted a linear model (estimated using OLS) to predict nplants with trat\n(formula: nplants ~ trat). The model explains a statistically significant and\nsubstantial proportion of variance (R2 = 0.61, F(1, 22) = 34.19, p &lt; .001, adj.\nR2 = 0.59). The model's intercept, corresponding to trat = 0, is at 95.75 (95%\nCI [89.63, 101.87], t(22) = 32.43, p &lt; .001). Within this model:\n\n  - The effect of trat is statistically significant and negative (beta = -0.76,\n95% CI [-1.03, -0.49], t(22) = -5.85, p &lt; .001; Std. beta = -0.78, 95% CI\n[-1.06, -0.50])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\n\n\nModelo misto - Exemplo\n\n\nCode\n# Modelo misto com efeitos aleatórios de experimento e bloco\nm_misto &lt;- lmer(nplants ~ trat + (1 | exp/bloco), data = estande)\n\n# Intervalos de confiança e sumário do modelo\nconfint(m_misto)\n\n\n                 2.5 %     97.5 %\n.sig01       3.3332097 14.4218422\n.sig02       7.2377419 47.8269818\n.sigma       9.7314178 13.9359486\n(Intercept) 43.4631239 96.0274587\ntrat        -0.7328972 -0.4044812\n\n\nCode\nsummary(m_misto)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: nplants ~ trat + (1 | exp/bloco)\n   Data: estande\n\nREML criterion at convergence: 575.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.21697 -0.63351  0.04292  0.67094  1.92907 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n bloco:exp (Intercept)  54.76    7.40   \n exp       (Intercept) 377.43   19.43   \n Residual              134.99   11.62   \nNumber of obs: 72, groups:  bloco:exp, 12; exp, 3\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 69.74524   11.57191   6.027\ntrat        -0.56869    0.08314  -6.840\n\nCorrelation of Fixed Effects:\n     (Intr)\ntrat -0.111\n\n\nCode\n# ANOVA para verificar significância dos efeitos fixos\nAnova(m_misto) # com A maiúsculo\n\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: nplants\n      Chisq Df Pr(&gt;Chisq)    \ntrat 46.788  1  7.909e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nGráfico com linhas de regressão manuais\n\n\nCode\n# Gráfico com diferentes linhas de regressão para comparação\nestande |&gt;\n  ggplot(aes(trat, nplants, color = factor(exp))) +\n  geom_point() +\n  geom_abline(intercept = 69.74, slope = -0.568, linewidth = 2) + # Linha principal\n  geom_abline(intercept = 43, slope = -0.73, linetype = \"dashed\") + # Linha comparativa\n  geom_abline(intercept = 96, slope = -0.40, linetype = \"dashed\")   # Outra linha comparativa"
  },
  {
    "objectID": "analise_estatistica.html#modelo-misto",
    "href": "analise_estatistica.html#modelo-misto",
    "title": "analise_estatistica",
    "section": "Modelo misto",
    "text": "Modelo misto\nEm um modelo misto, as observações são organizadas em grupos ou subgrupos, e cada um desses grupos pode apresentar efeitos aleatórios e/ou fixos distintos, conforme a estrutura dos dados. Por exemplo, quando os dados são coletados em diferentes localidades geográficas, é comum incluir um efeito aleatório para cada local, como ocorre no conjunto de dados estande.\n\n\nCode\nlibrary(lme4)\nmix &lt;- lmer(nplants ~trat + (trat | exp),\n            data = estande)\nsummary(mix)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: nplants ~ trat + (trat | exp)\n   Data: estande\n\nREML criterion at convergence: 580.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.0988 -0.6091  0.1722  0.6360  1.9963 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr \n exp      (Intercept) 510.68405 22.5983       \n          trat          0.05516  0.2349  -0.82\n Residual             167.91303 12.9581       \nNumber of obs: 72, groups:  exp, 3\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  69.7452    13.2146   5.278\ntrat         -0.5687     0.1643  -3.462\n\nCorrelation of Fixed Effects:\n     (Intr)\ntrat -0.731\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00274249 (tol = 0.002, component 1)\n\n\n\n\nCode\nlibrary(car)\nAnova(mix)\n\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: nplants\n      Chisq Df Pr(&gt;Chisq)    \ntrat 11.985  1  0.0005362 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nQuando se usa o modelo misto, considera que todos os experimentos são agrupados, então considera que amostra é aleatória. Para fazer o modelo de regressão em grupo (misto) acrescenta-se na função aestetic o argumento group = exp.\n\n\nCode\nestande &lt;- read_excel(\"dados.xlsx\", \"estande\")\nestande |&gt;\n  ggplot(aes(trat, nplants, group = exp))+\n  geom_point()+\n  #facet_wrap(~ exp)+\n  geom_smooth(se =  F, method = \"lm\")\n\n\n\n\n\n\n\n\n\nDe modo geral, os modelos mistos são mais eficazes do que aqueles que analisam cada experimento separadamente, pois conseguem considerar a variação tanto entre os experimentos quanto dentro deles. Além disso, esses modelos permitem analisar os dados de forma integrada, preservando informações importantes sobre a estrutura hierárquica dos dados."
  },
  {
    "objectID": "analise_estatistica.html#modelo-glm",
    "href": "analise_estatistica.html#modelo-glm",
    "title": "analise_estatistica",
    "section": "Modelo GLM",
    "text": "Modelo GLM\nO modelo linear generalizado (GLM) é uma extensão do modelo linear tradicional que possibilita trabalhar com diferentes tipos de variáveis resposta, tanto categóricas quanto contínuas. Além disso, o GLM permite que a relação entre a variável resposta e as explicativas seja não linear, ou seja, não está restrito à suposição de uma relação linear entre elas.\n\n\nCode\nlm1 &lt;- lm(nplants ~ trat, data = exp3)\nsummary(lm1)\n\n\n\nCall:\nlm(formula = nplants ~ trat, data = exp3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.5887  -3.9597   0.7177   5.5806  19.8952 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  95.7500     2.9529  32.425  &lt; 2e-16 ***\ntrat         -0.7634     0.1306  -5.847 6.97e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.53 on 22 degrees of freedom\nMultiple R-squared:  0.6085,    Adjusted R-squared:  0.5907 \nF-statistic: 34.19 on 1 and 22 DF,  p-value: 6.968e-06\n\n\n\n\nCode\nglm1 &lt;- stats::glm(nplants ~ trat, family = stats::gaussian(), data = exp3)\n\nglm2 &lt;- stats::glm(nplants ~ trat, family = stats::poisson(link = \"log\"), data = exp3)\n\nAIC(glm1)\n\n\n[1] 185.0449\n\n\nCode\nAIC(glm2)\n\n\n[1] 183.9324\n\n\n\n\nCode\nAIC(glm2)\n\n\n[1] 183.9324\n\n\n\n\nCode\nsummary(glm1)\n\n\n\nCall:\nstats::glm(formula = nplants ~ trat, family = stats::gaussian(), \n    data = exp3)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  95.7500     2.9529  32.425  &lt; 2e-16 ***\ntrat         -0.7634     0.1306  -5.847 6.97e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 110.9787)\n\n    Null deviance: 6235.8  on 23  degrees of freedom\nResidual deviance: 2441.5  on 22  degrees of freedom\nAIC: 185.04\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n\nCode\nsummary(glm2)\n\n\n\nCall:\nstats::glm(formula = nplants ~ trat, family = stats::poisson(link = \"log\"), \n    data = exp3)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.571590   0.029539 154.762  &lt; 2e-16 ***\ntrat        -0.009965   0.001488  -6.697 2.13e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 77.906  on 23  degrees of freedom\nResidual deviance: 29.952  on 22  degrees of freedom\nAIC: 183.93\n\nNumber of Fisher Scoring iterations: 4\n\n\nO modelo linear generalizado com distribuição gaussiana (family = gaussian) é indicado quando a variável resposta é contínua e segue uma distribuição normal, funcionando de forma equivalente ao modelo linear clássico (lm). Por outro lado, o modelo com distribuição de Poisson (family = poisson) é apropriado quando a variável resposta é um número inteiro não negativo e segue uma distribuição de Poisson.\nO critério AIC (Akaike’s Information Criterion) é utilizado para selecionar o melhor modelo entre várias opções, considerando tanto o ajuste aos dados quanto a complexidade do modelo. Modelos com valores menores de AIC são preferíveis, pois indicam um equilíbrio melhor entre precisão e simplicidade. No caso dos dados analisados, o modelo com família Poisson apresentou o menor AIC, indicando ser o mais adequado."
  },
  {
    "objectID": "analise_estatistica.html#ajustando-modelo-linear-simples-e-quadratico",
    "href": "analise_estatistica.html#ajustando-modelo-linear-simples-e-quadratico",
    "title": "analise_estatistica",
    "section": "Ajustando modelo linear simples e quadratico",
    "text": "Ajustando modelo linear simples e quadratico\nPara ajustar modelos de regressão linear — seja simples ou quadrático — utiliza-se a função lm() no R. Essa função recebe como argumentos uma fórmula que define a relação entre a variável dependente e a(s) variável(is) independente(s), além do conjunto de dados a ser utilizado.\nPor exemplo, para um modelo linear simples, a fórmula seria y ~ x, onde y é a variável resposta e x é a variável explicativa. Para um modelo quadrático, a fórmula pode ser y ~ x + I(x^2), incluindo o termo ao quadrado de x.\nO modelo ajustado é armazenado como um objeto do tipo lm, que pode ser examinado com a função summary(). Esse resumo fornece os coeficientes estimados, valores-p, estatísticas de ajuste (como o R²) e outros diagnósticos úteis.\n\nCoeficiente de Determinação (R²)\nO R² representa a proporção da variação da variável resposta que é explicada pelo modelo ajustado. Seu valor varia entre 0 e 1:\n\nR² = 0: o modelo não explica nenhuma variação nos dados;\nR² = 1: o modelo explica toda a variação observada.\n\nQuanto maior o R², melhor o modelo se ajusta aos dados, indicando maior capacidade explicativa por parte das variáveis independentes.\n\n\nCode\nestande2 &lt;- estande |&gt;\n  filter(exp ==2) |&gt;\n  group_by(trat) |&gt;\n  summarise(mean_nplants = mean(nplants))\n  \nestande2|&gt;\n  ggplot(aes(trat, mean_nplants))+\n  geom_point()+\n  #geom_line()\n  geom_smooth(formula = y ~ poly(x, 2), method = \"lm\", color = \"black\")+\n  annotate(geom = \"text\", \n           x = 25, y = 70,\n           label = \"y = 66.3 - 1.777x + 0.0222x2\n           R2 = 0.0.88\")"
  },
  {
    "objectID": "analise_estatistica.html#modelo-quadrático",
    "href": "analise_estatistica.html#modelo-quadrático",
    "title": "analise_estatistica",
    "section": "Modelo Quadrático",
    "text": "Modelo Quadrático\nDiferente do modelo linear, que descreve a relação entre duas variáveis por meio de uma linha reta, o modelo quadrático permite identificar padrões não lineares, com comportamento curvo.\nPara ajustar um modelo quadrático no R, utiliza-se a função lm(), incluindo o termo ao quadrado da variável independente na fórmula. Por exemplo, para modelar a relação entre uma variável dependente y e uma independente x.\n\n\nCode\nestande2 &lt;- estande2 |&gt;\n  mutate(trat2 = trat^2)\n  m1 &lt;- lm(mean_nplants ~ trat, data = estande2)\nsummary(m1)\n\n\n\nCall:\nlm(formula = mean_nplants ~ trat, data = estande2)\n\nResiduals:\n     1      2      3      4      5      6 \n12.764 -2.134 -6.782 -3.327 -4.669  4.147 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  60.9857     4.5505  13.402 0.000179 ***\ntrat         -0.7007     0.2012  -3.483 0.025294 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.117 on 4 degrees of freedom\nMultiple R-squared:  0.752, Adjusted R-squared:   0.69 \nF-statistic: 12.13 on 1 and 4 DF,  p-value: 0.02529\n\n\n\n\nCode\nhist(m1$residuals)\n\n\n\n\n\n\n\n\n\n\n\nCode\nm2 &lt;- lm(mean_nplants ~ trat + trat2,\n         data = estande2)\nsummary(m2)\n\n\n\nCall:\nlm(formula = mean_nplants ~ trat + trat2, data = estande2)\n\nResiduals:\n      1       2       3       4       5       6 \n 7.4484 -4.4200 -6.4386  1.0739  3.0474 -0.7111 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 66.30156    4.70800  14.083 0.000776 ***\ntrat        -1.77720    0.62263  -2.854 0.064878 .  \ntrat2        0.02223    0.01242   1.790 0.171344    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.517 on 3 degrees of freedom\nMultiple R-squared:  0.8801,    Adjusted R-squared:  0.8001 \nF-statistic: 11.01 on 2 and 3 DF,  p-value: 0.04152\n\n\n\n\nCode\nAIC(m1, m2)\n\n\n   df      AIC\nm1  3 45.72200\nm2  4 43.36151"
  },
  {
    "objectID": "analise_estatistica.html#duas-variáveis-resposta",
    "href": "analise_estatistica.html#duas-variáveis-resposta",
    "title": "analise_estatistica",
    "section": "Duas variáveis resposta",
    "text": "Duas variáveis resposta\n\nAjuste de Modelo Linear Simples e Quadrático\nPara ajustar modelos de regressão linear, seja simples ou quadrático, utiliza-se a função lm() no R. Essa função recebe como argumentos uma fórmula que define a relação entre a variável dependente e a(s) variável(is) independente(s), além do conjunto de dados a ser utilizado.\nPor exemplo, para um modelo linear simples, a fórmula seria y ~ x, onde y é a variável resposta e x é a variável explicativa. Para um modelo quadrático, a fórmula pode ser y ~ x + I(x^2), incluindo o termo ao quadrado de x.\nO modelo ajustado é armazenado como um objeto do tipo lm, que pode ser examinado com a função summary(). Esse resumo fornece os coeficientes estimados, valores-p, estatísticas de ajuste (como o R²) e outros diagnósticos úteis.\n\nImportando o conjunto de dados\n\n\nCode\nmofo &lt;- read_excel(\"dados.xlsx\", \"mofo\")\n\n\n\n\nVisualizando os dados\n\n\nCode\nmofo |&gt;\n  ggplot(aes(inc, yld))+\n  geom_point()+\n  geom_smooth(se = F, method = \"lm\")+\n  facet_wrap(~ study)\n\n\n\n\n\n\n\n\n\nFiltrando o experimento 1 (study = 1):\n\n\nCode\nmofo1 &lt;- mofo |&gt;\n  filter(study ==1)\nmofo1\n\n\n# A tibble: 13 × 5\n   study treat   inc   scl   yld\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     1    76  2194  2265\n 2     1     2    53  1663  2618\n 3     1     3    42  1313  2554\n 4     1     4    37  1177  2632\n 5     1     5    29   753  2820\n 6     1     6    42  1343  2799\n 7     1     7    55  1519  2503\n 8     1     8    40   516  2967\n 9     1     9    26   643  2965\n10     1    10    18   400  3088\n11     1    11    27   643  3044\n12     1    12    28   921  2925\n13     1    13    36  1196  2867\n\n\nA função cor.test() é utilizada para calcular o coeficiente de correlação entre duas variáveis numéricas. Além de fornecer o valor da correlação (como o coeficiente de Pearson), ela também realiza um teste de hipótese para verificar se essa correlação é estatisticamente significativa, ou seja, se é improvável que tenha ocorrido ao acaso.\n\n\nCode\ncor.test(mofo1$inc, mofo1$yld)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  mofo1$inc and mofo1$yld\nt = -6.8451, df = 11, p-value = 0.00002782\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9699609 -0.6921361\nsample estimates:\n       cor \n-0.8999278 \n\n\nFiltrando o experimento 2:\n\n\nCode\nmofo1 &lt;- mofo |&gt;\n  filter(study ==2)\nmofo1\n\n\n# A tibble: 13 × 5\n   study treat   inc   scl   yld\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     2     1    76  1331  2257\n 2     2     2    44   756  2393\n 3     2     3    24   338  2401\n 4     2     4    33   581  2568\n 5     2     5    37   588  2320\n 6     2     6    34   231  2308\n 7     2     7    31   925  2389\n 8     2     8    16   119  2614\n 9     2     9    10   394  2681\n10     2    10     8   206  2694\n11     2    11    15   275  2674\n12     2    12     7   131  2666\n13     2    13    19   588  2454\n\n\n\n\nCode\ncor.test(mofo1$inc, mofo1$yld)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  mofo1$inc and mofo1$yld\nt = -4.6638, df = 11, p-value = 0.0006894\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9426562 -0.4790750\nsample estimates:\n       cor \n-0.8149448 \n\n\nFiltrando o experimento 4:\n\n\nCode\nmofo1 &lt;- mofo |&gt;\n  filter(study ==4)\nmofo1\n\n\n# A tibble: 13 × 5\n   study treat   inc   scl   yld\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     4     1    69  6216  1893\n 2     4     2    39  2888  2451\n 3     4     3    41  2272  2232\n 4     4     4    39  2868  2609\n 5     4     5    40  2412  2383\n 6     4     6    40  2372  2480\n 7     4     7    44  3424  2577\n 8     4     8    43  1744  2367\n 9     4     9    26  1456  2769\n10     4    10    29  1732  2907\n11     4    11    30  1080  2298\n12     4    12    34  1592  2976\n13     4    13    44  3268  2200\n\n\n\n\nCode\ncor.test(mofo1$inc, mofo1$yld)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  mofo1$inc and mofo1$yld\nt = -3.7242, df = 11, p-value = 0.003357\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9194503 -0.3327077\nsample estimates:\n       cor \n-0.7467931 \n\n\nFiltrando o experimento 3:\n\n\nCode\nmofo1 &lt;- mofo |&gt;\n  filter(study ==3)\nmofo1\n\n\n# A tibble: 13 × 5\n   study treat   inc   scl   yld\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     3     1    65  5013  2839\n 2     3     2    33  3619  3375\n 3     3     3    40  2325  3264\n 4     3     4    35  2588  3301\n 5     3     5    48  3969  3220\n 6     3     6    31  1556  3321\n 7     3     7    39  3175  3229\n 8     3     8    25  1763  3517\n 9     3     9    26  2894  3595\n10     3    10    21   350  3702\n11     3    11    23   419  3652\n12     3    12    25   644  3608\n13     3    13    33  2850  3334\n\n\n\n\n\nMatrizes de Correlação\nA matriz de correlação é uma tabela que exibe os coeficientes de correlação entre todos os pares de variáveis de um conjunto de dados. Cada célula da matriz indica a força e a direção da relação entre duas variáveis, geralmente usando o coeficiente de correlação de Pearson, embora outras medidas (como Spearman ou Kendall) também possam ser utilizadas, conforme o contexto da análise.\nGerando matriz de correlação para as variáveis selecionadas:\n\n\nCode\nmofo1 &lt;- mofo |&gt;\n  filter(study ==3)\nmofo1\n\n\n# A tibble: 13 × 5\n   study treat   inc   scl   yld\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     3     1    65  5013  2839\n 2     3     2    33  3619  3375\n 3     3     3    40  2325  3264\n 4     3     4    35  2588  3301\n 5     3     5    48  3969  3220\n 6     3     6    31  1556  3321\n 7     3     7    39  3175  3229\n 8     3     8    25  1763  3517\n 9     3     9    26  2894  3595\n10     3    10    21   350  3702\n11     3    11    23   419  3652\n12     3    12    25   644  3608\n13     3    13    33  2850  3334\n\n\n\n\nCode\ncor.test(mofo1$inc, mofo1$yld)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  mofo1$inc and mofo1$yld\nt = -10.9, df = 11, p-value = 0.0000003105\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9872663 -0.8579544\nsample estimates:\n      cor \n-0.956692 \n\n\n\n\nCode\nlibrary(dplyr)\n\ncor(mofo1[, 3:5])\n\n\n           inc        scl       yld\ninc  1.0000000  0.8441514 -0.956692\nscl  0.8441514  1.0000000 -0.836512\nyld -0.9566920 -0.8365120  1.000000\n\n\n\n\nGráficos de Correlação\nPara visualizar matrizes de correlação, o pacote corrplot é uma ferramenta amplamente utilizada no R. Ele oferece diversas funções para explorar e representar visualmente as relações entre variáveis em um conjunto de dados, facilitando a identificação de padrões de correlação.\nPrincipais funções do pacote corrplot:\n\ncorr.test(): realiza testes estatísticos para matrizes de correlação, calculando coeficientes de correlação, valores-p e intervalos de confiança, permitindo avaliar a significância das correlações.\ncorrplot(): gera gráficos que exibem a matriz de correlação com diferentes estilos visuais. Permite personalizar o tipo de gráfico, as cores, adicionar os valores numéricos dos coeficientes, além de possibilitar agrupamentos hierárquicos.\n\n\n\nCode\nmofo1 &lt;- mofo |&gt;\n  filter(study ==3)\nmofo1\n\n\n# A tibble: 13 × 5\n   study treat   inc   scl   yld\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     3     1    65  5013  2839\n 2     3     2    33  3619  3375\n 3     3     3    40  2325  3264\n 4     3     4    35  2588  3301\n 5     3     5    48  3969  3220\n 6     3     6    31  1556  3321\n 7     3     7    39  3175  3229\n 8     3     8    25  1763  3517\n 9     3     9    26  2894  3595\n10     3    10    21   350  3702\n11     3    11    23   419  3652\n12     3    12    25   644  3608\n13     3    13    33  2850  3334\n\n\n\n\nCode\ncor.test(mofo1$inc, mofo1$yld)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  mofo1$inc and mofo1$yld\nt = -10.9, df = 11, p-value = 0.0000003105\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9872663 -0.8579544\nsample estimates:\n      cor \n-0.956692 \n\n\n\n\nCode\npcor &lt;- cor(mofo1[, 3:5])\n\nlibrary(corrplot)\ncorrplot(pcor, method = 'number', type = \"lower\")\n\n\n\n\n\n\n\n\n\n\n\nModelo de Kendall\nO coeficiente de correlação de Kendall é uma medida não paramétrica que avalia a associação entre duas variáveis ordinais ou variáveis medidas em escala ordinal. Assim como o coeficiente de Pearson, o coeficiente de Kendall varia entre -1 e 1, indicando a direção e a força da relação.\nPor ser não paramétrico, o método de Kendall é mais robusto em situações onde os dados não seguem uma relação linear ou quando as variáveis não possuem distribuição normal, sendo uma alternativa adequada para analisar associações em dados ordinais ou com distribuições não normais.\n\n\nCode\nmofo1 &lt;- mofo |&gt;\n  filter(study ==3)\nmofo1\n\n\n# A tibble: 13 × 5\n   study treat   inc   scl   yld\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     3     1    65  5013  2839\n 2     3     2    33  3619  3375\n 3     3     3    40  2325  3264\n 4     3     4    35  2588  3301\n 5     3     5    48  3969  3220\n 6     3     6    31  1556  3321\n 7     3     7    39  3175  3229\n 8     3     8    25  1763  3517\n 9     3     9    26  2894  3595\n10     3    10    21   350  3702\n11     3    11    23   419  3652\n12     3    12    25   644  3608\n13     3    13    33  2850  3334\n\n\n\n\nCode\nshapiro.test(mofo1$inc)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  mofo1$inc\nW = 0.87111, p-value = 0.05412\n\n\n\n\nCode\nshapiro.test(mofo1$yld)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  mofo1$yld\nW = 0.92193, p-value = 0.2663\n\n\n\n\nCode\ncor.test(mofo1$inc, mofo1$yld, method = \"spearman\")\n\n\n\n    Spearman's rank correlation rho\n\ndata:  mofo1$inc and mofo1$yld\nS = 715.97, p-value = 0.00000007166\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.9669458 \n\n\n\n\nCode\npcor &lt;- cor(mofo1[, 3:5], method = \"spearman\")\n\nlibrary(corrplot)\ncorrplot(pcor, method = 'number', type = \"lower\")"
  },
  {
    "objectID": "analise_estatistica.html#gráfico-de-barras---frequência",
    "href": "analise_estatistica.html#gráfico-de-barras---frequência",
    "title": "analise_estatistica",
    "section": "Gráfico de barras - frequência",
    "text": "Gráfico de barras - frequência\n\n\nCode\nsurvey |&gt; \n  filter(residue != \"NA\") |&gt; \n  count(residue, species) |&gt; \n  ggplot(aes(residue, n, fill = species)) +\n  geom_col() +\n  scale_fill_brewer(palette = \"Greens\") +\n  theme_minimal() +\n  labs(x = \"Resíduo\", y = \"Frequência\", fill = \"Espécie\")"
  },
  {
    "objectID": "analise_estatistica.html#frequência-de-classe",
    "href": "analise_estatistica.html#frequência-de-classe",
    "title": "analise_estatistica",
    "section": "Frequência de classe",
    "text": "Frequência de classe\nA função chisq.test() é utilizada para realizar testes do qui-quadrado em duas principais situações:\n\nTestes de independência em tabelas de contingência — verifica se existe associação estatística entre duas variáveis categóricas;\nTestes de aderência (ou qualidade de ajuste) — avalia se a distribuição observada de uma variável categórica difere significativamente de uma distribuição esperada (teórica).\n\nEssa função retorna estatísticas como o valor do qui-quadrado, os graus de liberdade e o valor-p, que ajudam a determinar se as diferenças observadas são estatisticamente significativas.\n\n\nCode\nq &lt;- table (survey$residue, survey$species)\nchisq.test(q)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  q\nX-squared = 1.1997, df = 1, p-value = 0.2734"
  },
  {
    "objectID": "analise_estatistica.html#para-frequências-mais-baixas",
    "href": "analise_estatistica.html#para-frequências-mais-baixas",
    "title": "analise_estatistica",
    "section": "Para frequências mais baixas",
    "text": "Para frequências mais baixas\nA função fisher.test() realiza o teste exato de Fisher, que é utilizado para avaliar a independência entre linhas e colunas em uma tabela de contingência, especialmente quando os valores esperados são baixos (frequências menores que 5), condição na qual o teste do qui-quadrado pode não ser confiável.\nEsse teste verifica se há evidência de associação entre duas variáveis categóricas, assumindo que as margens da tabela (totais de linha e coluna) são fixas. Ele calcula exatamente a probabilidade de observar uma distribuição tão extrema quanto a observada, ou mais, sob a hipótese nula de independência.\n\n\nCode\nfisher.test(q)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  q\np-value = 0.2118\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.357205 1.311411\nsample estimates:\nodds ratio \n 0.6819103 \n\n\n\n\nCode\nq &lt;- table (survey$residue, survey$inc_class)\nchisq.test(q)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  q\nX-squared = 2.6165, df = 1, p-value = 0.1058\n\n\n\n\nCode\nsurvey |&gt; \n  filter(residue != \"NA\") |&gt; \n  count(residue, inc_class) |&gt; \n  ggplot(aes(residue, n, fill = inc_class)) +\n  geom_col() +\n  scale_fill_brewer(palette = \"Greens\") +\n  theme_minimal() +\n  labs(\n    x = \"Resíduo\",\n    y = \"Frequência\",\n    fill = \"Classe de Incidência\"\n  )"
  },
  {
    "objectID": "analise_estatistica.html#cruzamento-entre-variáveis",
    "href": "analise_estatistica.html#cruzamento-entre-variáveis",
    "title": "analise_estatistica",
    "section": "Cruzamento entre variáveis",
    "text": "Cruzamento entre variáveis\n\n\nCode\nsurvey |&gt; count (year)\n\n\n# A tibble: 3 × 2\n   year     n\n  &lt;dbl&gt; &lt;int&gt;\n1  2009   265\n2  2010   216\n3  2011   185\n\n\n\n\nCode\n#Frequência de ocorrência por ano\n\ntable (survey$year, survey$species)\n\n\n      \n       Fgra Fspp\n  2009  225   40\n  2010  187   29\n  2011  140   45\n\n\n\n\nCode\ncurve &lt;- read_excel(\"dados.xlsx\",\"curve\")\n\ncurve2 &lt;- curve |&gt; \n  group_by(Irrigation, day) |&gt; \n  summarise(mean_severity = mean (severity),\n            sd_severity = sd(severity))\n\ncurve2 |&gt; ggplot(aes(day,mean_severity, color=Irrigation))+\n  geom_point()+\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncurve2 |&gt; ggplot(aes(day,mean_severity, color=Irrigation))+\n  geom_point()+\n  geom_errorbar(aes(ymin=mean_severity - sd_severity,\n                    ymax = mean_severity + sd_severity),\n                width = 0.1)+\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(epifitter)\n\ncurve3 &lt;- curve |&gt; \n  group_by(Irrigation, rep) |&gt; \n  summarise(audpc = AUDPC(day, severity,\n                          y_proportion = F)) |&gt; \n  pivot_wider(1, names_from = Irrigation,\n            values_from = audpc)\n\nt.test(curve3$Drip, curve$Furrow)\n\n\n\n    One Sample t-test\n\ndata:  curve3$Drip\nt = 51.206, df = 2, p-value = 0.0003812\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 12.26473 14.51493\nsample estimates:\nmean of x \n 13.38983 \n\n\nExemplo:\n\n\nCode\nlibrary(gsheet)\n\ntw &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1t5xftb0xSRPNFhM7h_MiPNtwt2UFUcm9/edit#gid=1594889893\")\ntw |&gt; \n  group_by(cult,silicio,hai) |&gt; \n  summarise (mean_lesion = mean (as.numeric(lesion_size)),\n             sd_lesion = sd(lesion_size)) |&gt; \n  ggplot(aes(hai,mean_lesion, color = silicio))+\n  geom_line()+\n  geom_point()+\n  geom_errorbar(aes(ymin=mean_lesion - sd_lesion,\n                    ymax = mean_lesion + sd_lesion),\n                width = 0.1)+\n  facet_wrap(~cult)+\n   labs (y = \"Lesion size (mm)\", x = \"Hours after inoculation\")+\n  ggthemes::theme_few()+\nscale_color_manual(values = c(\"#1f78b4\", \"#6baed6\", \"#9ecae1\", \"#c6dbef\"))"
  },
  {
    "objectID": "analise_estatistica.html#teste-anova",
    "href": "analise_estatistica.html#teste-anova",
    "title": "analise_estatistica",
    "section": "Teste ANOVA",
    "text": "Teste ANOVA\nOs resultados da análise de variância podem ajudar a identificar quais variáveis e interações têm efeito significativo na variável resposta audpc.\n\n\nCode\naov1 &lt;- aov(sqrt(audpc) ~exp*cult*silicio, data = tw2)\nsummary(aov1)\n\n\n                 Df Sum Sq Mean Sq F value               Pr(&gt;F)    \nexp               1    0.1     0.1   0.033              0.85737    \ncult              1  135.0   135.0  74.615       0.000000000267 ***\nsilicio           1  839.4   839.4 464.034 &lt; 0.0000000000000002 ***\nexp:cult          1    0.0     0.0   0.000              0.99843    \nexp:silicio       1    0.0     0.0   0.002              0.96060    \ncult:silicio      1   19.3    19.3  10.671              0.00239 ** \nexp:cult:silicio  1    0.0     0.0   0.015              0.90324    \nResiduals        36   65.1     1.8                                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nlibrary(performance)\ncheck_normality(aov1)\n\n\nOK: residuals appear as normally distributed (p = 0.893).\n\n\n\n\nCode\ncheck_heteroscedasticity(aov1)\n\n\nOK: Error variance appears to be homoscedastic (p = 0.316).\n\n\n\n\nCode\nlibrary(emmeans)\nm1 &lt;- emmeans (aov1, ~cult | silicio, type = \"response\")\n\n\n\nExemplo\n\n\nCode\n# Tabela de frequência\ntab &lt;- table(survey$residue, survey$species)\n\n# Teste qui-quadrado\nchisq.test(tab)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tab\nX-squared = 1.1997, df = 1, p-value = 0.2734\n\n\nCode\n# Visualização\nlibrary(ggplot2)\nsurvey |&gt; \n  count(residue, species) |&gt; \n  ggplot(aes(residue, n, fill = species)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"#2171b5\", \"#6baed6\", \"#9ecae1\", \"#c6dbef\")) +\n  labs(x = \"Resíduo\", y = \"Frequência\", fill = \"Espécie\") +\n  theme_minimal()"
  },
  {
    "objectID": "analise_estatistica.html#teste-de-tukey-com-pacote-expdes.pt-experimento-em-dic",
    "href": "analise_estatistica.html#teste-de-tukey-com-pacote-expdes.pt-experimento-em-dic",
    "title": "analise_estatistica",
    "section": "Teste de Tukey com pacote ExpDes.pt (Experimento em DIC)",
    "text": "Teste de Tukey com pacote ExpDes.pt (Experimento em DIC)\n\nInstalar e carregar o pacote\n\n\nCode\n# Carregue o pacote\nlibrary(ExpDes.pt)\n\n\n\n\nCriar os dados de exemplo\nVamos supor que avaliamos a produtividade de 4 cultivares de milho, com 4 repetições cada:\n\n\nCode\n# Fator: tratamentos (cultivares)\ntrat &lt;- c(rep(\"A\", 4), rep(\"B\", 4), rep(\"C\", 4), rep(\"D\", 4))\n\n# Resposta: produtividade em kg/ha\nresp &lt;- c(5000, 5200, 5100, 4950,\n          5300, 5400, 5350, 5250,\n          4800, 4700, 4900, 4750,\n          5100, 5000, 4950, 5050)\n\n\n\n\nAnálise de variância com Tukey (DIC)\ndic(): realiza a análise para Delineamento Inteiramente Casualizado.\ntrat: vetor com os tratamentos (fator qualitativo).\nresp: vetor com a variável resposta (produtividade).\nquali = TRUE: indica que os tratamentos são qualitativos.\nmcomp = \"tukey\": escolhe o teste de comparação de médias de Tukey.\nsigT = 0.05: nível de significância (5%).\n\n\nCode\ndic(trat, resp, quali = TRUE, mcomp = \"tukey\", sigT = 0.05)\n\n\n------------------------------------------------------------------------\nQuadro da analise de variancia\n------------------------------------------------------------------------\n           GL     SQ     QM     Fc       Pr&gt;Fc\nTratamento  3 581250 193750 27.761 0.000011051\nResiduo    12  83750   6979                   \nTotal      15 665000                          \n------------------------------------------------------------------------\nCV = 1.65 %\n\n------------------------------------------------------------------------\nTeste de normalidade dos residuos ( Shapiro-Wilk ) \nValor-p:  0.6642314 \nDe acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais.\n------------------------------------------------------------------------\n\n------------------------------------------------------------------------\nTeste de homogeneidade de variancia \nvalor-p:  0.7743792 \nDe acordo com o teste de bartlett a 5% de significancia, as variancias podem ser consideradas homogeneas.\n------------------------------------------------------------------------\n\nTeste de Tukey\n------------------------------------------------------------------------\nGrupos Tratamentos Medias\na    B   5325 \n b   A   5062.5 \n b   D   5025 \n  c      C   4787.5 \n------------------------------------------------------------------------"
  },
  {
    "objectID": "analise_estatistica.html#teste-de-tukey-com-pacote-expdes.pt-dados-transformados-em-raiz---sqrt",
    "href": "analise_estatistica.html#teste-de-tukey-com-pacote-expdes.pt-dados-transformados-em-raiz---sqrt",
    "title": "analise_estatistica",
    "section": "Teste de Tukey com pacote ExpDes.pt (dados transformados em raiz - sqrt)",
    "text": "Teste de Tukey com pacote ExpDes.pt (dados transformados em raiz - sqrt)\n\n\nCode\ninsects &lt;- InsectSprays\n\ninsects$count2 &lt;- sqrt(insects$count)\n\ndic(insects$spray,\n    insects$count2,\n    mcomp = \"tukey\")\n\n\n------------------------------------------------------------------------\nQuadro da analise de variancia\n------------------------------------------------------------------------\n           GL      SQ      QM     Fc                      Pr&gt;Fc\nTratamento  5  88.438 17.6876 44.799 0.000000000000000000063345\nResiduo    66  26.058  0.3948                                  \nTotal      71 114.496                                          \n------------------------------------------------------------------------\nCV = 22.34 %\n\n------------------------------------------------------------------------\nTeste de normalidade dos residuos ( Shapiro-Wilk ) \nValor-p:  0.6813773 \nDe acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais.\n------------------------------------------------------------------------\n\n------------------------------------------------------------------------\nTeste de homogeneidade de variancia \nvalor-p:  0.5855673 \nDe acordo com o teste de bartlett a 5% de significancia, as variancias podem ser consideradas homogeneas.\n------------------------------------------------------------------------\n\nTeste de Tukey\n------------------------------------------------------------------------\nGrupos Tratamentos Medias\na    F   4.018617 \na    B   3.876631 \na    A   3.760678 \n b   D   2.164354 \n bc      E   1.809461 \n  c      C   1.244857 \n------------------------------------------------------------------------"
  },
  {
    "objectID": "analise_estatistica.html#teste-de-scott-knott---pacote-expdes.pt",
    "href": "analise_estatistica.html#teste-de-scott-knott---pacote-expdes.pt",
    "title": "analise_estatistica",
    "section": "Teste de Scott Knott - pacote ExpDes.pt",
    "text": "Teste de Scott Knott - pacote ExpDes.pt\nO método de Scott-Knott é uma técnica eficiente para comparar tratamentos em experimentos, especialmente quando o objetivo é agrupar médias em conjuntos homogêneos. Esse método busca minimizar a variabilidade dentro dos grupos e, ao mesmo tempo, maximizar a diferença entre eles, evitando sobreposição. Para isso, as médias dos tratamentos são ordenadas, permitindo sua classificação. Em seguida, são avaliadas todas as possíveis divisões (partições) com o propósito de identificar a melhor separação entre os grupos.\n\nExperimento em DIC\n\n\nCode\n#Agrupamento pelo teste de Scott Knott: O teste agrupa médias e serve para 1 fator apenas.\n\ndic(insects$spray,\n    insects$count2,\n    mcomp = \"sk\")\n\n\n------------------------------------------------------------------------\nQuadro da analise de variancia\n------------------------------------------------------------------------\n           GL      SQ      QM     Fc                      Pr&gt;Fc\nTratamento  5  88.438 17.6876 44.799 0.000000000000000000063345\nResiduo    66  26.058  0.3948                                  \nTotal      71 114.496                                          \n------------------------------------------------------------------------\nCV = 22.34 %\n\n------------------------------------------------------------------------\nTeste de normalidade dos residuos ( Shapiro-Wilk ) \nValor-p:  0.6813773 \nDe acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais.\n------------------------------------------------------------------------\n\n------------------------------------------------------------------------\nTeste de homogeneidade de variancia \nvalor-p:  0.5855673 \nDe acordo com o teste de bartlett a 5% de significancia, as variancias podem ser consideradas homogeneas.\n------------------------------------------------------------------------\n\nTeste de Scott-Knott\n------------------------------------------------------------------------\n  Grupos Tratamentos   Medias\n1      a           F 4.018617\n2      a           B 3.876631\n3      a           A 3.760678\n4      b           D 2.164354\n5      b           E 1.809461\n6      c           C 1.244857\n------------------------------------------------------------------------\n\n\n\n\nExperimento em DBC\n\nCriar dados de exemplo\nVamos supor que testamos 5 cultivares de feijão, com 4 blocos (repetições):\n\n\nCode\n# Tratamentos (cultivares)\ntrat &lt;- c(rep(\"A\",4), rep(\"B\",4), rep(\"C\",4), rep(\"D\",4), rep(\"E\",4))\n\n# Blocos (repetições)\nbloco &lt;- rep(1:4, 5)\n\n# Produtividade em kg/ha (variável resposta)\nresp &lt;- c(1800, 1850, 1750, 1820,  # A\n          2100, 2150, 2080, 2120,  # B\n          1700, 1680, 1720, 1690,  # C\n          1950, 1980, 1930, 1970,  # D\n          2200, 2250, 2220, 2180)  # E\n\n\n\n\nAnálise de variância + Scott-Knott (DBC)\ndbc(): função para analisar um experimento em Delineamento em Blocos Casualizados.\ntrat: vetor com os tratamentos (cultivares).\nbloco: vetor com os blocos (repetições).\nresp: vetor com a variável resposta (ex: produtividade).\nquali = TRUE: indica que os tratamentos são qualitativos.\nmcomp = \"sk\": aplica o teste de Scott-Knott para comparação de médias.\nsigT = 0.05: nível de significância de 5%.\n\n\nCode\ndbc(trat, bloco, resp, quali = TRUE, mcomp = \"sk\", sigT = 0.05)\n\n\n------------------------------------------------------------------------\nQuadro da analise de variancia\n------------------------------------------------------------------------\n           GL     SQ     QM      Fc   Pr&gt;Fc\nTratamento  4 719620 179905 263.919 0.00000\nBloco       3   4820   1607   2.357 0.12313\nResiduo    12   8180    682                \nTotal      19 732620                       \n------------------------------------------------------------------------\nCV = 1.33 %\n\n------------------------------------------------------------------------\nTeste de normalidade dos residuos \nvalor-p:  0.55218 \nDe acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais.\n------------------------------------------------------------------------\n\n------------------------------------------------------------------------\nTeste de homogeneidade de variancia \nvalor-p:  0.2899448 \nDe acordo com o teste de oneillmathews a 5% de significancia, as variancias podem ser consideradas homogeneas.\n------------------------------------------------------------------------\n\nTeste de Scott-Knott\n------------------------------------------------------------------------\n  Grupos Tratamentos Medias\n1      a           E 2212.5\n2      b           B 2112.5\n3      c           D 1957.5\n4      d           A 1805.0\n5      e           C 1697.5\n------------------------------------------------------------------------"
  },
  {
    "objectID": "mapas.html",
    "href": "mapas.html",
    "title": "mapas",
    "section": "",
    "text": "O conjunto de dados será o da ferrugem do café na Etiópia que está no arquivo de dados na nuvem.\n\n\n\nUsaremos a função gsheet2tbl () do pacote [gsheet] para carregar os dados no ambiente.\n\n\nCode\nlibrary(gsheet)\ncr &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=1871397229#gid=1871397229\")\n\n\n\n\n\nUsaremos a função datatable () do pacote [DT].\n\n\nCode\nlibrary(DT)  #cria uma tabela interativa\ndatatable(cr) #função do pacote DT\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\ncr |&gt; # Criar um gráfico\n  ggplot(aes(lon, lat)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rnaturalearth) # Baixar esse pacote\nlibrary(rnaturalearthhires) # Baixar esse pacote atraves do link abaixo\n\nlibrary(terra) # As vezes para gerar o gráfico é necessário baixar esses 2 programas.\nlibrary(sf) # sf = simplificado\n\n\nremotes::install_github(\"ropensci/rnaturalearthhires\") # Pegou link do google\n\n\n* checking for file 'C:\\Users\\igo_a\\AppData\\Local\\Temp\\RtmponCLEi\\remotes2c5c4d094f7d\\ropensci-rnaturalearthhires-e4736f6/DESCRIPTION' ... OK\n* preparing 'rnaturalearthhires':\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\n* building 'rnaturalearthhires_1.0.0.9000.tar.gz'\n\n\nCode\nETH &lt;- ne_states(country = \"Ethiopia\", # Criar data frame com esses dados\n                 returnclass = \"sf\")\n\n\nlibrary(tidyverse) # Gerar mapas\nlibrary(ggthemes)\nlibrary(ggspatial)\n\nETH |&gt; # ETH = state, estado ou provincia\n ggplot()+\n  geom_sf(fill = \"grey90\")+ # Cor do fundo\n  geom_point(data = cr, aes(lon, lat, color = inc))+\n  scale_color_viridis_c()+ # Escala de cores com gradiente de intensidade\n  ##theme_void() # Deixa mapa todo branco, limpo\n  #theme_map() + # Do pacote ggthemes, mesma função do theme_void\n  theme_minimal()+ # Olha esse temas no google\n  theme(legend.position = \"bottom\")+ # Colocar legenda embaixo, a escala de cor da incidência vai ficar abaixo ao invés do lado do mapa\n  annotation_scale(location = \"tl\")+ # Para adicionar escala\n  annotation_north_arrow(location = \"br\", which_north = \"true\")+ # Flecha indicando o norte\n  labs(title = \"Ferrugem do Café na Etiópia\",\n     x = \"Longitude\", y = \"Latitude\",\n     subtitle = \"Levantamento em fazendas\",\n     caption = \"Fonte: Del ponte et al. (2025)\",# Adicionar citação\n     color = \"Incidência (%)\")\n\n\n\n\n\n\n\n\n\nCode\n     ggsave(\"mapa_etiopia.png\", bg = \"white\") # Para salvar como imagem\n\n# Para salvar como pdf, # ggsave(\"mapa_etiopia.pdf\", bg = \"white\")\n\n\n\n\n\nConjunto de dados: Para plotar o mapa do pais, usa-se a função ne_countries\n\n\nCode\nBRA &lt;- ne_states(country = \"Brazil\", \n                    returnclass = \"sf\")\nggplot(BRA) +\ngeom_sf(color = \"white\",\n          fill = \"darkgreen\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nMinas Gerais:\n\n\nCode\nBRA &lt;- ne_states(country = \"Brazil\", \n                    returnclass = \"sf\")\nMG &lt;- BRA |&gt; filter(name_en == \"Minas Gerais\")\nggplot(BRA) +\ngeom_sf(color = \"black\",\n          fill = \"white\") +\n  geom_sf(data = MG, color = \"black\",\n            fill = \"green\")"
  },
  {
    "objectID": "mapas.html#exemplo-1-mapa-ferrugem-do-café-na-etiópia",
    "href": "mapas.html#exemplo-1-mapa-ferrugem-do-café-na-etiópia",
    "title": "mapas",
    "section": "",
    "text": "O conjunto de dados será o da ferrugem do café na Etiópia que está no arquivo de dados na nuvem."
  },
  {
    "objectID": "mapas.html#importa-os-dados",
    "href": "mapas.html#importa-os-dados",
    "title": "mapas",
    "section": "",
    "text": "Usaremos a função gsheet2tbl () do pacote [gsheet] para carregar os dados no ambiente.\n\n\nCode\nlibrary(gsheet)\ncr &lt;- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1bq2N19DcZdtax2fQW9OHSGMR0X2__Z9T/edit?gid=1871397229#gid=1871397229\")"
  },
  {
    "objectID": "mapas.html#visualiza-os-dados",
    "href": "mapas.html#visualiza-os-dados",
    "title": "mapas",
    "section": "",
    "text": "Usaremos a função datatable () do pacote [DT].\n\n\nCode\nlibrary(DT)  #cria uma tabela interativa\ndatatable(cr) #função do pacote DT"
  },
  {
    "objectID": "mapas.html#gráficos",
    "href": "mapas.html#gráficos",
    "title": "mapas",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\ncr |&gt; # Criar um gráfico\n  ggplot(aes(lon, lat)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rnaturalearth) # Baixar esse pacote\nlibrary(rnaturalearthhires) # Baixar esse pacote atraves do link abaixo\n\nlibrary(terra) # As vezes para gerar o gráfico é necessário baixar esses 2 programas.\nlibrary(sf) # sf = simplificado\n\n\nremotes::install_github(\"ropensci/rnaturalearthhires\") # Pegou link do google\n\n\n* checking for file 'C:\\Users\\igo_a\\AppData\\Local\\Temp\\RtmponCLEi\\remotes2c5c4d094f7d\\ropensci-rnaturalearthhires-e4736f6/DESCRIPTION' ... OK\n* preparing 'rnaturalearthhires':\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\n* building 'rnaturalearthhires_1.0.0.9000.tar.gz'\n\n\nCode\nETH &lt;- ne_states(country = \"Ethiopia\", # Criar data frame com esses dados\n                 returnclass = \"sf\")\n\n\nlibrary(tidyverse) # Gerar mapas\nlibrary(ggthemes)\nlibrary(ggspatial)\n\nETH |&gt; # ETH = state, estado ou provincia\n ggplot()+\n  geom_sf(fill = \"grey90\")+ # Cor do fundo\n  geom_point(data = cr, aes(lon, lat, color = inc))+\n  scale_color_viridis_c()+ # Escala de cores com gradiente de intensidade\n  ##theme_void() # Deixa mapa todo branco, limpo\n  #theme_map() + # Do pacote ggthemes, mesma função do theme_void\n  theme_minimal()+ # Olha esse temas no google\n  theme(legend.position = \"bottom\")+ # Colocar legenda embaixo, a escala de cor da incidência vai ficar abaixo ao invés do lado do mapa\n  annotation_scale(location = \"tl\")+ # Para adicionar escala\n  annotation_north_arrow(location = \"br\", which_north = \"true\")+ # Flecha indicando o norte\n  labs(title = \"Ferrugem do Café na Etiópia\",\n     x = \"Longitude\", y = \"Latitude\",\n     subtitle = \"Levantamento em fazendas\",\n     caption = \"Fonte: Del ponte et al. (2025)\",# Adicionar citação\n     color = \"Incidência (%)\")\n\n\n\n\n\n\n\n\n\nCode\n     ggsave(\"mapa_etiopia.png\", bg = \"white\") # Para salvar como imagem\n\n# Para salvar como pdf, # ggsave(\"mapa_etiopia.pdf\", bg = \"white\")"
  },
  {
    "objectID": "mapas.html#exemplo-2-mapa-brasil",
    "href": "mapas.html#exemplo-2-mapa-brasil",
    "title": "mapas",
    "section": "",
    "text": "Conjunto de dados: Para plotar o mapa do pais, usa-se a função ne_countries\n\n\nCode\nBRA &lt;- ne_states(country = \"Brazil\", \n                    returnclass = \"sf\")\nggplot(BRA) +\ngeom_sf(color = \"white\",\n          fill = \"darkgreen\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nMinas Gerais:\n\n\nCode\nBRA &lt;- ne_states(country = \"Brazil\", \n                    returnclass = \"sf\")\nMG &lt;- BRA |&gt; filter(name_en == \"Minas Gerais\")\nggplot(BRA) +\ngeom_sf(color = \"black\",\n          fill = \"white\") +\n  geom_sf(data = MG, color = \"black\",\n            fill = \"green\")"
  }
]